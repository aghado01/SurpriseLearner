{
  "files": [
    {
      "size_kb": 1.47,
      "content": "# .copilot Directory Instructions\n\nThis directory contains modernized helper tools and utilities organized according to current PowerShell Central practices.\n\n## Directory Structure\n\n- **helpers/diagnostics/** - Scanning, checking, and validation tools\n- **helpers/fixing/** - Automated repair and cleanup utilities\n- **helpers/testing/** - Test runners and debugging tools\n- **helpers/organize/** - File organization and structure management\n- **ForReview/** - Files requiring manual review and classification\n\n## PowerShell Standards\n\n- Use PowerShell 7+ syntax with complete bracket closure\n- Implement proper string interpolation with $() syntax\n- Include comprehensive error handling and logging\n- Follow strict parameter validation and type hints\n- Provide full replacement code, not snippets\n\n## Usage Guidelines\n\n1. **Diagnostics First** - Always run diagnostic tools before making changes\n2. **Fix Systematically** - Use fixing tools to normalize code formatting and structure\n3. **Test Thoroughly** - Run comprehensive tests after any changes\n4. **Organize Continuously** - Maintain clean directory structure with organization tools\n\n## File Naming Conventions\n\n- *_scanner.ps1 - Analysis and detection tools\n- *_check.ps1 - Quick verification scripts\n- *_fix.ps1 - Automated repair utilities\n- *_test.ps1 - Test utilities and runners\n- *_organizer.ps1 - Structure management tools\n\nThis structure supports scalable, maintainable PowerShell development workflows with AI assistance integration.\r\n",
      "content_preview": "# .copilot Directory Instructions\n\nThis directory contains modernized helper tools and utilities organized according to current PowerShell Central practices.\n\n## Directory Structure\n\n- **helpers/diagn...",
      "directory": ".copilot",
      "size_bytes": 1506,
      "is_binary": false,
      "last_modified": "2025-08-08 01:07:20",
      "path": ".copilot/copilot-instructions.md",
      "extension": ".md",
      "filename": "copilot-instructions.md"
    },
    {
      "size_kb": 1.08,
      "content": "# SurpriseLearner Workspace Instructions\n\n## Memory Orchestration Context\nThis project implements a sophisticated memory orchestration architecture for AI-assisted development:\n\n- **Session Memory**: Current terminal session context\n- **Sequence Memory**: Post-instruction execution logging\n- **Project Memory**: Cross-session continuity\n- **Global Memory**: Pattern learning across projects\n\n## Code Generation Context\nWhen generating code for this project:\n\n1. **Always consider the hierarchical memory architecture** - code should integrate with the four-tier memory system\n2. **Implement console-aware capture** - include hooks for terminal output integration\n3. **Follow the production-grade criteria** from ProductionGrade.md\n4. **Maintain the adaptive Bayesian learning patterns** from the RecursiveBayesianLearner.py\n\n## Development Workflow Integration\n- Use the Intel compute optimizations from compute_optimizations.py\n- Follow the device management patterns from device.py\n- Integrate with the existing configuration management system\n- Maintain compatibility with both CPU and CUDA execution paths\n",
      "content_preview": "# SurpriseLearner Workspace Instructions\n\n## Memory Orchestration Context\nThis project implements a sophisticated memory orchestration architecture for AI-assisted development:\n\n- **Session Memory**: ...",
      "directory": ".copilot",
      "size_bytes": 1111,
      "is_binary": false,
      "last_modified": "2025-08-08 01:50:01",
      "path": ".copilot/copilot-workspace-instructions.md",
      "extension": ".md",
      "filename": "copilot-workspace-instructions.md"
    },
    {
      "size_kb": 1.95,
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nQuick CI/CD readiness check.\r\n\"\"\"\r\n\r\nimport subprocess\r\nimport sys\r\n\r\ndef check_cicd_readiness():\r\n    \"\"\"Quick check for CI/CD readiness.\"\"\"\r\n    print(\"🔍 Quick CI/CD Readiness Check\\n\")\r\n\r\n    checks = [\r\n        {\r\n            'name': 'Package Import',\r\n            'cmd': 'python -c \"import adaptive_bayesian_driver; print(\\'OK\\')\"',\r\n            'critical': True\r\n        },\r\n        {\r\n            'name': 'Critical Whitespace Issues',\r\n            'cmd': 'python -m flake8 adaptive_bayesian_driver/ --select=W293 --count',\r\n            'critical': True\r\n        },\r\n        {\r\n            'name': 'Basic Syntax Check',\r\n            'cmd': 'python -m py_compile setup.py',\r\n            'critical': True\r\n        },\r\n        {\r\n            'name': 'Requirements Check',\r\n            'cmd': 'python -c \"import torch, numpy, matplotlib; print(\\'Dependencies OK\\')\"',\r\n            'critical': False\r\n        }\r\n    ]\r\n\r\n    passed = 0\r\n    critical_passed = 0\r\n    critical_total = sum(1 for check in checks if check['critical'])\r\n\r\n    for check in checks:\r\n        try:\r\n            result = subprocess.run(check['cmd'], shell=True, capture_output=True, text=True, check=True)\r\n            status = \"✅ PASS\"\r\n            passed += 1\r\n            if check['critical']:\r\n                critical_passed += 1\r\n        except subprocess.CalledProcessError:\r\n            status = \"❌ FAIL\"\r\n\r\n        critical_marker = \"🔴\" if check['critical'] else \"⚪\"\r\n        print(f\"{critical_marker} {check['name']}: {status}\")\r\n\r\n    print(f\"\\nResults: {passed}/{len(checks)} total, {critical_passed}/{critical_total} critical\")\r\n\r\n    if critical_passed == critical_total:\r\n        print(\"🎉 CI/CD READY - All critical checks passed!\")\r\n        return True\r\n    else:\r\n        print(\"⚠️  CI/CD NOT READY - Critical issues remain\")\r\n        return False\r\n\r\nif __name__ == \"__main__\":\r\n    success = check_cicd_readiness()\r\n    sys.exit(0 if success else 1)\r\n",
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nQuick CI/CD readiness check.\r\n\"\"\"\r\n\r\nimport subprocess\r\nimport sys\r\n\r\ndef check_cicd_readiness():\r\n    \"\"\"Quick check for CI/CD readiness.\"\"\"\r\n    print(\"🔍 Quick CI/CD Re...",
      "directory": ".copilot\\helpers\\diagnostics",
      "size_bytes": 1992,
      "is_binary": false,
      "last_modified": "2025-07-16 05:13:57",
      "path": ".copilot/helpers/diagnostics/cicd_check.py",
      "extension": ".py",
      "filename": "cicd_check.py"
    },
    {
      "size_kb": 4.64,
      "content": "#!/usr/bin/env python3\n\"\"\"\nComprehensive formatting scanner for the repository.\nDetects line ending issues, encoding problems, and whitespace issues.\n\"\"\"\n\nimport os\nimport re\nimport chardet\nfrom pathlib import Path\n\ndef check_file_encoding(file_path):\n    \"\"\"Check file encoding and detect potential issues.\"\"\"\n    try:\n        with open(file_path, 'rb') as f:\n            raw_data = f.read()\n\n        if not raw_data:\n            return \"empty\", None\n\n        detected = chardet.detect(raw_data)\n        encoding = detected.get('encoding', 'unknown')\n        confidence = detected.get('confidence', 0)\n\n        # Check for BOM\n        has_bom = raw_data.startswith(b'\\xef\\xbb\\xbf')\n\n        return encoding, confidence, has_bom\n    except Exception as e:\n        return f\"error: {e}\", None, None\n\ndef check_line_endings(file_path):\n    \"\"\"Check line ending consistency.\"\"\"\n    try:\n        with open(file_path, 'rb') as f:\n            content = f.read()\n\n        if not content:\n            return \"empty\"\n\n        crlf_count = content.count(b'\\r\\n')\n        lf_count = content.count(b'\\n') - crlf_count\n        cr_count = content.count(b'\\r') - crlf_count\n\n        endings = []\n        if crlf_count > 0:\n            endings.append(f\"CRLF({crlf_count})\")\n        if lf_count > 0:\n            endings.append(f\"LF({lf_count})\")\n        if cr_count > 0:\n            endings.append(f\"CR({cr_count})\")\n\n        if len(endings) > 1:\n            return f\"MIXED: {', '.join(endings)}\"\n        elif endings:\n            return endings[0]\n        else:\n            return \"NO_ENDINGS\"\n\n    except Exception as e:\n        return f\"error: {e}\"\n\ndef check_whitespace_issues(file_path):\n    \"\"\"Check for whitespace problems.\"\"\"\n    issues = []\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            lines = f.readlines()\n\n        for i, line in enumerate(lines, 1):\n            # Trailing whitespace\n            if line.rstrip('\\n\\r') != line.rstrip():\n                issues.append(f\"Line {i}: trailing whitespace\")\n\n            # Tabs mixed with spaces\n            if '\\t' in line and '    ' in line:\n                issues.append(f\"Line {i}: mixed tabs and spaces\")\n\n            # Non-ASCII characters that might cause issues\n            try:\n                line.encode('ascii')\n            except UnicodeEncodeError:\n                non_ascii = [c for c in line if ord(c) > 127]\n                if non_ascii:\n                    issues.append(f\"Line {i}: non-ASCII chars: {non_ascii[:3]}\")\n\n        return issues\n    except Exception as e:\n        return [f\"error reading file: {e}\"]\n\ndef scan_repository():\n    \"\"\"Scan the entire repository for formatting issues.\"\"\"\n    project_root = Path(\".\")\n    issues_found = {}\n\n    # File patterns to check\n    patterns = ['*.py', '*.txt', '*.md', '*.yml', '*.yaml', '*.cfg', '*.ini', '*.json']\n\n    print(\"🔍 Scanning repository for formatting issues...\\n\")\n\n    for pattern in patterns:\n        for file_path in project_root.rglob(pattern):\n            # Skip certain directories\n            if any(part in str(file_path) for part in ['.git', '__pycache__', '.pytest_cache', 'build', 'dist']):\n                continue\n\n            print(f\"Checking: {file_path}\")\n            file_issues = []\n\n            # Check encoding\n            encoding_info = check_file_encoding(file_path)\n            if encoding_info[0] not in ['utf-8', 'ascii'] or (len(encoding_info) > 2 and encoding_info[2]):\n                file_issues.append(f\"Encoding: {encoding_info}\")\n\n            # Check line endings\n            line_endings = check_line_endings(file_path)\n            if 'MIXED' in line_endings or 'CR(' in line_endings:\n                file_issues.append(f\"Line endings: {line_endings}\")\n\n            # Check whitespace issues for text files\n            if file_path.suffix in ['.py', '.txt', '.md', '.yml', '.yaml', '.cfg']:\n                whitespace_issues = check_whitespace_issues(file_path)\n                if whitespace_issues:\n                    file_issues.extend(whitespace_issues[:5])  # Limit to first 5 issues\n\n            if file_issues:\n                issues_found[str(file_path)] = file_issues\n\n    return issues_found\n\ndef main():\n    \"\"\"Main function to run the formatting scan.\"\"\"\n    issues = scan_repository()\n\n    if not issues:\n        print(\"✅ No formatting issues found!\")\n        return 0\n\n    print(f\"\\n❌ Found formatting issues in {len(issues)} files:\\n\")\n\n    for file_path, file_issues in issues.items():\n        print(f\"📁 {file_path}:\")\n        for issue in file_issues:\n            print(f\"   • {issue}\")\n        print()\n\n    print(f\"Total files with issues: {len(issues)}\")\n    return 1\n\nif __name__ == \"__main__\":\n    exit(main())\n",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"\nComprehensive formatting scanner for the repository.\nDetects line ending issues, encoding problems, and whitespace issues.\n\"\"\"\n\nimport os\nimport re\nimport chardet\nfrom pathl...",
      "directory": ".copilot\\helpers\\diagnostics",
      "size_bytes": 4747,
      "is_binary": false,
      "last_modified": "2025-07-16 11:21:07",
      "path": ".copilot/helpers/diagnostics/format_scanner.py",
      "extension": ".py",
      "filename": "format_scanner.py"
    },
    {
      "size_kb": 0.19,
      "content": "# Diagnostic Tools\n\nThis directory contains scripts for scanning, checking, and validating system health.\n\n- Format scanners\n- CI/CD readiness checks\n- Build validation tools\n- Health diagnostics\r\n",
      "content_preview": "# Diagnostic Tools\n\nThis directory contains scripts for scanning, checking, and validating system health.\n\n- Format scanners\n- CI/CD readiness checks\n- Build validation tools\n- Health diagnostics\r\n",
      "directory": ".copilot\\helpers\\diagnostics",
      "size_bytes": 197,
      "is_binary": false,
      "last_modified": "2025-08-08 01:07:19",
      "path": ".copilot/helpers/diagnostics/README.md",
      "extension": ".md",
      "filename": "README.md"
    },
    {
      "size_kb": 2.39,
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nQuick validation script to test if the package builds and imports correctly.\r\nThis helps identify CI/CD issues locally.\r\n\"\"\"\r\n\r\nimport sys\r\nimport traceback\r\n\r\ndef test_imports():\r\n    \"\"\"Test that all main modules can be imported.\"\"\"\r\n    print(\"Testing imports...\")\r\n\r\n    try:\r\n        import adaptive_bayesian_driver\r\n        print(\"✓ Main package import successful\")\r\n    except Exception as e:\r\n        print(f\"✗ Main package import failed: {e}\")\r\n        return False\r\n\r\n    try:\r\n        from adaptive_bayesian_driver.models import RecursiveBayesianLearner, SurpriseMeter\r\n        print(\"✓ Core models import successful\")\r\n    except Exception as e:\r\n        print(f\"✗ Core models import failed: {e}\")\r\n        return False\r\n\r\n    try:\r\n        from adaptive_bayesian_driver.environment import SceneRenderer, VolatilityController\r\n        print(\"✓ Environment modules import successful\")\r\n    except Exception as e:\r\n        print(f\"✗ Environment modules import failed: {e}\")\r\n        return False\r\n\r\n    try:\r\n        from adaptive_bayesian_driver.config import load_config\r\n        print(\"✓ Config module import successful\")\r\n    except Exception as e:\r\n        print(f\"✗ Config module import failed: {e}\")\r\n        return False\r\n\r\n    return True\r\n\r\ndef test_basic_functionality():\r\n    \"\"\"Test basic functionality without full computation.\"\"\"\r\n    print(\"\\nTesting basic functionality...\")\r\n\r\n    try:\r\n        from adaptive_bayesian_driver.models import SurpriseMeter\r\n        from adaptive_bayesian_driver.models.surprise import SurpriseType\r\n\r\n        # Test SurpriseMeter instantiation\r\n        meter = SurpriseMeter(mode=SurpriseType.EPISTEMIC)\r\n        print(\"✓ SurpriseMeter instantiation successful\")\r\n\r\n        return True\r\n    except Exception as e:\r\n        print(f\"✗ Basic functionality test failed: {e}\")\r\n        traceback.print_exc()\r\n        return False\r\n\r\ndef main():\r\n    \"\"\"Main validation function.\"\"\"\r\n    print(\"=== Build Validation ===\")\r\n\r\n    success = True\r\n\r\n    # Test imports\r\n    if not test_imports():\r\n        success = False\r\n\r\n    # Test basic functionality\r\n    if not test_basic_functionality():\r\n        success = False\r\n\r\n    if success:\r\n        print(\"\\n✅ All validation tests passed!\")\r\n        return 0\r\n    else:\r\n        print(\"\\n❌ Some validation tests failed!\")\r\n        return 1\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n",
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nQuick validation script to test if the package builds and imports correctly.\r\nThis helps identify CI/CD issues locally.\r\n\"\"\"\r\n\r\nimport sys\r\nimport traceback\r\n\r\ndef test_im...",
      "directory": ".copilot\\helpers\\diagnostics",
      "size_bytes": 2445,
      "is_binary": false,
      "last_modified": "2025-07-16 05:13:57",
      "path": ".copilot/helpers/diagnostics/validate_build.py",
      "extension": ".py",
      "filename": "validate_build.py"
    },
    {
      "size_kb": 4.65,
      "content": "#!/usr/bin/env python3\n\"\"\"\nComprehensive formatter to fix all repository formatting issues.\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\n\ndef fix_file_encoding_and_format(file_path):\n    \"\"\"Fix encoding and formatting issues in a file.\"\"\"\n    print(f\"Fixing: {file_path}\")\n\n    try:\n        # Try to read with various encodings\n        content = None\n        for encoding in ['utf-8', 'iso-8859-1', 'windows-1252', 'macroman']:\n            try:\n                with open(file_path, 'r', encoding=encoding) as f:\n                    content = f.read()\n                break\n            except UnicodeDecodeError:\n                continue\n\n        if content is None:\n            print(f\"  ❌ Could not decode {file_path}\")\n            return False\n\n        # Fix common non-ASCII characters in code\n        fixes = {\n            '->': '->',  # Arrow to dash-greater\n            '**2': '**2',  # Superscript 2\n            '+/-': '+/-',  # Plus-minus\n            'tau': 'tau',  # Greek tau\n        }\n\n        original_content = content\n        for old, new in fixes.items():\n            content = content.replace(old, new)\n\n        # Remove trailing whitespace from each line\n        lines = content.splitlines()\n        lines = [line.rstrip() for line in lines]\n        content = '\\n'.join(lines)\n\n        # Ensure file ends with newline if it's not empty\n        if content and not content.endswith('\\n'):\n            content += '\\n'\n\n        # Write back as UTF-8\n        with open(file_path, 'w', encoding='utf-8', newline='\\n') as f:\n            f.write(content)\n\n        if content != original_content:\n            print(f\"  ✅ Fixed formatting and encoding\")\n        else:\n            print(f\"  ✅ Already correct\")\n\n        return True\n\n    except Exception as e:\n        print(f\"  ❌ Error fixing {file_path}: {e}\")\n        return False\n\ndef restore_empty_files():\n    \"\"\"Restore content to important empty files.\"\"\"\n    empty_files_content = {\n        'src/__init__.py': '\"\"\"Main package initialization.\"\"\"\\n',\n        'src/environment/__init__.py': '\"\"\"Environment module.\"\"\"\\n',\n        'src/models/__init__.py': '\"\"\"Models module.\"\"\"\\n',\n        'src/utils/__init__.py': '\"\"\"Utilities module.\"\"\"\\n',\n        'src/tests/__init__.py': '\"\"\"Tests module.\"\"\"\\n',\n        'adaptive_bayesian_driver/models/utils_particle.py': '''\"\"\"\nParticle filter utilities for adaptive Bayesian learning.\n\"\"\"\n\nimport numpy as np\nimport torch\nfrom typing import Dict, List, Tuple, Optional, Callable\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# This file was restored after being corrupted\n# Content needs to be reimplemented based on project needs\n''',\n        'adaptive_bayesian_driver/utils/visualization.py': '''\"\"\"\nVisualization utilities for adaptive Bayesian learning.\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom typing import Dict, List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# This file was restored after being corrupted\n# Content needs to be reimplemented based on project needs\n''',\n    }\n\n    for file_path, content in empty_files_content.items():\n        if os.path.exists(file_path):\n            try:\n                with open(file_path, 'r') as f:\n                    current = f.read().strip()\n                if not current:  # File is empty\n                    with open(file_path, 'w', encoding='utf-8') as f:\n                        f.write(content)\n                    print(f\"✅ Restored: {file_path}\")\n            except Exception as e:\n                print(f\"❌ Error restoring {file_path}: {e}\")\n\ndef main():\n    \"\"\"Main formatting fix function.\"\"\"\n    print(\"🔧 Fixing repository formatting issues...\\n\")\n\n    # First restore empty files\n    print(\"📝 Restoring empty files...\")\n    restore_empty_files()\n    print()\n\n    # Fix all Python and text files\n    patterns = ['*.py', '*.md', '*.txt', '*.yml', '*.yaml']\n    project_root = Path(\".\")\n\n    fixed_count = 0\n    total_count = 0\n\n    for pattern in patterns:\n        for file_path in project_root.rglob(pattern):\n            # Skip certain directories\n            if any(part in str(file_path) for part in ['.git', '__pycache__', '.pytest_cache', 'build', 'dist', '.egg-info']):\n                continue\n\n            total_count += 1\n            if fix_file_encoding_and_format(file_path):\n                fixed_count += 1\n\n    print(f\"\\n🎉 Fixed {fixed_count}/{total_count} files\")\n    print(\"\\n✅ Repository formatting cleanup complete!\")\n    print(\"\\nRecommendations:\")\n    print(\"1. Run 'git status' to see what changed\")\n    print(\"2. Test your code after these fixes\")\n    print(\"3. Run 'python -m flake8 . --count' to check for remaining issues\")\n\nif __name__ == \"__main__\":\n    main()\n",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"\nComprehensive formatter to fix all repository formatting issues.\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\n\ndef fix_file_encoding_and_format(file_path):\n    \"\"\"Fix e...",
      "directory": ".copilot\\helpers\\fixing",
      "size_bytes": 4757,
      "is_binary": false,
      "last_modified": "2025-07-16 11:21:07",
      "path": ".copilot/helpers/fixing/fix_formatting.py",
      "extension": ".py",
      "filename": "fix_formatting.py"
    },
    {
      "size_kb": 0.16,
      "content": "# Fixing Tools\n\nThis directory contains automated repair and cleanup utilities.\n\n- Formatting fixers\n- Whitespace cleanup\n- Automated repairs\n- Code normalization\r\n",
      "content_preview": "# Fixing Tools\n\nThis directory contains automated repair and cleanup utilities.\n\n- Formatting fixers\n- Whitespace cleanup\n- Automated repairs\n- Code normalization\r\n",
      "directory": ".copilot\\helpers\\fixing",
      "size_bytes": 164,
      "is_binary": false,
      "last_modified": "2025-08-08 01:07:19",
      "path": ".copilot/helpers/fixing/README.md",
      "extension": ".md",
      "filename": "README.md"
    },
    {
      "size_kb": 14.38,
      "content": "# PowerShell 7+ Legacy .copilot Directory Modernization Script\n# Brings legacy chatbot/copilot structure in line with current practices\n\nparam(\n    [string]$SourceDir = $PWD.Path,\n    [string]$BackupSuffix = \"_backup_$(Get-Date -Format 'yyyyMMdd_HHmmss')\",\n    [switch]$WhatIf,\n    [switch]$Verbose\n)\n\n# Modern .copilot structure definition\n$ModernStructure = @{\n    '.copilot' = @{\n        'helpers'   = @{\n            'diagnostics' = @('*_scanner.py', '*_check.py', '*_diagnostic.py', 'format_scanner.py', 'cicd_check.py', 'validate_*.py')\n            'fixing'      = @('*_fix.py', '*_cleanup.py', 'fix_*.py', 'clean_*.py')\n            'testing'     = @('*_test.py', 'test_*.py', 'comprehensive_test.py', 'run_*_tests.py')\n            'organize'    = @('organize*.py', '*_organizer.py', 'organize.py')\n        }\n        'ForReview' = @()\n    }\n}\n\n# File classification rules\n$ClassificationRules = @{\n    'diagnostics' = @(\n        @{ Pattern = '*scanner*'; Weight = 10 }\n        @{ Pattern = '*check*'; Weight = 10 }\n        @{ Pattern = '*diagnostic*'; Weight = 10 }\n        @{ Pattern = '*validate*'; Weight = 8 }\n        @{ Pattern = '*ci*'; Weight = 6 }\n        @{ Pattern = 'format_scanner.py'; Weight = 15 }\n        @{ Pattern = 'cicd_check.py'; Weight = 15 }\n    )\n    'fixing'      = @(\n        @{ Pattern = '*fix*'; Weight = 10 }\n        @{ Pattern = '*cleanup*'; Weight = 10 }\n        @{ Pattern = '*clean*'; Weight = 8 }\n        @{ Pattern = 'fix_formatting.py'; Weight = 15 }\n    )\n    'testing'     = @(\n        @{ Pattern = '*test*'; Weight = 10 }\n        @{ Pattern = 'comprehensive_test.py'; Weight = 15 }\n        @{ Pattern = 'run_*_tests.py'; Weight = 12 }\n    )\n    'organize'    = @(\n        @{ Pattern = 'organize*'; Weight = 15 }\n        @{ Pattern = '*organizer*'; Weight = 10 }\n    )\n}\n\nfunction Write-ModernizationLog {\n    param(\n        [string]$Message,\n        [string]$Level = \"INFO\",\n        [ConsoleColor]$Color = \"White\"\n    )\n\n    $timestamp = Get-Date -Format \"yyyy-MM-dd HH:mm:ss\"\n    $logMessage = \"[$timestamp] [$Level] $Message\"\n\n    if ($Verbose -or $Level -eq \"ERROR\" -or $Level -eq \"WARNING\") {\n        Write-Host $logMessage -ForegroundColor $Color\n    }\n\n    # Also log to file\n    $logMessage | Out-File -FilePath \"$(Join-Path $SourceDir 'modernization.log')\" -Append -Encoding UTF8\n}\n\nfunction Get-FileClassification {\n    param(\n        [System.IO.FileInfo]$File\n    )\n\n    $bestMatch = $null\n    $highestScore = 0\n\n    foreach ($category in $ClassificationRules.Keys) {\n        foreach ($rule in $ClassificationRules[$category]) {\n            if ($File.Name -like $rule.Pattern) {\n                if ($rule.Weight -gt $highestScore) {\n                    $highestScore = $rule.Weight\n                    $bestMatch = $category\n                }\n            }\n        }\n    }\n\n    return @{\n        Category   = $bestMatch\n        Confidence = $highestScore\n    }\n}\n\nfunction Test-IsRelevantFile {\n    param(\n        [System.IO.FileInfo]$File\n    )\n\n    # Check if file is relevant based on various criteria\n    $relevancyScore = 0\n\n    # PowerShell files are generally relevant\n    if ($File.Extension -eq '.ps1' -or $File.Extension -eq '.psm1') {\n        $relevancyScore += 10\n    }\n\n    # Python files with specific patterns\n    if ($File.Extension -eq '.py') {\n        $relevancyScore += 5\n    }\n\n    # Configuration and documentation files\n    if ($File.Extension -in @('.md', '.yml', '.yaml', '.json', '.txt')) {\n        $relevancyScore += 3\n    }\n\n    # Check for helper/utility keywords in filename\n    $utilityKeywords = @('helper', 'utility', 'tool', 'script', 'fix', 'test', 'diagnostic', 'scanner', 'check')\n    foreach ($keyword in $utilityKeywords) {\n        if ($File.BaseName -like \"*$keyword*\") {\n            $relevancyScore += 2\n        }\n    }\n\n    # Files that are likely irrelevant\n    $irrelevantPatterns = @('temp*', 'tmp*', '*.bak', '*.old', '*backup*', '*.cache')\n    foreach ($pattern in $irrelevantPatterns) {\n        if ($File.Name -like $pattern) {\n            $relevancyScore -= 10\n        }\n    }\n\n    return $relevancyScore -gt 2\n}\n\nfunction New-ModernDirectoryStructure {\n    param(\n        [string]$BasePath\n    )\n\n    Write-ModernizationLog \"Creating modern .copilot directory structure...\" \"INFO\" \"Cyan\"\n\n    $copilotPath = Join-Path $BasePath \".copilot\"\n    $helpersPath = Join-Path $copilotPath \"helpers\"\n\n    # Create main directories\n    $directories = @(\n        $copilotPath,\n        $helpersPath,\n        (Join-Path $helpersPath \"diagnostics\"),\n        (Join-Path $helpersPath \"fixing\"),\n        (Join-Path $helpersPath \"testing\"),\n        (Join-Path $helpersPath \"organize\"),\n        (Join-Path $copilotPath \"ForReview\")\n    )\n\n    foreach ($dir in $directories) {\n        if (!(Test-Path $dir)) {\n            if (!$WhatIf) {\n                & New-Item -Path $dir -ItemType Directory -Force | Out-Null\n            }\n            Write-ModernizationLog \"Created directory: $dir\" \"INFO\" \"Green\"\n        }\n        else {\n            Write-ModernizationLog \"Directory already exists: $dir\" \"INFO\" \"Yellow\"\n        }\n    }\n\n    # Create README files for each directory\n    $readmeContent = @{\n        'diagnostics' = \"# Diagnostic Tools`n`nThis directory contains scripts for scanning, checking, and validating system health.`n`n- Format scanners`n- CI/CD readiness checks`n- Build validation tools`n- Health diagnostics\"\n        'fixing'      = \"# Fixing Tools`n`nThis directory contains automated repair and cleanup utilities.`n`n- Formatting fixers`n- Whitespace cleanup`n- Automated repairs`n- Code normalization\"\n        'testing'     = \"# Testing Utilities`n`nThis directory contains test runners and debugging tools.`n`n- Comprehensive test runners`n- Specific test utilities`n- Debug helpers`n- Test orchestration\"\n        'organize'    = \"# Organization Tools`n`nThis directory contains file organization and structure management tools.`n`n- Directory organizers`n- File classification tools`n- Structure maintenance\"\n    }\n\n    foreach ($category in $readmeContent.Keys) {\n        $readmePath = Join-Path (Join-Path $helpersPath $category) \"README.md\"\n        if (!(Test-Path $readmePath)) {\n            if (!$WhatIf) {\n                $readmeContent[$category] | Out-File -FilePath $readmePath -Encoding UTF8\n            }\n            Write-ModernizationLog \"Created README: $readmePath\" \"INFO\" \"Green\"\n        }\n    }\n}\n\nfunction Move-LegacyFiles {\n    param(\n        [string]$SourcePath,\n        [string]$DestinationBase\n    )\n\n    Write-ModernizationLog \"Analyzing and moving legacy files...\" \"INFO\" \"Cyan\"\n\n    # Get all files from legacy directories\n    $legacyPaths = @('_chatbot', '_diagnostics', '_fixing', '_testing', 'debugging', 'validation', 'reporting')\n    $allFiles = @()\n\n    foreach ($legacyPath in $legacyPaths) {\n        $fullLegacyPath = Join-Path $SourcePath $legacyPath\n        if (Test-Path $fullLegacyPath) {\n            Write-ModernizationLog \"Scanning legacy directory: $legacyPath\" \"INFO\" \"Gray\"\n            $files = & Get-ChildItem -Path $fullLegacyPath -Recurse -File\n            $allFiles += $files\n        }\n    }\n\n    # Also scan root directory for orphaned files\n    $rootFiles = & Get-ChildItem -Path $SourcePath -File | Where-Object {\n        $_.Extension -in @('.py', '.ps1', '.psm1', '.md') -and\n        $_.Name -notlike 'README*' -and\n        $_.Name -notlike 'modernization*'\n    }\n    $allFiles += $rootFiles\n\n    Write-ModernizationLog \"Found $($allFiles.Count) files to analyze\" \"INFO\" \"White\"\n\n    $moveOperations = @{\n        'diagnostics' = @()\n        'fixing'      = @()\n        'testing'     = @()\n        'organize'    = @()\n        'ForReview'   = @()\n    }\n\n    # Classify each file\n    foreach ($file in $allFiles) {\n        $classification = Get-FileClassification -File $file\n        $isRelevant = Test-IsRelevantFile -File $file\n\n        if ($classification.Category -and $isRelevant) {\n            $moveOperations[$classification.Category] += @{\n                File       = $file\n                Confidence = $classification.Confidence\n                Reason     = \"Classified as $($classification.Category) with confidence $($classification.Confidence)\"\n            }\n            Write-ModernizationLog \"Classified '$($file.Name)' as $($classification.Category) (confidence: $($classification.Confidence))\" \"INFO\" \"Gray\"\n        }\n        else {\n            $reason = if (!$isRelevant) { \"Low relevancy score\" } else { \"No clear classification\" }\n            $moveOperations['ForReview'] += @{\n                File       = $file\n                Confidence = 0\n                Reason     = $reason\n            }\n            Write-ModernizationLog \"Moving '$($file.Name)' to ForReview: $reason\" \"WARNING\" \"Yellow\"\n        }\n    }\n\n    # Execute move operations\n    foreach ($category in $moveOperations.Keys) {\n        $targetDir = if ($category -eq 'ForReview') {\n            Join-Path $DestinationBase \".copilot\\ForReview\"\n        }\n        else {\n            Join-Path $DestinationBase \".copilot\\helpers\\$category\"\n        }\n\n        foreach ($operation in $moveOperations[$category]) {\n            $sourceFile = $operation.File\n            $targetPath = Join-Path $targetDir $sourceFile.Name\n\n            # Handle name conflicts\n            if (Test-Path $targetPath) {\n                $counter = 1\n                $baseName = [System.IO.Path]::GetFileNameWithoutExtension($sourceFile.Name)\n                $extension = $sourceFile.Extension\n                do {\n                    $newName = \"$baseName`_$counter$extension\"\n                    $targetPath = Join-Path $targetDir $newName\n                    $counter++\n                } while (Test-Path $targetPath)\n\n                Write-ModernizationLog \"Renamed due to conflict: $($sourceFile.Name) -> $(Split-Path $targetPath -Leaf)\" \"WARNING\" \"Yellow\"\n            }\n\n            if (!$WhatIf) {\n                try {\n                    & Move-Item -Path $sourceFile.FullName -Destination $targetPath -Force\n                    Write-ModernizationLog \"Moved: $($sourceFile.FullName) -> $targetPath\" \"INFO\" \"Green\"\n                }\n                catch {\n                    Write-ModernizationLog \"Failed to move $($sourceFile.FullName): $($_.Exception.Message)\" \"ERROR\" \"Red\"\n                }\n            }\n            else {\n                Write-ModernizationLog \"WOULD MOVE: $($sourceFile.FullName) -> $targetPath\" \"INFO\" \"Magenta\"\n            }\n        }\n    }\n}\n\nfunction Remove-EmptyLegacyDirectories {\n    param(\n        [string]$BasePath\n    )\n\n    Write-ModernizationLog \"Cleaning up empty legacy directories...\" \"INFO\" \"Cyan\"\n\n    $legacyPaths = @('_chatbot', '_diagnostics', '_fixing', '_testing', 'debugging', 'validation', 'reporting')\n\n    foreach ($legacyPath in $legacyPaths) {\n        $fullLegacyPath = Join-Path $BasePath $legacyPath\n        if (Test-Path $fullLegacyPath) {\n            # Check if directory is empty (recursively)\n            $remainingItems = & Get-ChildItem -Path $fullLegacyPath -Recurse -Force\n\n            if ($remainingItems.Count -eq 0) {\n                if (!$WhatIf) {\n                    & Remove-Item -Path $fullLegacyPath -Recurse -Force\n                    Write-ModernizationLog \"Removed empty legacy directory: $legacyPath\" \"INFO\" \"Green\"\n                }\n                else {\n                    Write-ModernizationLog \"WOULD REMOVE empty directory: $legacyPath\" \"INFO\" \"Magenta\"\n                }\n            }\n            else {\n                Write-ModernizationLog \"Legacy directory $legacyPath still contains $($remainingItems.Count) items - keeping\" \"INFO\" \"Yellow\"\n            }\n        }\n    }\n}\n\nfunction New-CopilotInstructionsFile {\n    param(\n        [string]$BasePath\n    )\n\n    $instructionsPath = Join-Path $BasePath \".copilot\\copilot-instructions.md\"\n\n    $instructions = @\"\n# .copilot Directory Instructions\n\nThis directory contains modernized helper tools and utilities organized according to current PowerShell Central practices.\n\n## Directory Structure\n\n- **helpers/diagnostics/** - Scanning, checking, and validation tools\n- **helpers/fixing/** - Automated repair and cleanup utilities\n- **helpers/testing/** - Test runners and debugging tools\n- **helpers/organize/** - File organization and structure management\n- **ForReview/** - Files requiring manual review and classification\n\n## PowerShell Standards\n\n- Use PowerShell 7+ syntax with complete bracket closure\n- Implement proper string interpolation with `$()` syntax\n- Include comprehensive error handling and logging\n- Follow strict parameter validation and type hints\n- Provide full replacement code, not snippets\n\n## Usage Guidelines\n\n1. **Diagnostics First** - Always run diagnostic tools before making changes\n2. **Fix Systematically** - Use fixing tools to normalize code formatting and structure\n3. **Test Thoroughly** - Run comprehensive tests after any changes\n4. **Organize Continuously** - Maintain clean directory structure with organization tools\n\n## File Naming Conventions\n\n- `*_scanner.ps1` - Analysis and detection tools\n- `*_check.ps1` - Quick verification scripts\n- `*_fix.ps1` - Automated repair utilities\n- `*_test.ps1` - Test utilities and runners\n- `*_organizer.ps1` - Structure management tools\n\nThis structure supports scalable, maintainable PowerShell development workflows with AI assistance integration.\n\"@\n\n    if (!$WhatIf) {\n        $instructions | Out-File -FilePath $instructionsPath -Encoding UTF8\n    }\n\n    Write-ModernizationLog \"Created copilot instructions file: $instructionsPath\" \"INFO\" \"Green\"\n}\n\n# Main execution\ntry {\n    Write-ModernizationLog \"Starting .copilot directory modernization...\" \"INFO\" \"Cyan\"\n    Write-ModernizationLog \"Source directory: $SourceDir\" \"INFO\" \"White\"\n\n    if ($WhatIf) {\n        Write-ModernizationLog \"Running in WhatIf mode - no changes will be made\" \"INFO\" \"Magenta\"\n    }\n\n    # Step 1: Create modern directory structure\n    New-ModernDirectoryStructure -BasePath $SourceDir\n\n    # Step 2: Move and classify legacy files\n    Move-LegacyFiles -SourcePath $SourceDir -DestinationBase $SourceDir\n\n    # Step 3: Clean up empty legacy directories\n    Remove-EmptyLegacyDirectories -BasePath $SourceDir\n\n    # Step 4: Create copilot instructions\n    New-CopilotInstructionsFile -BasePath $SourceDir\n\n    Write-ModernizationLog \"Modernization complete!\" \"INFO\" \"Green\"\n    Write-ModernizationLog \"Review the ForReview directory for files needing manual classification\" \"INFO\" \"Yellow\"\n    Write-ModernizationLog \"Log file created: $(Join-Path $SourceDir 'modernization.log')\" \"INFO\" \"Gray\"\n\n}\ncatch {\n    Write-ModernizationLog \"Error during modernization: $($_.Exception.Message)\" \"ERROR\" \"Red\"\n    throw\n}\n",
      "content_preview": "# PowerShell 7+ Legacy .copilot Directory Modernization Script\n# Brings legacy chatbot/copilot structure in line with current practices\n\nparam(\n    [string]$SourceDir = $PWD.Path,\n    [string]$BackupS...",
      "directory": ".copilot\\helpers\\fixing",
      "size_bytes": 14729,
      "is_binary": false,
      "last_modified": "2025-08-08 01:06:21",
      "path": ".copilot/helpers/fixing/RefactorCleanup.ps1",
      "extension": ".ps1",
      "filename": "RefactorCleanup.ps1"
    },
    {
      "size_kb": 1.79,
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nChatbot file organization helper.\r\nUse this to ensure all chatbot-created files are properly organized.\r\n\"\"\"\r\n\r\nimport os\r\nfrom pathlib import Path\r\n\r\ndef organize_chatbot_files():\r\n    \"\"\"Organize chatbot-created files into proper directories.\"\"\"\r\n\r\n    # Define the organizational structure\r\n    structure = {\r\n        '_chatbot/_diagnostics': [\r\n            'format_scanner.py',\r\n            'cicd_check.py',\r\n            'diagnostic.py',\r\n            'validate_build.py',\r\n            'ci_test.py',\r\n            'final_validation.py',\r\n            '*_check.py',\r\n            '*_scanner.py',\r\n            '*_diagnostic.py'\r\n        ],\r\n        '_chatbot/_fixing': [\r\n            'fix_formatting.py',\r\n            'clean_whitespace.py',\r\n            '*_fix.py',\r\n            '*_cleanup.py'\r\n        ]\r\n    }\r\n\r\n    project_root = Path('.')\r\n\r\n    # Create directories if they don't exist\r\n    for dir_path in structure.keys():\r\n        os.makedirs(dir_path, exist_ok=True)\r\n        print(f\"✓ Directory ready: {dir_path}\")\r\n\r\n    print(\"\\n📁 Chatbot file organization structure ready!\")\r\n    print(\"\\nFor future AI assistants:\")\r\n    print(\"- Place diagnostic tools in: _chatbot/_diagnostics/\")\r\n    print(\"- Place fixing tools in: _chatbot/_fixing/\")\r\n    print(\"- Never create scripts in project root\")\r\n    print(\"- Always use descriptive names\")\r\n\r\ndef get_chatbot_dir(script_type):\r\n    \"\"\"Get the appropriate directory for a chatbot script type.\"\"\"\r\n    mapping = {\r\n        'diagnostic': '_chatbot/_diagnostics',\r\n        'fix': '_chatbot/_fixing',\r\n        'check': '_chatbot/_diagnostics',\r\n        'scan': '_chatbot/_diagnostics',\r\n        'cleanup': '_chatbot/_fixing'\r\n    }\r\n    return mapping.get(script_type, '_chatbot')\r\n\r\nif __name__ == \"__main__\":\r\n    organize_chatbot_files()\r\n",
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nChatbot file organization helper.\r\nUse this to ensure all chatbot-created files are properly organized.\r\n\"\"\"\r\n\r\nimport os\r\nfrom pathlib import Path\r\n\r\ndef organize_chatbot...",
      "directory": ".copilot\\helpers\\organize",
      "size_bytes": 1831,
      "is_binary": false,
      "last_modified": "2025-07-16 05:13:57",
      "path": ".copilot/helpers/organize/organize.py",
      "extension": ".py",
      "filename": "organize.py"
    },
    {
      "size_kb": 0.17,
      "content": "# Organization Tools\n\nThis directory contains file organization and structure management tools.\n\n- Directory organizers\n- File classification tools\n- Structure maintenance\r\n",
      "content_preview": "# Organization Tools\n\nThis directory contains file organization and structure management tools.\n\n- Directory organizers\n- File classification tools\n- Structure maintenance\r\n",
      "directory": ".copilot\\helpers\\organize",
      "size_bytes": 173,
      "is_binary": false,
      "last_modified": "2025-08-08 01:07:19",
      "path": ".copilot/helpers/organize/README.md",
      "extension": ".md",
      "filename": "README.md"
    },
    {
      "size_kb": 2.26,
      "content": "# PowerShell 7+ Standards for Copilot Generation\n# This file demonstrates the coding standards expected in this project\n\n#Requires -Version 7.0\n\n[CmdletBinding()]\nparam(\n    [Parameter(Mandatory = $true)]\n    [ValidateNotNullOrEmpty()]\n    [string]$ProjectPath,\n\n    [Parameter()]\n    [ValidateSet('Development', 'Production', 'Testing')]\n    [string]$Environment = 'Development'\n)\n\nfunction Invoke-CopilotStandardsExample {\n    <#\n    .SYNOPSIS\n        Example function demonstrating PowerShell 7+ standards for Copilot\n\n    .DESCRIPTION\n        This function serves as a template for all PowerShell code generated\n        by Copilot, ensuring consistent syntax and best practices\n\n    .PARAMETER InputObject\n        The object to process with strict typing\n\n    .EXAMPLE\n        Invoke-CopilotStandardsExample -InputObject $myData\n    #>\n\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory = $true, ValueFromPipeline = $true)]\n        [PSCustomObject]$InputObject\n    )\n\n    begin {\n        Write-Information \"Starting processing with environment: $Environment\" -InformationAction Continue\n        $processedCount = 0\n    }\n\n    process {\n        try {\n            # Demonstrate proper string interpolation\n            $message = \"Processing item: $($InputObject.Name) at path: $($InputObject.Path)\"\n            Write-Verbose $message\n\n            # Demonstrate proper hashtable splatting\n            $parameters = @{\n                Path = $InputObject.Path\n                Recurse = $true\n                Force = $Environment -eq 'Production'\n            }\n\n            # Demonstrate call operator usage\n            $results = & Get-ChildItem @parameters\n\n            # Proper nested bracket structure\n            if ($results) {\n                foreach ($result in $results) {\n                    if ($result.Extension -in @('.ps1', '.py', '.md')) {\n                        Write-Output \"Found valid file: $($result.FullName)\"\n                        $processedCount++\n                    }\n                }\n            }\n\n        }\n        catch {\n            Write-Error \"Failed to process $($InputObject.Name): $($_.Exception.Message)\"\n            throw\n        }\n    }\n\n    end {\n        Write-Information \"Processed $processedCount items successfully\" -InformationAction Continue\n    }\n}\n",
      "content_preview": "# PowerShell 7+ Standards for Copilot Generation\n# This file demonstrates the coding standards expected in this project\n\n#Requires -Version 7.0\n\n[CmdletBinding()]\nparam(\n    [Parameter(Mandatory = $tr...",
      "directory": ".copilot\\helpers",
      "size_bytes": 2310,
      "is_binary": false,
      "last_modified": "2025-08-08 01:48:47",
      "path": ".copilot/helpers/powershell-standards.ps1",
      "extension": ".ps1",
      "filename": "powershell-standards.ps1"
    },
    {
      "size_kb": 2.18,
      "content": "#!/usr/bin/env python3\n\"\"\"Comprehensive test runner to identify failing tests.\"\"\"\n\nimport sys\nimport os\nimport subprocess\nimport traceback\n\n# Add the project root to the Python path\nsys.path.insert(0, os.path.abspath('.'))\n\ndef run_test(test_name):\n    \"\"\"Run a single test and capture output.\"\"\"\n    try:\n        result = subprocess.run(\n            [sys.executable, '-m', 'pytest', test_name, '-v'],\n            capture_output=True,\n            text=True,\n            cwd=os.path.abspath('.')\n        )\n\n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr\n        }\n    except Exception as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }\n\ndef main():\n    \"\"\"Run all the failing tests mentioned.\"\"\"\n    failing_tests = [\n        \"tests/test_environment.py::TestSceneRenderer::test_render_scene\",\n        \"tests/test_recursive_bayes.py::TestSurpriseMeter::test_baseline_update\",\n        \"tests/test_recursive_bayes.py::TestSurpriseMeter::test_chi2_surprise\",\n        \"tests/test_recursive_bayes.py::TestSurpriseMeter::test_normalized_surprise\",\n        \"tests/test_recursive_bayes.py::TestSurpriseMeter::test_reconstruction_surprise\",\n    ]\n\n    results = {}\n\n    for test in failing_tests:\n        print(f\"\\n{'='*60}\")\n        print(f\"Running test: {test}\")\n        print('='*60)\n\n        result = run_test(test)\n        results[test] = result\n\n        if result['returncode'] == 0:\n            print(\"✅ PASSED\")\n        else:\n            print(\"❌ FAILED\")\n            print(f\"Return code: {result['returncode']}\")\n            print(f\"STDOUT:\\n{result['stdout']}\")\n            print(f\"STDERR:\\n{result['stderr']}\")\n\n    # Summary\n    print(f\"\\n{'='*60}\")\n    print(\"SUMMARY\")\n    print('='*60)\n\n    passed = 0\n    failed = 0\n\n    for test, result in results.items():\n        status = \"✅ PASSED\" if result['returncode'] == 0 else \"❌ FAILED\"\n        print(f\"{test}: {status}\")\n        if result['returncode'] == 0:\n            passed += 1\n        else:\n            failed += 1\n\n    print(f\"\\nTotal: {passed} passed, {failed} failed\")\n\nif __name__ == \"__main__\":\n    main()\n",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"Comprehensive test runner to identify failing tests.\"\"\"\n\nimport sys\nimport os\nimport subprocess\nimport traceback\n\n# Add the project root to the Python path\nsys.path.insert(0,...",
      "directory": ".copilot\\helpers\\testing",
      "size_bytes": 2230,
      "is_binary": false,
      "last_modified": "2025-07-16 11:21:07",
      "path": ".copilot/helpers/testing/comprehensive_test.py",
      "extension": ".py",
      "filename": "comprehensive_test.py"
    },
    {
      "size_kb": 0.17,
      "content": "# Testing Utilities\n\nThis directory contains test runners and debugging tools.\n\n- Comprehensive test runners\n- Specific test utilities\n- Debug helpers\n- Test orchestration\r\n",
      "content_preview": "# Testing Utilities\n\nThis directory contains test runners and debugging tools.\n\n- Comprehensive test runners\n- Specific test utilities\n- Debug helpers\n- Test orchestration\r\n",
      "directory": ".copilot\\helpers\\testing",
      "size_bytes": 173,
      "is_binary": false,
      "last_modified": "2025-08-08 01:07:19",
      "path": ".copilot/helpers/testing/README.md",
      "extension": ".md",
      "filename": "README.md"
    },
    {
      "size_kb": 1.79,
      "content": "#!/usr/bin/env python3\n\"\"\"Run the specific failing tests directly.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nimport unittest\nfrom tests.test_environment import TestSceneRenderer\nfrom tests.test_recursive_bayes import TestSurpriseMeter\n\ndef main():\n    # Test SceneRenderer\n    print(\"=\"*50)\n    print(\"Testing SceneRenderer\")\n    print(\"=\"*50)\n\n    try:\n        suite = unittest.TestLoader().loadTestsFromTestCase(TestSceneRenderer)\n        runner = unittest.TextTestRunner(verbosity=2)\n        result = runner.run(suite)\n\n        if result.failures:\n            print(\"\\nFAILURES:\")\n            for test, traceback in result.failures:\n                print(f\"Test: {test}\")\n                print(f\"Traceback: {traceback}\")\n\n        if result.errors:\n            print(\"\\nERRORS:\")\n            for test, traceback in result.errors:\n                print(f\"Test: {test}\")\n                print(f\"Traceback: {traceback}\")\n\n    except Exception as e:\n        print(f\"Error running SceneRenderer tests: {e}\")\n\n    # Test SurpriseMeter\n    print(\"\\n\" + \"=\"*50)\n    print(\"Testing SurpriseMeter\")\n    print(\"=\"*50)\n\n    try:\n        suite = unittest.TestLoader().loadTestsFromTestCase(TestSurpriseMeter)\n        runner = unittest.TextTestRunner(verbosity=2)\n        result = runner.run(suite)\n\n        if result.failures:\n            print(\"\\nFAILURES:\")\n            for test, traceback in result.failures:\n                print(f\"Test: {test}\")\n                print(f\"Traceback: {traceback}\")\n\n        if result.errors:\n            print(\"\\nERRORS:\")\n            for test, traceback in result.errors:\n                print(f\"Test: {test}\")\n                print(f\"Traceback: {traceback}\")\n\n    except Exception as e:\n        print(f\"Error running SurpriseMeter tests: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"Run the specific failing tests directly.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nimport unittest\nfrom tests.test_environment import TestSceneRender...",
      "directory": ".copilot\\helpers\\testing",
      "size_bytes": 1834,
      "is_binary": false,
      "last_modified": "2025-07-16 11:21:07",
      "path": ".copilot/helpers/testing/run_specific_tests.py",
      "extension": ".py",
      "filename": "run_specific_tests.py"
    },
    {
      "size_kb": 1.97,
      "content": "#!/usr/bin/env python3\n\"\"\"Test AdaptiveParticleFilter reset functionality.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nfrom adaptive_bayesian_driver.models import AdaptiveParticleFilter\nimport numpy as np\n\ndef test_particle_filter_reset():\n    \"\"\"Test that AdaptiveParticleFilter reset works without IndexError.\"\"\"\n    print(\"Testing AdaptiveParticleFilter reset...\")\n\n    # Create config\n    config = {\n        'n_particles': 50,\n        'state_dim': 3,\n        'observation_dim': 2,\n        'process_noise_std': 0.1,\n        'observation_noise_std': 0.1,\n        'adaptive_noise': True,\n        'adaptive_resampling': True\n    }\n\n    # Create particle filter\n    pf = AdaptiveParticleFilter(config)\n\n    print(f\"Initial particles shape: {pf.particles.shape}\")\n    print(f\"Initial weights shape: {pf.weights.shape}\")\n\n    # Run some updates to trigger potential resampling\n    for i in range(5):\n        observation = np.random.randn(2)\n        print(f\"Update {i+1}: observation = {observation}\")\n        try:\n            result = pf.update(observation)\n            print(f\"  Update successful, ESS: {result['effective_sample_size']:.2f}\")\n        except Exception as e:\n            print(f\"  Update failed: {e}\")\n            return False\n\n    # Test reset\n    print(\"\\nTesting reset...\")\n    try:\n        pf.reset()\n        print(\"Reset successful!\")\n\n        # Verify reset worked\n        assert len(pf.state_history) == 0, \"State history should be empty after reset\"\n        assert abs(np.sum(pf.weights) - 1.0) < 1e-6, \"Weights should sum to 1.0\"\n        assert pf.particles.shape == (50, 3), \"Particles shape should be preserved\"\n\n        print(\"All reset checks passed!\")\n        return True\n\n    except Exception as e:\n        print(f\"Reset failed: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    success = test_particle_filter_reset()\n    if success:\n        print(\"\\n✅ All tests passed!\")\n    else:\n        print(\"\\n❌ Tests failed!\")\n    sys.exit(0 if success else 1)\n",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"Test AdaptiveParticleFilter reset functionality.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nfrom adaptive_bayesian_driver.models import AdaptivePartic...",
      "directory": ".copilot\\helpers\\testing",
      "size_bytes": 2016,
      "is_binary": false,
      "last_modified": "2025-07-16 11:21:07",
      "path": ".copilot/helpers/testing/test_particle_filter.py",
      "extension": ".py",
      "filename": "test_particle_filter.py"
    },
    {
      "size_kb": 1.04,
      "content": "#!/usr/bin/env python3\n\"\"\"Test SceneRenderer directly to understand the issue.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nfrom adaptive_bayesian_driver.environment import SceneRenderer\n\n# Test configuration\nconfig = {\n    'visual_parameters': {\n        'scene_size': [64, 64],\n        'safe_background_color': [50, 50, 50],\n        'danger_background_color': [255, 215, 0]\n    }\n}\n\nrenderer = SceneRenderer(config)\n\n# Test parameters\nparams = {\n    'has_danger': True,\n    'orientation_angle': 45.0,\n    'difficulty': 0.2,\n    'correct_direction': 'right'\n}\n\nscene, metadata = renderer.render_scene(params)\n\nprint(\"Test Results:\")\nprint(f\"Scene shape: {scene.shape}\")\nprint(f\"Scene type: {type(scene)}\")\nprint(f\"Expected shape: (1, 64, 64, 3)\")\nprint(f\"Assertion would pass: {scene.shape == (1, 64, 64, 3)}\")\nprint(f\"Metadata: {metadata}\")\n\n# Test if it's actually (1, 64, 64) without the 3\nprint(f\"Scene shape equals (1, 64, 64): {scene.shape == (1, 64, 64)}\")\nprint(f\"Scene shape equals (1, 64, 64, 3): {scene.shape == (1, 64, 64, 3)}\")\n",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"Test SceneRenderer directly to understand the issue.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nfrom adaptive_bayesian_driver.environment import Scene...",
      "directory": ".copilot\\helpers\\testing",
      "size_bytes": 1063,
      "is_binary": false,
      "last_modified": "2025-07-16 11:21:07",
      "path": ".copilot/helpers/testing/test_scene_debug.py",
      "extension": ".py",
      "filename": "test_scene_debug.py"
    },
    {
      "size_kb": 0.41,
      "content": "## Implementation Checklist\n### Memory Orchestration Core\n- [ ] Implement session boundary detection in PowerShell\n- [ ] Create memory scope management in Python-\n- [ ] Develop context injection API - [ ] Implement console capture hooks in PowerShell\n- [ ] Create adaptive assistance coordination system\n- [ ] Develop cross-project pattern learning integration\n- [ ] Implement production-grade reliability features\n",
      "content_preview": "## Implementation Checklist\n### Memory Orchestration Core\n- [ ] Implement session boundary detection in PowerShell\n- [ ] Create memory scope management in Python-\n- [ ] Develop context injection API -...",
      "directory": ".copilot",
      "size_bytes": 415,
      "is_binary": false,
      "last_modified": "2025-08-08 02:09:51",
      "path": ".copilot/implementation_checklist.md",
      "extension": ".md",
      "filename": "implementation_checklist.md"
    },
    {
      "size_kb": 0.55,
      "content": "# Version control\r\n.git/\r\n.gitignore\r\n\r\n# Python artifacts\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.so\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\n*.egg-info/\r\n.installed.cfg\r\n*.egg\r\n\r\n# Development environments\r\n.env\r\n.venv/\r\nenv/\r\nvenv/\r\nENV/\r\n\r\n# Testing and coverage\r\n.pytest_cache/\r\n.coverage\r\nhtmlcov/\r\n.tox/\r\n\r\n# Documentation\r\ndocs/_build/\r\nnotebooks/\r\n\r\n# IDE files\r\n.vscode/\r\n.idea/\r\n*.swp\r\n*.swo\r\n*~\r\n\r\n# OS files\r\n.DS_Store\r\nThumbs.db\r\n\r\n# Project-specific\r\n_chatbot/\r\n_reports/\r\n*.log\r\n",
      "content_preview": "# Version control\r\n.git/\r\n.gitignore\r\n\r\n# Python artifacts\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.so\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nv...",
      "directory": "",
      "size_bytes": 566,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": ".dockerignore",
      "extension": ".dockerignore",
      "filename": ".dockerignore"
    },
    {
      "size_kb": 3.25,
      "content": "# SurpriseLearner Project Copilot Instructions\n\n## Project Overview\nThis is an adaptive Bayesian learning framework combining LC-NE inspired surprise detection with PowerShell automation tools for AI-assisted development workflows.\n\n## PowerShell Standards (PowerShell 7+)\n- Use strict bracket closure in all nested blocks: `if ($condition) { foreach ($item in $collection) { ... } }`\n- String interpolation with `$()` syntax: `\"Processing file: $($file.Name)\"`\n- Prefer full replacement code over snippets between \"other code...\"\n- Use `&` call operator for explicit command invocation: `& Get-ChildItem @params`\n- Complete parameter splatting patterns with proper hashtables\n\n## Python Standards\n- Follow adaptive_bayesian_driver package structure with proper typing\n- Use torch.device patterns for CUDA-first development\n- Maintain production-grade error handling and logging\n- Implement comprehensive docstrings for all ML components\n\n## Directory Structure Rules\n- Helper scripts belong in `.copilot/helpers/` with subdirectories: diagnostics, fixing, testing, organize\n- All PowerShell utilities use `.ps1` extension with proper comment-based help\n- Python modules follow strict import ordering and type hints\n- Configuration files in `config/` directory with YAML validation\n\n## Code Generation Preferences\n- Always provide complete, executable functions rather than partial snippets\n- Include comprehensive error handling and parameter validation\n- Generate strict PowerShell syntax with complete bracket closure\n- Implement logging and progress indicators for long-running operations\n- Follow the memory orchestration patterns for AI-augmented workflows\n# SurpriseLearner Project Copilot Instructions\n\n## Project Overview\nThis is an adaptive Bayesian learning framework combining LC-NE inspired surprise detection with PowerShell automation tools for AI-assisted development workflows.\n\n## PowerShell Standards (PowerShell 7+)\n- Use strict bracket closure in all nested blocks: `if ($condition) { foreach ($item in $collection) { ... } }`\n- String interpolation with `$()` syntax: `\"Processing file: $($file.Name)\"`\n- Prefer full replacement code over snippets between \"other code...\"\n- Use `&` call operator for explicit command invocation: `& Get-ChildItem @params`\n- Complete parameter splatting patterns with proper hashtables\n\n## Python Standards\n- Follow adaptive_bayesian_driver package structure with proper typing\n- Use torch.device patterns for CUDA-first development\n- Maintain production-grade error handling and logging\n- Implement comprehensive docstrings for all ML components\n\n## Directory Structure Rules\n- Helper scripts belong in `.copilot/helpers/` with subdirectories: diagnostics, fixing, testing, organize\n- All PowerShell utilities use `.ps1` extension with proper comment-based help\n- Python modules follow strict import ordering and type hints\n- Configuration files in `config/` directory with YAML validation\n\n## Code Generation Preferences\n- Always provide complete, executable functions rather than partial snippets\n- Include comprehensive error handling and parameter validation\n- Generate strict PowerShell syntax with complete bracket closure\n- Implement logging and progress indicators for long-running operations\n- Follow the memory orchestration patterns for AI-augmented workflows\n",
      "content_preview": "# SurpriseLearner Project Copilot Instructions\n\n## Project Overview\nThis is an adaptive Bayesian learning framework combining LC-NE inspired surprise detection with PowerShell automation tools for AI-...",
      "directory": ".github",
      "size_bytes": 3326,
      "is_binary": false,
      "last_modified": "2025-08-08 01:29:12",
      "path": ".github/copilot-instructions.md",
      "extension": ".md",
      "filename": "copilot-instructions.md"
    },
    {
      "size_kb": 5.22,
      "content": "---\napplyTo: \"{**/*.ps1,**/*.py}\"\n---\n\n# Memory Orchestration Architecture Instructions\n\n## Core Architecture Overview\nThis project implements a sophisticated memory orchestration system for AI-assisted development with four-tier memory management:\n1. **Session Memory**: Current terminal session context\n2. **Sequence Memory**: Post-instruction execution logging\n3. **Project Memory**: Cross-session continuity\n4. **Global Memory**: Pattern learning across projects\n\n## Memory Orchestration Integration Patterns\n\n### For PowerShell Scripts\nMemory-aware script initialization\nparam(\n[string]$SessionId = (New-Guid).ToString(),\n[string]$MemoryContext = \"Development\",\n[switch]$PersistSession\n)\n\nSession boundary detection\nfunction Test-SessionContinuity {\n[CmdletBinding()]\nparam(\n[Parameter(Mandatory = $true)]\n[string]$WorkspaceType\n)\n\ntext\n$sessionMarkers = @{\n    GitRepository = Test-Path \".git\"\n    CopilotWorkspace = Test-Path \".copilot\"\n    ProjectMemory = Test-Path \"memory/project.json\"\n}\n\nreturn $sessionMarkers[$WorkspaceType]\n}\n\nConsole-aware capture integration\nfunction Write-MemoryLog {\nparam(\n[Parameter(Mandatory = $true)]\n[string]$Message,\n[ValidateSet('Session', 'Sequence', 'Project', 'Global')]\n[string]$Scope = 'Session',\n[hashtable]$Context = @{}\n)\n\ntext\n$logEntry = @{\n    Timestamp = Get-Date -Format 'yyyy-MM-dd HH:mm:ss'\n    Scope = $Scope\n    Message = $Message\n    Context = $Context\n    SessionId = $SessionId\n}\n\n# Hierarchical logging integration\n$logPath = switch ($Scope) {\n    'Session' { \"memory/session/$SessionId.log\" }\n    'Sequence' { \"memory/sequence/$(Get-Date -Format 'yyyyMMdd').log\" }\n    'Project' { \"memory/project/activity.log\" }\n    'Global' { \"$env:USERPROFILE/.copilot/global/patterns.log\" }\n}\n\n$logEntry | ConvertTo-Json -Compress | Out-File -FilePath $logPath -Append -Encoding UTF8\n}\n\ntext\n\n### For Python Components\nfrom typing import Dict, Any, Optional\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass MemoryOrchestrator:\n\"\"\"\nCentral intelligence hub for memory orchestration across development sessions.\n\ntext\nManages context flow between human developer, AI assistants, and development environment.\nImplements cadence-based memory injection and rolling context windows.\n\"\"\"\n\ndef __init__(self, session_id: str, workspace_type: str = \"development\"):\n    self.session_id = session_id\n    self.workspace_type = workspace_type\n    self.memory_scopes = {\n        'session': Path(f\"memory/session/{session_id}\"),\n        'sequence': Path(\"memory/sequence\"),\n        'project': Path(\"memory/project\"),\n        'global': Path.home() / \".copilot\" / \"global\"\n    }\n    self._ensure_memory_structure()\n\ndef inject_context(self, scope: str, context_data: Dict[str, Any]) -> None:\n    \"\"\"Inject context data into specified memory scope.\"\"\"\n    memory_path = self.memory_scopes[scope] / f\"{datetime.now().isoformat()}.json\"\n\n    memory_entry = {\n        'timestamp': datetime.now().isoformat(),\n        'scope': scope,\n        'session_id': self.session_id,\n        'workspace_type': self.workspace_type,\n        'context': context_data\n    }\n\n    with open(memory_path, 'w', encoding='utf-8') as f:\n        json.dump(memory_entry, f, indent=2)\n\ndef synthesize_context(self, query_scope: str, max_entries: int = 50) -> Dict[str, Any]:\n    \"\"\"Synthesize relevant context from memory hierarchy.\"\"\"\n    context_synthesis = {\n        'session_context': self._load_recent_context('session', max_entries),\n        'project_patterns': self._load_recent_context('project', 20),\n        'global_insights': self._load_recent_context('global', 10)\n    }\n\n    return self._filter_relevant_context(context_synthesis, query_scope)\ntext\n\n## Development Workflow Integration Requirements\n\n### Session Boundary Management\n- **Automatic Detection**: Scripts must detect workspace transitions (Git repos, AI workspaces, ephemeral sessions)\n- **State Persistence**: Critical state must survive session boundaries through JSON serialization\n- **Context Migration**: Support context transfer between development environments\n\n### Console-Aware Capture\n- **Command Monitoring**: Register hooks for development commands (test, build, debug)\n- **Output Interception**: Capture and filter console output for AI consumption\n- **Bidirectional Communication**: Enable AI feedback integration with terminal workflows\n\n### Adaptive Assistance Coordination\n- **Context-Aware Triggers**: Map development phases to appropriate AI agent allocation\n- **Dynamic Handoff**: Support seamless agent transitions based on workflow context\n- **Learning Integration**: Incorporate pattern recognition from the adaptive Bayesian learning system\n\n## Cross-Project Pattern Learning\nAll components should contribute to global pattern learning:\n- Log recurring development patterns with structured metadata\n- Support pattern similarity scoring for context transfer\n- Enable recommendation generation based on historical patterns\n- Maintain compatibility with the RecursiveBayesianLearner for surprise detection in development workflows\n\n## Production-Grade Reliability\n- Implement graceful degradation when memory systems are unavailable\n- Use atomic file operations for memory persistence\n- Include comprehensive error recovery for corrupted memory states\n- Support memory cleanup and archival for long-running projects\n",
      "content_preview": "---\napplyTo: \"{**/*.ps1,**/*.py}\"\n---\n\n# Memory Orchestration Architecture Instructions\n\n## Core Architecture Overview\nThis project implements a sophisticated memory orchestration system for AI-assist...",
      "directory": ".github\\instructions",
      "size_bytes": 5341,
      "is_binary": false,
      "last_modified": "2025-08-08 02:10:38",
      "path": ".github/instructions/memory-orchestration.instructions.md",
      "extension": ".md",
      "filename": "memory-orchestration.instructions.md"
    },
    {
      "size_kb": 2.95,
      "content": "---\napplyTo: \"**/*.ps1\"\n---\n\n# PowerShell 7+ Helper Scripts Instructions\n\n## Core PowerShell Standards\n- **PowerShell Version**: Require PowerShell 7+ with `#Requires -Version 7.0`\n- **Bracket Closure**: Complete bracket closure in all nested blocks - every `{`, `(`, `[` must have matching closing bracket\n- **String Interpolation**: Use `$()` syntax for variable expansion in double-quoted strings\n- **Call Operator**: Use `&` operator for explicit command invocation, especially with splatted parameters\n\n## Script Structure Requirements\n- Include comprehensive comment-based help with `.SYNOPSIS`, `.DESCRIPTION`, `.PARAMETER`, `.EXAMPLE`\n- Use `[CmdletBinding()]` for advanced function capabilities\n- Implement proper parameter validation with `[Parameter()]` attributes and `[ValidateSet()]` where applicable\n- Include `try-catch-finally` blocks for robust error handling\n\n## SurpriseLearner Project Context\nThis project implements an adaptive Bayesian learning framework with PowerShell automation tools for AI-assisted development workflows. When generating helper scripts:\n\n- **Diagnostics**: Scripts for scanning, validation, and health checks should integrate with the hierarchical logging system\n- **Fixing**: Automated repair tools should be non-destructive and use staging/review directories\n- **Testing**: Test runners should support both CPU and CUDA execution paths from the adaptive_bayesian_driver package\n- **Organization**: File management tools should respect the modern `.copilot/helpers/` directory structure\n\n## Coding Patterns\n- **Pipeline Support**: Design functions to work in PowerShell pipelines with proper input/output object types\n- **Error Handling**: Implement comprehensive error handling with specific exit codes and detailed error messages\n- **Documentation**: Maintain up-to-date help documentation that reflects the current implementation\n- **Testing**: Include Pester tests for all non-trivial functions\n- **Performance**: Optimize for performance, especially when processing large datasets\n- **Security**: Follow security best practices, including input validation and secure credential handling\n\n### Proper parameter splatting\n$parameters = @{\nPath = $SourcePath\nRecurse = $true\nForce = $Environment -eq 'Production'\n}\n$results = & Get-ChildItem @parameters\n\n### String interpolation with complete syntax\n$message = \"Processing file: $($file.Name) in directory: $($file.Directory.FullName)\"\n\n### Nested bracket completion example\nif ($condition) {\nforeach ($item in $collection) {\nif ($item.Property -eq $targetValue) {\nWrite-Output \"Found match: $($item.Name)\"}}}\n\n## Integration Requirements\n- All scripts must support `-WhatIf` parameter for safe preview mode\n- Include progress indicators for long-running operations using `Write-Progress`\n- Log operations to both console and file using the project's hierarchical logger patterns\n- Ensure compatibility with both Windows and Linux execution environments\n- Support the memory orchestration architecture for session continuity\n",
      "content_preview": "---\napplyTo: \"**/*.ps1\"\n---\n\n# PowerShell 7+ Helper Scripts Instructions\n\n## Core PowerShell Standards\n- **PowerShell Version**: Require PowerShell 7+ with `#Requires -Version 7.0`\n- **Bracket Closure...",
      "directory": ".github\\instructions",
      "size_bytes": 3023,
      "is_binary": false,
      "last_modified": "2025-08-08 02:02:55",
      "path": ".github/instructions/powershell-helpers.instructions.md",
      "extension": ".md",
      "filename": "powershell-helpers.instructions.md"
    },
    {
      "size_kb": 3.07,
      "content": "---\napplyTo: \"adaptive_bayesian_driver/**/*.py\"\n---\n\n# Python ML Models Instructions for Adaptive Bayesian Driver\n\n## Project Architecture Context\nThis package implements LC-NE inspired adaptive Bayesian learning for autonomous driving scenarios with:\n- **Recursive Bayesian Learning**: Ego vehicle embodied learning with surprise detection\n- **CUDA-First Development**: Optimized for GPU acceleration with CPU fallback\n- **Production-Grade Standards**: Comprehensive error handling, logging, and type hints\n\n## Core Development Standards\n- **Python Version**: Minimum Python 3.11 with full type annotation support\n- **Type Hints**: Mandatory type hints for all function signatures using `from typing import Dict, List, Tuple, Optional`\n- **Error Handling**: Comprehensive try-except blocks with specific exception types\n- **Logging**: Use structured logging with appropriate log levels\n- **Docstrings**: Google-style docstrings for all classes and methods\n\n## PyTorch and CUDA Patterns\n### Device management - always include fallback\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nMemory-efficient tensor operations\nwith torch.no_grad():\npredictions = model(data.to(device))\n\n### Proper CUDA memory management\nif torch.cuda.is_available():\ntorch.cuda.empty_cache()\n\n\n## Adaptive Bayesian Learning Context\nWhen implementing models, follow these architectural principles:\n- **Surprise Detection**: Implement LC-NE inspired adaptive thresholds with continuous uncertainty functions\n- **Two-Timescale Dynamics**: Fast latent updates, slow generator parameter adaptation\n- **Ego Vehicle Perspective**: Embodied learning approach for rule acquisition and bias emergence\n- **Reconstruction Error Patterns**: Use generative prior errors for context uncertainty estimation\n\n## Class Structure Requirements\nclass ModelName:\n\"\"\"\nBrief description of model purpose.\nKey Features:\n- Feature 1 with specific technical detail\n- Feature 2 with implementation context\n- Feature 3 with performance characteristics\n\"\"\"\n\ndef __init__(self, config: Dict[str, Any]):\n    self.config = config\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # Initialize state tracking variables\n\ndef method_name(self, param: torch.Tensor, metadata: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Method description with clear purpose.\n\n    Args:\n        param: Description with tensor shape expectations\n        metadata: Description of required metadata keys\n\n    Returns:\n        Dictionary with specific key descriptions\n\"\"\"\n\n## Integration with Compute Optimizations\n- Leverage `IntelComputeOptimizer` for hardware-specific optimizations\n- Support both CPU (Intel MKL-DNN) and GPU execution paths\n- Use `get_device()` utility for consistent device management\n- Implement batch size adaptation based on available memory\n\n## Memory Orchestration Compatibility\n- Methods should be stateful to support session continuity\n- Include `get_system_state()` methods for checkpoint serialization\n- Support memory injection patterns for cross-session learning transfer\n- Implement proper cleanup for long-running learning sessions\n",
      "content_preview": "---\napplyTo: \"adaptive_bayesian_driver/**/*.py\"\n---\n\n# Python ML Models Instructions for Adaptive Bayesian Driver\n\n## Project Architecture Context\nThis package implements LC-NE inspired adaptive Bayes...",
      "directory": ".github\\instructions",
      "size_bytes": 3144,
      "is_binary": false,
      "last_modified": "2025-08-08 02:05:28",
      "path": ".github/instructions/python-ml-models.instructions.md",
      "extension": ".md",
      "filename": "python-ml-models.instructions.md"
    },
    {
      "size_kb": 2.25,
      "content": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\npip-wheel-metadata/\r\nshare/python-wheels/\r\n.installed.cfg\r\n*.egg-info/\r\n*.egg\r\n\r\n# PyInstaller\r\n#  Usually these files are written by a python script from a template\r\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\r\n*.manifest\r\n*.spec\r\n\r\n# Installer logs\r\npip-log.txt\r\npip-delete-this-directory.txt\r\n\r\n# Unit test / coverage reports\r\nhtmlcov/\r\n.tox/\r\n.nox/\r\n.coverage\r\n.coverage.*\r\n.cache\r\nnosetests.xml\r\ncoverage.xml\r\n*.cover\r\n*.py,cover\r\n.hypothesis/\r\n.pytest_cache/\r\n\r\n# Translations\r\n*.mo\r\n*.pot\r\n\r\n# Django stuff:\r\n*.log\r\nlocal_settings.py\r\ndb.sqlite3\r\ndb.sqlite3-journal\r\n\r\n# Flask stuff:\r\ninstance/\r\n.webassets-cache\r\n\r\n# Scrapy stuff:\r\n.scrapy\r\n\r\n# Sphinx documentation\r\ndocs/_build/\r\n\r\n# PyBuilder\r\ntarget/\r\n\r\n# Jupyter Notebook\r\n.ipynb_checkpoints\r\n\r\n# IPython\r\nprofile_default/\r\nipython_config.py\r\n\r\n# pyenv\r\n.python-version\r\n\r\n# pipenv\r\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\r\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\r\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\r\n#   install all needed dependencies.\r\n#Pipfile.lock\r\n\r\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\r\n__pypackages__/\r\n\r\n# Celery stuff\r\ncelerybeat-schedule\r\ncelerybeat.pid\r\n\r\n# SageMath parsed files\r\n*.sage.py\r\n\r\n# Environments\r\n# .env\r\n.venv\r\nenv/\r\nvenv/\r\nENV/\r\nenv.bak/\r\nvenv.bak/\r\n\r\n# Spyder project settings\r\n.spyderproject\r\n.spyproject\r\n\r\n# Rope project settings\r\n.ropeproject\r\n\r\n# mkdocs documentation\r\n/site\r\n\r\n# mypy\r\n.mypy_cache/\r\n.dmypy.json\r\ndmypy.json\r\n\r\n# Pyre type checker\r\n.pyre/\r\n\r\n# Project-specific\r\ndebug_scripts/\r\n*_debug.py\r\n*test_debug.py\r\nrun_specific_tests.py\r\ntest_particle_filter.py\r\ncomprehensive_test.py\r\ntorch_cache/\r\n.ci_env\r\n*.tmp\r\n*.bak\r\n\r\n# Chatbot assistant tools\r\n# _chatbot/\r\n# !_chatbot/README.md\r\n\r\n# VS Code\r\n.vscode/\r\n*.code-workspace\r\n\r\n# PyCharm\r\n.idea/\r\n\r\n# MacOS\r\n.DS_Store\r\n\r\n# Windows\r\nThumbs.db\r\nehthumbs.db\r\nDesktop.ini\r\n",
      "content_preview": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n...",
      "directory": "",
      "size_bytes": 2303,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": ".gitignore",
      "extension": ".gitignore",
      "filename": ".gitignore"
    },
    {
      "size_kb": 0.93,
      "content": "\"\"\"\nAdaptive Bayesian Driver - Production ML Package\n\"\"\"\n\n__version__ = \"0.2.0\"\n__author__ = \"Azriel Ghadooshahy\"\n\n# Core system imports with robust error handling\ntry:\n    import torch\n    import numpy as np\n\n    CUDA_AVAILABLE = torch.cuda.is_available()\n    DEVICE = torch.device(\"cuda\" if CUDA_AVAILABLE else \"cpu\")\n\n    if CUDA_AVAILABLE:\n        GPU_COUNT = torch.cuda.device_count()\n        GPU_NAME = torch.cuda.get_device_name(0)\n    else:\n        GPU_COUNT = 0\n        GPU_NAME = \"No GPU Available\"\n\nexcept ImportError as e:\n    print(f\"⚠️  Critical dependency missing: {e}\")\n    CUDA_AVAILABLE = False\n    DEVICE = \"cpu\"\n    GPU_COUNT = 0\n    GPU_NAME = \"Dependencies Missing\"\n\nfrom .config import load_config\nfrom .utils.device import get_device, setup_cuda_environment\n\n__all__ = [\n    \"__version__\",\n    \"CUDA_AVAILABLE\",\n    \"DEVICE\",\n    \"GPU_COUNT\",\n    \"GPU_NAME\",\n    \"load_config\",\n    \"get_device\",\n    \"setup_cuda_environment\"\n]\n",
      "content_preview": "\"\"\"\nAdaptive Bayesian Driver - Production ML Package\n\"\"\"\n\n__version__ = \"0.2.0\"\n__author__ = \"Azriel Ghadooshahy\"\n\n# Core system imports with robust error handling\ntry:\n    import torch\n    import num...",
      "directory": "adaptive_bayesian_driver",
      "size_bytes": 955,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": "adaptive_bayesian_driver/__init__.py",
      "extension": ".py",
      "filename": "__init__.py"
    },
    {
      "size_kb": 0.07,
      "content": "\"\"\"Applications module for Adaptive Bayesian Driver.\"\"\"\n\n__all__ = []\n",
      "content_preview": "\"\"\"Applications module for Adaptive Bayesian Driver.\"\"\"\n\n__all__ = []\n",
      "directory": "adaptive_bayesian_driver\\applications",
      "size_bytes": 70,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": "adaptive_bayesian_driver/applications/__init__.py",
      "extension": ".py",
      "filename": "__init__.py"
    },
    {
      "size_kb": 2.91,
      "content": "\"\"\"Production-grade configuration management.\"\"\"\n\nimport logging\nimport sys\nfrom pathlib import Path\nfrom typing import Any\n\nimport torch\nimport yaml  # type: ignore\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConfigurationError(Exception):\n    \"\"\"Configuration-related errors.\"\"\"\n\n    pass\n\n\ndef load_config(config_path: str = \"config/experiment.yaml\") -> dict[str, Any]:\n    \"\"\"\n    Load and validate configuration from YAML file.\n\n    Args:\n        config_path: Path to YAML configuration file\n\n    Returns:\n        Validated configuration dictionary\n\n    Raises:\n        ConfigurationError: If configuration is invalid or missing\n    \"\"\"\n    config_file = Path(config_path)\n\n    if not config_file.exists():\n        alternative_paths = [\n            \"config/default.yaml\",\n            \"adaptive_bayesian_driver/config/default.yaml\",\n            \"experiments/config.yaml\",\n        ]\n\n        for alt_path in alternative_paths:\n            if Path(alt_path).exists():\n                config_file = Path(alt_path)\n                logger.warning(f\"Using alternative config: {alt_path}\")\n                break\n        else:\n            raise ConfigurationError(\n                f\"No configuration file found. \"\n                f\"Tried: {config_path}, {alternative_paths}\"\n            )\n\n    try:\n        with open(config_file, encoding=\"utf-8\") as f:\n            config_data = yaml.safe_load(f)\n    except yaml.YAMLError as e:\n        raise ConfigurationError(f\"Invalid YAML in {config_path}: {e}\") from e\n\n    # Ensure config is a dictionary\n    if not isinstance(config_data, dict):\n        raise ConfigurationError(\n            f\"Configuration must be a dictionary, got {type(config_data)}\"\n        )\n\n    config: dict[str, Any] = config_data\n\n    # Validate and enhance configuration\n    config = _validate_and_enhance_config(config)\n    return config\n\n\ndef _validate_and_enhance_config(config: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Validate and enhance configuration with system information.\"\"\"\n\n    # Add device configuration\n    cuda_available = torch.cuda.is_available()\n    device_name = str(torch.device(\"cuda\" if cuda_available else \"cpu\"))\n    gpu_count = torch.cuda.device_count() if cuda_available else 0\n\n    config[\"device\"] = {\n        \"cuda_available\": cuda_available,\n        \"device_name\": device_name,\n        \"gpu_count\": gpu_count,\n    }\n\n    # Add system information\n    python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n    cuda_version = torch.version.cuda if cuda_available else None\n\n    config[\"system\"] = {\n        \"python_version\": python_version,\n        \"torch_version\": torch.__version__,\n        \"cuda_version\": cuda_version,\n    }\n\n    # Validate critical sections\n    required_sections = [\"model\", \"training\", \"environment\"]\n    for section in required_sections:\n        if section not in config:\n            logger.warning(f\"Missing configuration section: {section}\")\n            config[section] = {}\n\n    return config\n",
      "content_preview": "\"\"\"Production-grade configuration management.\"\"\"\n\nimport logging\nimport sys\nfrom pathlib import Path\nfrom typing import Any\n\nimport torch\nimport yaml  # type: ignore\n\nlogger = logging.getLogger(__name...",
      "directory": "adaptive_bayesian_driver",
      "size_bytes": 2983,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": "adaptive_bayesian_driver/config.py",
      "extension": ".py",
      "filename": "config.py"
    },
    {
      "size_kb": 0.13,
      "content": "\"\"\"Environment package for adaptive Bayesian driver.\"\"\"\n\n# Import key environment classes here when they are ready\n\n__all__ = []\n",
      "content_preview": "\"\"\"Environment package for adaptive Bayesian driver.\"\"\"\n\n# Import key environment classes here when they are ready\n\n__all__ = []\n",
      "directory": "adaptive_bayesian_driver\\environment",
      "size_bytes": 129,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": "adaptive_bayesian_driver/environment/__init__.py",
      "extension": ".py",
      "filename": "__init__.py"
    },
    {
      "size_kb": 0.18,
      "content": "\"\"\"Models package for adaptive Bayesian driver.\"\"\"\r\n\r\n# Import key model classes here when they are ready\r\n# from .RecursiveBayesianLearner import RecursiveBayesianLearner\r\n\r\n__all__ = []\r\n",
      "content_preview": "\"\"\"Models package for adaptive Bayesian driver.\"\"\"\r\n\r\n# Import key model classes here when they are ready\r\n# from .RecursiveBayesianLearner import RecursiveBayesianLearner\r\n\r\n__all__ = []\r\n",
      "directory": "adaptive_bayesian_driver\\models",
      "size_bytes": 189,
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "path": "adaptive_bayesian_driver/models/__init__.py",
      "extension": ".py",
      "filename": "__init__.py"
    },
    {
      "size_kb": 25.0,
      "content": "# src/models/bayesian_learner.py\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom typing import Dict, List, Tuple, Optional\r\nfrom .generator import Simple2LayerReLU\r\nfrom .surprise_detector import SurpriseDetector\r\n\r\nclass RecursiveBayesianLearner:\r\n    \"\"\"\r\n    Complete implementation of adaptive recursive Bayesian learning framework\r\n    \r\n    Key Features:\r\n    - LC-NE inspired surprise detection with adaptive thresholds\r\n    - Ego vehicle embodied learning for rule acquisition\r\n    - Two-timescale dynamics (fast latent, slow generator updates)\r\n    - Emergent bias tracking through reconstruction error patterns\r\n    - Computer vision processing of fork intersection scenes\r\n    - Danger avoidance task with yellow/white background detection\r\n    \"\"\"\r\n    \r\n    def __init__(self, config: Dict):\r\n        self.config = config\r\n        \r\n        # Initialize generative prior (2-layer ReLU)\r\n        scene_size = config['visual_parameters']['scene_size']\r\n        self.generator = Simple2LayerReLU(\r\n            latent_dim=config['model_parameters']['latent_dim'],\r\n            output_dim=np.prod(scene_size),\r\n            hidden_dim=config['model_parameters']['generator_hidden']\r\n        )\r\n        \r\n        # Initialize surprise detection system\r\n        self.surprise_detector = SurpriseDetector(\r\n            baseline_window=config['model_parameters']['reconstruction_baseline_window']\r\n        )\r\n        \r\n        # Learning state tracking\r\n        self.reconstruction_history = []\r\n        self.context_uncertainty_history = []\r\n        self.performance_history = []\r\n        self.surprise_history = []\r\n        self.action_history = []\r\n        \r\n        # Ego vehicle state for embodied learning\r\n        self.ego_vehicle_state = {\r\n            'recent_danger_encounters': [],\r\n            'rule_confidence': 0.5,\r\n            'conservative_bias': 0.0,\r\n            'volatility_estimate': 0.5\r\n        }\r\n        \r\n        # Rule learning system\r\n        self.danger_avoidance_strength = 0.5\r\n        self.rule_violation_memory = []\r\n        \r\n        # Adaptive learning parameters\r\n        self.base_learning_rate = config['model_parameters']['learning_rate']\r\n        self.adaptation_rate = config['biological_parameters']['adaptation_rate']\r\n        \r\n    def process_scene_sequence(self, scenes: torch.Tensor, \r\n                             metadata: List[Dict]) -> Dict:\r\n        \"\"\"Process continuous sequence of fork intersection scenes\"\"\"\r\n        results = {\r\n            'surprise_signals': [],\r\n            'adaptive_thresholds': [],\r\n            'reconstruction_errors': [],\r\n            'actions': [],\r\n            'correct_actions': [],\r\n            'context_uncertainty': [],\r\n            'performance': [],\r\n            'ego_states': [],\r\n            'rule_violations': [],\r\n            'learning_rates': []\r\n        }\r\n        \r\n        for step, (scene, meta) in enumerate(zip(scenes, metadata)):\r\n            # Process single scene through complete framework\r\n            step_results = self._process_single_scene(scene, meta, step)\r\n            \r\n            # Store all results for analysis\r\n            for key, value in step_results.items():\r\n                results[key].append(value)\r\n                \r\n        return results\r\n    \r\n    def _process_single_scene(self, scene: torch.Tensor, metadata: Dict, \r\n                            step: int) -> Dict:\r\n        \"\"\"\r\n        Process single scene through recursive Bayesian framework with ego vehicle perspective\r\n        \"\"\"\r\n        \r\n        # 1. Establish ego vehicle context (enables rule understanding)\r\n        ego_state = self._update_ego_vehicle_state(scene, metadata, step)\r\n        \r\n        # 2. Compute reconstruction error via generative prior\r\n        reconstruction_error = self.generator.compute_reconstruction_error(scene)\r\n        self.reconstruction_history.append(reconstruction_error)\r\n        \r\n        # 3. Compute emergent context uncertainty\r\n        context_uncertainty = self._compute_context_uncertainty()\r\n        prediction_confidence = self._compute_prediction_confidence()\r\n        self.context_uncertainty_history.append(context_uncertainty)\r\n        \r\n        # 4. Adaptive surprise threshold (continuous function of uncertainty)\r\n        adaptive_threshold = self.surprise_detector.adaptive_surprise_threshold(\r\n            context_uncertainty, prediction_confidence, ego_state['volatility_regime']\r\n        )\r\n        \r\n        # 5. Compute surprise signal with adaptive threshold\r\n        surprise = self.surprise_detector.compute_surprise(\r\n            reconstruction_error, \r\n            self.reconstruction_history,\r\n            context_uncertainty=context_uncertainty,\r\n            prediction_confidence=prediction_confidence\r\n        )\r\n        self.surprise_history.append(surprise)\r\n        \r\n        # 6. Make ego vehicle action decision with rule learning\r\n        action = self._make_ego_vehicle_decision(scene, metadata, surprise, ego_state)\r\n        self.action_history.append(action)\r\n        \r\n        # 7. Get feedback and experience consequences\r\n        feedback = self._get_feedback(action, metadata['correct_action'])\r\n        self.performance_history.append(feedback)\r\n        \r\n        # 8. Detect rule violations for learning\r\n        rule_violation = self._detect_rule_violation(action, metadata, ego_state)\r\n        \r\n        # 9. Update beliefs based on embodied experience\r\n        learning_rate = self._update_beliefs_with_ego_experience(\r\n            surprise, feedback, reconstruction_error, ego_state, adaptive_threshold, rule_violation\r\n        )\r\n        \r\n        return {\r\n            'surprise_signal': surprise,\r\n            'adaptive_threshold': adaptive_threshold,\r\n            'reconstruction_error': reconstruction_error,\r\n            'action': action,\r\n            'correct_action': metadata['correct_action'],\r\n            'context_uncertainty': context_uncertainty,\r\n            'performance': feedback,\r\n            'ego_state': ego_state.copy(),\r\n            'rule_violation': rule_violation,\r\n            'learning_rate': learning_rate\r\n        }\r\n    \r\n    def _update_ego_vehicle_state(self, scene: torch.Tensor, metadata: Dict, step: int) -> Dict:\r\n        \"\"\"Update ego vehicle state for embodied rule learning\"\"\"\r\n        \r\n        # Detect current scene characteristics\r\n        danger_detected = self._detect_danger_background(scene)\r\n        scene_difficulty = self._assess_scene_difficulty(scene)\r\n        \r\n        # Update danger encounter history\r\n        if danger_detected:\r\n            self.ego_vehicle_state['recent_danger_encounters'].append(step)\r\n            # Keep only recent encounters (last 100 steps)\r\n            self.ego_vehicle_state['recent_danger_encounters'] = \\\r\n                self.ego_vehicle_state['recent_danger_encounters'][-100:]\r\n        \r\n        # Estimate current volatility regime\r\n        volatility_regime = self._estimate_volatility_regime()\r\n        \r\n        # Update rule confidence based on recent performance\r\n        self._update_rule_confidence()\r\n        \r\n        # Update conservative bias based on recent rule violations\r\n        self._update_conservative_bias()\r\n        \r\n        return {\r\n            'position': 'intersection_approach',\r\n            'speed': 'moderate',\r\n            'visibility': scene_difficulty,\r\n            'volatility_regime': volatility_regime,\r\n            'danger_frequency': len(self.ego_vehicle_state['recent_danger_encounters']) / 100,\r\n            'rule_confidence': self.ego_vehicle_state['rule_confidence'],\r\n            'conservative_bias': self.ego_vehicle_state['conservative_bias'],\r\n            'recent_performance': np.mean(self.performance_history[-10:]) if len(self.performance_history) >= 10 else 0.5\r\n        }\r\n    \r\n    def _compute_context_uncertainty(self) -> float:\r\n        \"\"\"Compute emergent context uncertainty from reconstruction error patterns\"\"\"\r\n        if len(self.reconstruction_history) < 10:\r\n            return 0.5  # Initial uncertainty\r\n        \r\n        # Use variance in recent reconstruction errors as uncertainty proxy\r\n        recent_errors = self.reconstruction_history[-20:]\r\n        recent_surprises = self.surprise_history[-20:] if len(self.surprise_history) >= 20 else [0.0] * 20\r\n        \r\n        # Combine reconstruction error variance with surprise volatility\r\n        error_uncertainty = np.var(recent_errors) / (np.mean(recent_errors) + 1e-6)\r\n        surprise_uncertainty = np.var(recent_surprises) if len(recent_surprises) > 1 else 0.0\r\n        \r\n        # Weight combination\r\n        uncertainty = 0.7 * error_uncertainty + 0.3 * surprise_uncertainty\r\n        \r\n        return np.clip(uncertainty, 0.0, 1.0)\r\n    \r\n    def _compute_prediction_confidence(self) -> float:\r\n        \"\"\"Compute prediction confidence from recent performance\"\"\"\r\n        if len(self.performance_history) < 5:\r\n            return 0.5\r\n        \r\n        recent_performance = self.performance_history[-10:]\r\n        confidence = np.mean(recent_performance)\r\n        \r\n        # Adjust for consistency\r\n        consistency = 1.0 - np.var(recent_performance)\r\n        confidence = 0.8 * confidence + 0.2 * consistency\r\n        \r\n        return np.clip(confidence, 0.0, 1.0)\r\n    \r\n    def _estimate_volatility_regime(self) -> str:\r\n        \"\"\"Estimate current environmental volatility regime\"\"\"\r\n        if len(self.surprise_history) < 20:\r\n            return 'moderate'\r\n        \r\n        recent_surprise_rate = np.mean([abs(s) > 0.5 for s in self.surprise_history[-20:]])\r\n        \r\n        if recent_surprise_rate > 0.4:\r\n            return 'volatile'\r\n        elif recent_surprise_rate < 0.1:\r\n            return 'stable'\r\n        else:\r\n            return 'moderate'\r\n    \r\n    def _make_ego_vehicle_decision(self, scene: torch.Tensor, metadata: Dict, \r\n                                 surprise: float, ego_state: Dict) -> str:\r\n        \"\"\"\r\n        Make ego vehicle decision with learned rule understanding\r\n        \"\"\"\r\n        \r\n        # Detect visual features\r\n        danger_detected = self._detect_danger_background(scene)\r\n        left_cue_detected = self._detect_left_orientation(scene)\r\n        \r\n        # Apply rule hierarchy with uncertainty modulation\r\n        if danger_detected:\r\n            # Danger avoidance rule with confidence modulation\r\n            base_action = 'right' if left_cue_detected else 'left'\r\n            \r\n            # Modulate based on rule confidence and surprise\r\n            rule_strength = self.ego_vehicle_state['rule_confidence']\r\n            if surprise > ego_state.get('surprise_threshold', 0.5):\r\n                # High surprise → more conservative (stronger rule application)\r\n                rule_strength = min(1.0, rule_strength * 1.2)\r\n            \r\n            # Apply conservative bias when uncertain\r\n            if ego_state['rule_confidence'] < 0.7:\r\n                conservative_action = self._apply_conservative_strategy(base_action, ego_state)\r\n                action = conservative_action\r\n            else:\r\n                action = base_action\r\n                \r\n        else:\r\n            # Safe condition: follow bar direction with bias consideration\r\n            base_action = 'left' if left_cue_detected else 'right'\r\n            \r\n            # Apply learned biases from reconstruction error patterns\r\n            action = self._apply_emergent_bias(base_action, ego_state)\r\n        \r\n        return action\r\n    \r\n    def _detect_danger_background(self, scene: torch.Tensor) -> bool:\r\n        \"\"\"Detect danger sign from yellow background vs white/gray\"\"\"\r\n        mean_intensity = torch.mean(scene).item()\r\n        # Yellow background has higher intensity than gray/white\r\n        return mean_intensity > 0.6\r\n    \r\n    def _detect_left_orientation(self, scene: torch.Tensor) -> bool:\r\n        \"\"\"Detect left-pointing vs right-pointing oriented bars\"\"\"\r\n        h, w = scene.shape\r\n        center_region = scene[h//2-8:h//2+8, w//2-8:w//2+8]\r\n        \r\n        if center_region.numel() == 0:\r\n            return False\r\n        \r\n        # Gradient-based orientation detection\r\n        grad_x = torch.diff(center_region, dim=1)\r\n        grad_y = torch.diff(center_region, dim=0)\r\n        \r\n        if grad_x.numel() == 0 or grad_y.numel() == 0:\r\n            return False\r\n        \r\n        # Left-pointing bars have specific gradient correlation pattern\r\n        try:\r\n            # Align dimensions for correlation\r\n            min_size = min(grad_x.shape[0], grad_y.shape[0])\r\n            grad_x_flat = grad_x[:min_size, :].flatten()\r\n            grad_y_flat = grad_y[:min_size, :].flatten()\r\n            \r\n            if len(grad_x_flat) > 1 and len(grad_y_flat) > 1:\r\n                correlation = torch.corrcoef(torch.stack([grad_x_flat, grad_y_flat]))[0,1]\r\n                return correlation.item() > 0\r\n            else:\r\n                return False\r\n        except:\r\n            return False\r\n    \r\n    def _assess_scene_difficulty(self, scene: torch.Tensor) -> float:\r\n        \"\"\"Assess visual difficulty of current scene\"\"\"\r\n        # Use variance and edge density as difficulty proxy\r\n        intensity_var = torch.var(scene).item()\r\n        edges = torch.abs(torch.diff(scene, dim=0)).sum() + torch.abs(torch.diff(scene, dim=1)).sum()\r\n        edge_density = edges.item() / scene.numel()\r\n        \r\n        difficulty = 1.0 - (intensity_var + edge_density) / 2.0\r\n        return np.clip(difficulty, 0.0, 1.0)\r\n    \r\n    def _apply_conservative_strategy(self, base_action: str, ego_state: Dict) -> str:\r\n        \"\"\"Apply conservative bias when rule confidence is low\"\"\"\r\n        conservative_bias = ego_state['conservative_bias']\r\n        \r\n        # When uncertain, prefer right turns (arbitrary but consistent choice)\r\n        if conservative_bias > 0.3:\r\n            return 'right'\r\n        else:\r\n            return base_action\r\n    \r\n    def _apply_emergent_bias(self, base_action: str, ego_state: Dict) -> str:\r\n        \"\"\"Apply emergent directional bias learned from reconstruction patterns\"\"\"\r\n        \r\n        # Extract bias from recent reconstruction error patterns\r\n        if len(self.reconstruction_history) < 20:\r\n            return base_action\r\n        \r\n        # Analyze correlation between actions and reconstruction errors\r\n        recent_actions = self.action_history[-20:]\r\n        recent_errors = self.reconstruction_history[-20:]\r\n        \r\n        if len(recent_actions) < 20:\r\n            return base_action\r\n        \r\n        # Simple bias estimation\r\n        left_errors = [e for a, e in zip(recent_actions, recent_errors) if a == 'left']\r\n        right_errors = [e for a, e in zip(recent_actions, recent_errors) if a == 'right']\r\n        \r\n        if len(left_errors) > 0 and len(right_errors) > 0:\r\n            left_avg_error = np.mean(left_errors)\r\n            right_avg_error = np.mean(right_errors)\r\n            \r\n            # Prefer direction with lower average reconstruction error\r\n            if left_avg_error < right_avg_error * 0.9:  # 10% preference threshold\r\n                bias_action = 'left'\r\n            elif right_avg_error < left_avg_error * 0.9:\r\n                bias_action = 'right'\r\n            else:\r\n                bias_action = base_action\r\n        else:\r\n            bias_action = base_action\r\n        \r\n        return bias_action\r\n    \r\n    def _get_feedback(self, action: str, correct_action: str) -> int:\r\n        \"\"\"Get binary feedback signal (1 = correct, 0 = incorrect)\"\"\"\r\n        return 1 if action == correct_action else 0\r\n    \r\n    def _detect_rule_violation(self, action: str, metadata: Dict, ego_state: Dict) -> bool:\r\n        \"\"\"Detect if action violates the danger avoidance rule\"\"\"\r\n        danger_present = metadata['danger_present']\r\n        left_cue = metadata['left_cue']\r\n        \r\n        if danger_present:\r\n            # Rule: danger + left cue → turn right, danger + right cue → turn left\r\n            correct_avoidance = 'right' if left_cue else 'left'\r\n            return action != correct_avoidance\r\n        \r\n        return False\r\n    \r\n    def _update_beliefs_with_ego_experience(self, surprise: float, feedback: int, \r\n                                          reconstruction_error: float, ego_state: Dict,\r\n                                          adaptive_threshold: float, rule_violation: bool) -> float:\r\n        \"\"\"\r\n        Update beliefs based on embodied experience with adaptive learning rate\r\n        \"\"\"\r\n        \r\n        # Calculate learning signal strength\r\n        performance_error = 1 - feedback  # 1 if wrong, 0 if correct\r\n        surprise_magnitude = max(0, surprise - adaptive_threshold)\r\n        \r\n        # Combine multiple error sources\r\n        total_learning_signal = (\r\n            0.4 * performance_error +           # Task performance\r\n            0.3 * surprise_magnitude +          # Prediction surprise\r\n            0.2 * float(rule_violation) +       # Rule violation\r\n            0.1 * reconstruction_error          # Perceptual error\r\n        )\r\n        \r\n        # Adaptive learning rate based on uncertainty and ego state\r\n        base_rate = self.adaptation_rate\r\n        uncertainty_modulation = 1.0 + ego_state['context_uncertainty']\r\n        confidence_modulation = 2.0 - ego_state['rule_confidence']\r\n        \r\n        adaptive_learning_rate = base_rate * uncertainty_modulation * confidence_modulation\r\n        \r\n        # Apply learning if signal is significant\r\n        if total_learning_signal > 0.1:\r\n            # Update generator parameters (slow timescale)\r\n            self._update_generator_weights(total_learning_signal, adaptive_learning_rate)\r\n            \r\n            # Update rule knowledge (ego-specific learning)\r\n            self._update_rule_knowledge(ego_state, feedback, rule_violation, total_learning_signal)\r\n            \r\n            # Update context beliefs (emergent from patterns)\r\n            self._update_context_beliefs(surprise, feedback, reconstruction_error)\r\n        \r\n        return adaptive_learning_rate\r\n    \r\n    def _update_generator_weights(self, learning_signal: float, learning_rate: float):\r\n        \"\"\"Update generative prior weights based on surprise-driven adaptation\"\"\"\r\n        \r\n        # Implement simplified weight update (in practice, would use proper gradients)\r\n        adaptation_strength = learning_rate * learning_signal\r\n        \r\n        # Simple perturbation-based update for demonstration\r\n        with torch.no_grad():\r\n            for param in self.generator.parameters():\r\n                if param.grad is not None:\r\n                    # Apply adaptation with momentum\r\n                    param.data -= adaptation_strength * param.grad\r\n                else:\r\n                    # Small random perturbation when no gradient available\r\n                    noise = torch.randn_like(param) * adaptation_strength * 0.01\r\n                    param.data += noise\r\n    \r\n    def _update_rule_knowledge(self, ego_state: Dict, feedback: int, \r\n                             rule_violation: bool, learning_signal: float):\r\n        \"\"\"Update internal rule representations based on ego vehicle experience\"\"\"\r\n        \r\n        # Update rule confidence based on performance\r\n        if rule_violation and feedback == 0:\r\n            # Rule violation led to bad outcome → strengthen rule\r\n            self.ego_vehicle_state['rule_confidence'] = min(1.0, \r\n                self.ego_vehicle_state['rule_confidence'] + 0.1 * learning_signal)\r\n        elif not rule_violation and feedback == 1:\r\n            # Following rule led to good outcome → reinforce rule\r\n            self.ego_vehicle_state['rule_confidence'] = min(1.0,\r\n                self.ego_vehicle_state['rule_confidence'] + 0.05 * learning_signal)\r\n        elif rule_violation and feedback == 1:\r\n            # Rule violation led to good outcome → weaken rule slightly\r\n            self.ego_vehicle_state['rule_confidence'] = max(0.0,\r\n                self.ego_vehicle_state['rule_confidence'] - 0.02 * learning_signal)\r\n        \r\n        # Track rule violations for pattern learning\r\n        self.rule_violation_memory.append({\r\n            'violation': rule_violation,\r\n            'feedback': feedback,\r\n            'learning_signal': learning_signal\r\n        })\r\n        \r\n        # Keep only recent memory\r\n        self.rule_violation_memory = self.rule_violation_memory[-50:]\r\n    \r\n    def _update_rule_confidence(self):\r\n        \"\"\"Update rule confidence based on recent performance patterns\"\"\"\r\n        if len(self.performance_history) < 10:\r\n            return\r\n        \r\n        recent_performance = self.performance_history[-10:]\r\n        recent_violations = self.rule_violation_memory[-10:] if len(self.rule_violation_memory) >= 10 else []\r\n        \r\n        # Higher performance → higher rule confidence\r\n        performance_factor = np.mean(recent_performance)\r\n        \r\n        # Fewer violations → higher rule confidence\r\n        violation_factor = 1.0 - np.mean([v['violation'] for v in recent_violations]) if recent_violations else 1.0\r\n        \r\n        # Update with exponential moving average\r\n        alpha = 0.1\r\n        new_confidence = 0.6 * performance_factor + 0.4 * violation_factor\r\n        self.ego_vehicle_state['rule_confidence'] = (\r\n            (1 - alpha) * self.ego_vehicle_state['rule_confidence'] + \r\n            alpha * new_confidence\r\n        )\r\n    \r\n    def _update_conservative_bias(self):\r\n        \"\"\"Update conservative bias based on recent rule violations and uncertainty\"\"\"\r\n        if len(self.context_uncertainty_history) < 5:\r\n            return\r\n        \r\n        recent_uncertainty = np.mean(self.context_uncertainty_history[-5:])\r\n        recent_violations = len([v for v in self.rule_violation_memory[-10:] if v['violation']]) if len(self.rule_violation_memory) >= 10 else 0\r\n        \r\n        # Higher uncertainty → more conservative\r\n        # More violations → more conservative\r\n        target_bias = 0.5 * recent_uncertainty + 0.3 * (recent_violations / 10.0)\r\n        \r\n        # Smooth update\r\n        alpha = 0.05\r\n        self.ego_vehicle_state['conservative_bias'] = (\r\n            (1 - alpha) * self.ego_vehicle_state['conservative_bias'] + \r\n            alpha * target_bias\r\n        )\r\n    \r\n    def _update_context_beliefs(self, surprise: float, feedback: int, reconstruction_error: float):\r\n        \"\"\"Update context beliefs based on surprise and performance patterns\"\"\"\r\n        \r\n        # This implements emergent bias tracking through error patterns\r\n        # The bias emerges from the patterns of reconstruction errors and surprise signals\r\n        # rather than being explicitly programmed\r\n        \r\n        # Track statistical patterns in reconstruction errors\r\n        if len(self.reconstruction_history) >= 20:\r\n            # Analyze recent error patterns for context switching\r\n            recent_errors = self.reconstruction_history[-20:]\r\n            error_trend = np.diff(recent_errors)\r\n            \r\n            # Detect potential context switches from error dynamics\r\n            if np.std(error_trend) > np.mean(recent_errors) * 0.1:\r\n                # High variability suggests context instability\r\n                self.ego_vehicle_state['volatility_estimate'] = min(1.0,\r\n                    self.ego_vehicle_state['volatility_estimate'] + 0.05)\r\n            else:\r\n                # Low variability suggests stable context\r\n                self.ego_vehicle_state['volatility_estimate'] = max(0.0,\r\n                    self.ego_vehicle_state['volatility_estimate'] - 0.02)\r\n    \r\n    def get_learning_metrics(self) -> Dict:\r\n        \"\"\"Get comprehensive learning performance metrics\"\"\"\r\n        if len(self.performance_history) < 10:\r\n            return {\r\n                'performance': 0.5, \r\n                'surprise_rate': 0.5,\r\n                'context_uncertainty': 0.5,\r\n                'rule_confidence': 0.5\r\n            }\r\n        \r\n        recent_performance = np.mean(self.performance_history[-50:])\r\n        recent_surprise = np.mean([abs(s) for s in self.surprise_history[-50:]])\r\n        recent_uncertainty = np.mean(self.context_uncertainty_history[-20:])\r\n        \r\n        return {\r\n            'performance': recent_performance,\r\n            'surprise_rate': recent_surprise,\r\n            'context_uncertainty': recent_uncertainty,\r\n            'rule_confidence': self.ego_vehicle_state['rule_confidence'],\r\n            'conservative_bias': self.ego_vehicle_state['conservative_bias'],\r\n            'volatility_estimate': self.ego_vehicle_state['volatility_estimate'],\r\n            'total_rule_violations': len([v for v in self.rule_violation_memory if v['violation']]),\r\n            'adaptation_rate': np.mean([abs(s) > 0.5 for s in self.surprise_history[-20:]]) if len(self.surprise_history) >= 20 else 0.0\r\n        }\r\n    \r\n    def get_system_state(self) -> Dict:\r\n        \"\"\"Get current complete system state for analysis\"\"\"\r\n        return {\r\n            'ego_vehicle_state': self.ego_vehicle_state.copy(),\r\n            'recent_performance': self.performance_history[-10:] if len(self.performance_history) >= 10 else [],\r\n            'recent_surprises': self.surprise_history[-10:] if len(self.surprise_history) >= 10 else [],\r\n            'recent_uncertainties': self.context_uncertainty_history[-10:] if len(self.context_uncertainty_history) >= 10 else [],\r\n            'generator_state': 'pretrained',  # Could add actual generator analysis\r\n            'learning_metrics': self.get_learning_metrics()\r\n        }\r\n",
      "content_preview": "# src/models/bayesian_learner.py\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom typing import Dict, List, Tuple, Optional\r\nfrom .generator import Simple2LayerReLU\r\nfrom .surprise_detec...",
      "directory": "adaptive_bayesian_driver\\models",
      "size_bytes": 25597,
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "path": "adaptive_bayesian_driver/models/RecursiveBayesianLearner.py",
      "extension": ".py",
      "filename": "RecursiveBayesianLearner.py"
    },
    {
      "size_kb": 0.32,
      "content": "\"\"\"\r\nParticle filter utilities for adaptive Bayesian learning.\r\n\"\"\"\r\n\r\nimport numpy as np\r\nimport torch\r\nfrom typing import Dict, List, Tuple, Optional, Callable\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n# This file was restored after being corrupted\r\n# Content needs to be reimplemented based on project needs\r\n",
      "content_preview": "\"\"\"\r\nParticle filter utilities for adaptive Bayesian learning.\r\n\"\"\"\r\n\r\nimport numpy as np\r\nimport torch\r\nfrom typing import Dict, List, Tuple, Optional, Callable\r\nimport logging\r\n\r\nlogger = logging.ge...",
      "directory": "adaptive_bayesian_driver\\models",
      "size_bytes": 329,
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "path": "adaptive_bayesian_driver/models/utils_particle.py",
      "extension": ".py",
      "filename": "utils_particle.py"
    },
    {
      "size_kb": 0.11,
      "content": "\"\"\"Utilities package for adaptive Bayesian driver.\"\"\"\r\n\r\nfrom .device import get_device\r\n\r\n__all__ = [\"get_device\"]\r\n",
      "content_preview": "\"\"\"Utilities package for adaptive Bayesian driver.\"\"\"\r\n\r\nfrom .device import get_device\r\n\r\n__all__ = [\"get_device\"]\r\n",
      "directory": "adaptive_bayesian_driver\\utils",
      "size_bytes": 117,
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "path": "adaptive_bayesian_driver/utils/__init__.py",
      "extension": ".py",
      "filename": "__init__.py"
    },
    {
      "size_kb": 8.88,
      "content": "#  compute_optimizer.py\nimport torch\nimport os\nimport psutil\nimport platform\nfrom typing import Dict, Tuple, Any\nimport warnings\n\nclass IntelComputeOptimizer:\n    \"\"\"\n    Hardware-aware compute optimization for Intel 8-core + 32GB RAM system.\n    Optimizes PyTorch performance for uncertainty-aware computer vision demo.\n    \"\"\"\n\n    def __init__(self, target_gpu_memory_fraction: float = 0.8):\n        self.system_specs = self._detect_system_specs()\n        self.target_gpu_memory_fraction = target_gpu_memory_fraction\n        self.device = self._setup_optimal_device()\n        self.config = self._generate_optimal_config()\n\n    def _detect_system_specs(self) -> Dict[str, Any]:\n        \"\"\"Detect and validate system specifications\"\"\"\n        specs = {\n            'cpu_count': os.cpu_count(),\n            'physical_cores': psutil.cpu_count(logical=False),\n            'logical_cores': psutil.cpu_count(logical=True),\n            'total_memory_gb': psutil.virtual_memory().total / (1024**3),\n            'available_memory_gb': psutil.virtual_memory().available / (1024**3),\n            'platform': platform.processor(),\n            'is_intel': 'intel' in platform.processor().lower(),\n            'supports_mkldnn': torch.backends.mkldnn.is_available(),\n        }\n        return specs\n\n    def _setup_optimal_device(self) -> torch.device:\n        \"\"\"Configure optimal PyTorch device with Intel-specific optimizations\"\"\"\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        if device.type == 'cpu':\n            self._optimize_cpu_performance()\n        else:\n            self._optimize_gpu_performance()\n\n        return device\n\n    def _optimize_cpu_performance(self):\n        \"\"\"Aggressive CPU optimizations for 8-core Intel system\"\"\"\n\n        # Intel MKL optimizations - aggressive for your hardware\n        if self.system_specs['is_intel'] and self.system_specs['supports_mkldnn']:\n            torch.backends.mkldnn.enabled = True\n            torch.backends.mkldnn.verbose = 0  # Reduce logging overhead\n\n        # Threading optimization for 8-core system\n        # Use 6 threads (reserve 2 for system + other processes)\n        optimal_threads = min(6, self.system_specs['physical_cores'])\n        torch.set_num_threads(optimal_threads)\n\n        # Single interop thread to reduce overhead on multi-core systems\n        torch.set_num_interop_threads(1)\n\n        # Intel-specific environment variables\n        os.environ['OMP_NUM_THREADS'] = str(optimal_threads)\n        os.environ['MKL_NUM_THREADS'] = str(optimal_threads)\n        os.environ['NUMEXPR_NUM_THREADS'] = str(optimal_threads)\n\n        # Intel MKL specific optimizations\n        os.environ['KMP_AFFINITY'] = 'granularity=fine,compact,1,0'\n        os.environ['KMP_BLOCKTIME'] = '1'  # Reduce thread idle time\n\n        # Memory optimization for 32GB system\n        torch.set_default_dtype(torch.float32)  # Efficient precision\n\n        print(f\"🔧 Intel CPU optimizations enabled:\")\n        print(f\"   - Threads: {optimal_threads}/{self.system_specs['logical_cores']}\")\n        print(f\"   - MKL-DNN: {torch.backends.mkldnn.enabled}\")\n        print(f\"   - Available RAM: {self.system_specs['available_memory_gb']:.1f}GB\")\n\n    def _optimize_gpu_performance(self):\n        \"\"\"GPU optimizations with CPU fallback awareness\"\"\"\n        if torch.cuda.is_available():\n            # Enable optimized attention for GPU\n            torch.backends.cuda.enable_flash_sdp(True)\n\n            # Memory management\n            torch.cuda.empty_cache()\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n\n            print(f\"🚀 GPU acceleration: {torch.cuda.get_device_name(0)}\")\n            print(f\"   - GPU Memory: {gpu_memory:.1f}GB\")\n            print(f\"   - Target usage: {self.target_gpu_memory_fraction*100}%\")\n\n    def _generate_optimal_config(self) -> Dict[str, Any]:\n        \"\"\"Generate hardware-optimized configuration\"\"\"\n\n        if self.device.type == 'cpu':\n            # Aggressive CPU config for your 32GB + 8-core system\n            return {\n                # Batch sizing - larger batches for 32GB RAM\n                'batch_size_train': 64,    # Increased from conservative 32\n                'batch_size_eval': 128,    # Large eval batches for throughput\n\n                # Uncertainty quantification\n                'mc_samples': 8,           # More samples for better uncertainty\n                'ensemble_size': 4,        # Multiple models for robust UQ\n\n                # DataLoader optimization for 8 cores\n                'num_workers': 6,          # Reserve 2 cores for training process\n                'pin_memory': True,        # Faster CPU->GPU transfer when available\n                'persistent_workers': True, # Reduce worker startup overhead\n\n                # Memory and precision\n                'precision': torch.float32,\n                'memory_format': torch.channels_last, # Intel optimization\n\n                # Training optimization\n                'gradient_accumulation_steps': 1,  # Direct updates with large RAM\n                'max_memory_usage_gb': 24,         # Conservative limit for 32GB\n            }\n        else:\n            # GPU configuration with CPU preprocessing support\n            return {\n                'batch_size_train': 256,   # Large GPU batches\n                'batch_size_eval': 512,\n                'mc_samples': 10,\n                'ensemble_size': 5,\n                'num_workers': 8,          # Full core utilization for data loading\n                'pin_memory': True,\n                'persistent_workers': True,\n                'precision': torch.float16, # GPU half precision\n                'memory_format': torch.channels_last,\n                'gradient_accumulation_steps': 1,\n                'max_memory_usage_gb': self.target_gpu_memory_fraction *\n                                     torch.cuda.get_device_properties(0).total_memory / (1024**3)\n            }\n\n    def create_optimized_dataloader(self, dataset, shuffle: bool = True, drop_last: bool = True):\n        \"\"\"Create hardware-optimized DataLoader\"\"\"\n        return torch.utils.data.DataLoader(\n            dataset,\n            batch_size=self.config['batch_size_train'] if shuffle else self.config['batch_size_eval'],\n            shuffle=shuffle,\n            num_workers=self.config['num_workers'],\n            pin_memory=self.config['pin_memory'],\n            persistent_workers=self.config['persistent_workers'],\n            drop_last=drop_last\n        )\n\n    def profile_performance(self, model, test_loader) -> Dict[str, float]:\n        \"\"\"Benchmark model performance on your hardware\"\"\"\n        import time\n\n        model.eval()\n        total_samples = 0\n        total_time = 0\n\n        with torch.no_grad():\n            start_time = time.time()\n            for batch_idx, (data, targets) in enumerate(test_loader):\n                data = data.to(self.device)\n\n                # Your uncertainty-aware prediction\n                if hasattr(model, 'predict_with_uncertainty'):\n                    predictions, uncertainty = model.predict_with_uncertainty(data)\n                else:\n                    predictions = model(data)\n\n                total_samples += data.size(0)\n\n                # Break after reasonable sample for quick profiling\n                if batch_idx > 50:  # ~6400 samples with batch_size=128\n                    break\n\n            total_time = time.time() - start_time\n\n        throughput = total_samples / total_time\n        latency = 1000 / throughput  # ms per sample\n\n        return {\n            'device': str(self.device),\n            'samples_processed': total_samples,\n            'total_time_sec': total_time,\n            'throughput_samples_per_sec': throughput,\n            'avg_latency_ms': latency,\n            'memory_usage_gb': self._get_memory_usage()\n        }\n\n    def _get_memory_usage(self) -> float:\n        \"\"\"Get current memory usage\"\"\"\n        if self.device.type == 'cuda':\n            return torch.cuda.memory_allocated() / (1024**3)\n        else:\n            return psutil.Process().memory_info().rss / (1024**3)\n\n    def print_optimization_summary(self):\n        \"\"\"Print comprehensive optimization summary\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"🎯 INTEL COMPUTE OPTIMIZATION SUMMARY\")\n        print(\"=\"*60)\n        print(f\"Hardware: {self.system_specs['physical_cores']}-core Intel, {self.system_specs['total_memory_gb']:.0f}GB RAM\")\n        print(f\"Device: {self.device}\")\n        print(f\"PyTorch threads: {torch.get_num_threads()}\")\n        print(f\"MKL-DNN enabled: {torch.backends.mkldnn.enabled}\")\n        print(\"\\nOptimal Configuration:\")\n        for key, value in self.config.items():\n            print(f\"  {key}: {value}\")\n        print(\"=\"*60)\n\n# Convenience function for immediate use\ndef setup_optimal_compute(target_gpu_memory_fraction: float = 0.8) -> IntelComputeOptimizer:\n    \"\"\"One-line setup for your Intel 8-core system\"\"\"\n    optimizer = IntelComputeOptimizer(target_gpu_memory_fraction)\n    optimizer.print_optimization_summary()\n    return optimizer\n",
      "content_preview": "#  compute_optimizer.py\nimport torch\nimport os\nimport psutil\nimport platform\nfrom typing import Dict, Tuple, Any\nimport warnings\n\nclass IntelComputeOptimizer:\n    \"\"\"\n    Hardware-aware compute optimi...",
      "directory": "adaptive_bayesian_driver\\utils",
      "size_bytes": 9097,
      "is_binary": false,
      "last_modified": "2025-08-02 09:07:28",
      "path": "adaptive_bayesian_driver/utils/compute_optimizations.py",
      "extension": ".py",
      "filename": "compute_optimizations.py"
    },
    {
      "size_kb": 1.05,
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nDemo script for adaptive-bayesian-driver package.\r\nTests basic functionality and package import.\r\n\"\"\"\r\n\r\nimport sys\r\nimport os\r\n\r\n# Add current directory to Python path for development\r\nsys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\r\n\r\ntry:\r\n    import adaptive_bayesian_driver\r\n    print(\"✓ Package import successful\")\r\n    print(f\"✓ Version: {adaptive_bayesian_driver.__version__}\")\r\n    print(f\"✓ Author: {adaptive_bayesian_driver.__author__}\")\r\n    print(f\"✓ CUDA Available: {adaptive_bayesian_driver.CUDA_AVAILABLE}\")\r\n    print(f\"✓ Device: {adaptive_bayesian_driver.DEVICE}\")\r\n\r\n    # Test device utility\r\n    from adaptive_bayesian_driver.utils.device import (\r\n        get_device,\r\n        get_device_info\r\n    )\r\n    device = get_device()\r\n    print(f\"✓ Device utility: {device}\")\r\n    print(f\"✓ Device info: {get_device_info()}\")\r\n\r\n    print(\"\\n🎉 Demo completed successfully!\")\r\n\r\nexcept Exception as e:\r\n    print(f\"❌ Error: {e}\")\r\n    import traceback\r\n    traceback.print_exc()\r\n    sys.exit(1)\r\n",
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nDemo script for adaptive-bayesian-driver package.\r\nTests basic functionality and package import.\r\n\"\"\"\r\n\r\nimport sys\r\nimport os\r\n\r\n# Add current directory to Python path fo...",
      "directory": "adaptive_bayesian_driver\\utils",
      "size_bytes": 1074,
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "path": "adaptive_bayesian_driver/utils/demo.py",
      "extension": ".py",
      "filename": "demo.py"
    },
    {
      "size_kb": 3.44,
      "content": "\"\"\"Advanced GPU device management for CUDA-first ML development.\"\"\"\r\n\r\nimport torch\r\nimport platform\r\nimport sys\r\nfrom typing import Dict, Any, Optional\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef get_device(prefer_cuda: bool = True) -> torch.device:\r\n    \"\"\"\r\n    Get optimal compute device with fallback hierarchy.\r\n\r\n    Args:\r\n        prefer_cuda: Whether to prefer CUDA if available\r\n\r\n    Returns:\r\n        PyTorch device optimized for current hardware\r\n    \"\"\"\r\n    if prefer_cuda and torch.cuda.is_available():\r\n        device = torch.device('cuda')\r\n        logger.info(f\"Using CUDA device: {torch.cuda.get_device_name()}\")\r\n        return device\r\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\r\n        device = torch.device('mps')\r\n        logger.info(\"Using Apple Silicon MPS device\")\r\n        return device\r\n    else:\r\n        device = torch.device('cpu')\r\n        logger.info(\"Using CPU device\")\r\n        return device\r\n\r\n\r\ndef setup_cuda_environment() -> Dict[str, Any]:\r\n    \"\"\"\r\n    Comprehensive CUDA environment setup and diagnostics.\r\n\r\n    Returns:\r\n        Complete device information dictionary\r\n    \"\"\"\r\n    device_info = {\r\n        'cuda_available': torch.cuda.is_available(),\r\n        'cuda_version': torch.version.cuda,\r\n        'cudnn_version': (torch.backends.cudnn.version()\r\n                          if torch.backends.cudnn.is_available() else None),\r\n        'device_count': (torch.cuda.device_count()\r\n                         if torch.cuda.is_available() else 0),\r\n        'devices': []\r\n    }\r\n\r\n    if torch.cuda.is_available():\r\n        for i in range(torch.cuda.device_count()):\r\n            device_props = torch.cuda.get_device_properties(i)\r\n            major_minor = f\"{device_props.major}.{device_props.minor}\"\r\n            device_info['devices'].append({\r\n                'index': i,\r\n                'name': device_props.name,\r\n                'total_memory': device_props.total_memory,\r\n                'memory_allocated': torch.cuda.memory_allocated(i),\r\n                'memory_reserved': torch.cuda.memory_reserved(i),\r\n                'compute_capability': major_minor\r\n            })\r\n\r\n        # Set memory allocation strategy for better performance\r\n        torch.cuda.empty_cache()\r\n        if hasattr(torch.cuda, 'set_per_process_memory_fraction'):\r\n            torch.cuda.set_per_process_memory_fraction(0.8)\r\n\r\n    # Add system information\r\n    device_info['system'] = {\r\n        'platform': platform.platform(),\r\n        'python_version': sys.version,\r\n        'torch_version': torch.__version__,\r\n        'cpu_count': torch.get_num_threads()\r\n    }\r\n\r\n    return device_info\r\n\r\n\r\ndef optimize_cuda_performance():\r\n    \"\"\"Apply CUDA performance optimizations.\"\"\"\r\n    if not torch.cuda.is_available():\r\n        logger.warning(\"CUDA not available - skipping optimizations\")\r\n        return\r\n\r\n    # Enable optimizations\r\n    torch.backends.cudnn.benchmark = True\r\n    torch.backends.cudnn.deterministic = False\r\n\r\n    # Memory management\r\n    torch.cuda.empty_cache()\r\n\r\n    logger.info(\"CUDA performance optimizations applied\")\r\n\r\n\r\ndef get_device_info() -> str:\r\n    \"\"\"\r\n    Get information about the current device.\r\n\r\n    Returns:\r\n        Device information string\r\n    \"\"\"\r\n    device = get_device()\r\n    if device.type == 'cuda':\r\n        return f\"CUDA device: {torch.cuda.get_device_name()}\"\r\n    elif device.type == 'mps':\r\n        return \"Apple Silicon MPS device\"\r\n    else:\r\n        return \"CPU device\"\r\n",
      "content_preview": "\"\"\"Advanced GPU device management for CUDA-first ML development.\"\"\"\r\n\r\nimport torch\r\nimport platform\r\nimport sys\r\nfrom typing import Dict, Any, Optional\r\nimport logging\r\n\r\nlogger = logging.getLogger(_...",
      "directory": "adaptive_bayesian_driver\\utils",
      "size_bytes": 3527,
      "is_binary": false,
      "last_modified": "2025-07-16 05:39:22",
      "path": "adaptive_bayesian_driver/utils/device.py",
      "extension": ".py",
      "filename": "device.py"
    },
    {
      "size_kb": 9.52,
      "content": "# Repository to JSON Exporter - Environment Agnostic\r\n# Usage: .\\Export-JsonRepo.ps1 -RepositoryPath \".\" -OutputPath \"backup.json\"\r\n# If no OutputPath specified, auto-generates: repo_export_{reponame}_{timestamp}.json\r\n\r\n\r\n# Repository to JSON Exporter - Environment Agnostic\r\nparam(\r\n    [string]$RepositoryPath = \".\",\r\n    [string]$OutputPath = \"\",  # Will auto-generate if not provided\r\n    [switch]$IncludeAllFiles = $false,\r\n    [int]$MaxFileSizeKB = 10000,  # Skip files larger than\r\n    [switch]$Verbose = $true,   # Show excluded files\r\n    [switch]$Debug = $false      # Show detailed filtering info\r\n)\r\n\r\n# Define file extensions to include (when not using -IncludeAllFiles)\r\n$IncludedExtensions = @(\r\n    '.py', '.md', '.yaml', '.yml', '.ipynb', '.cfg', '.env',\r\n    '.xml', '.html', '.css', '.js', '.ts', '.sql', '.sh',\r\n    '.bat', '.ps1', '.ini', '.conf', '.log', '.csv', '.toml', '.lock',\r\n    '.gitignore', '.dockerignore', '.editorconfig', '.json'\r\n    # '.txt',\r\n)\r\n\r\n# Directories to exclude\r\n#\r\n$ExcludedDirs = @('_snapshot', 'Modules', 'site-packages', '_backup', '_dev', '_cop', '_depr', '_dev', '_gem', '_prp', 'txt', '.git', '.vscode', '__pycache__', 'node_modules', '.pytest_cache', '_mypy_cache', '.mypy_cache', 'venv', 'env', '.env', 'dist', 'build', '.ruff_cache', '.black_cache')\r\n\r\nfunction Test-IsBinaryFile {\r\n    param([string]$FilePath)\r\n\r\n    try {\r\n        $bytes = [System.IO.File]::ReadAllBytes($FilePath)\r\n        if ($bytes.Length -eq 0) { return $false }\r\n\r\n        # Check first 1024 bytes for null characters (common in binary files)\r\n        $checkLength = [Math]::Min(1024, $bytes.Length)\r\n        for ($i = 0; $i -lt $checkLength; $i++) {\r\n            if ($bytes[$i] -eq 0) { return $true }\r\n        }\r\n        return $false\r\n    }\r\n    catch {\r\n        return $true  # If we can't read it, assume it's binary\r\n    }\r\n}\r\n\r\nfunction Get-FileContent {\r\n    param([string]$FilePath)\r\n\r\n    try {\r\n        if (Test-IsBinaryFile -FilePath $FilePath) {\r\n            return \"[BINARY FILE - Content not included]\"\r\n        }\r\n\r\n        # Try UTF-8 first, then fall back to default encoding\r\n        try {\r\n            return [System.IO.File]::ReadAllText($FilePath, [System.Text.Encoding]::UTF8)\r\n        }\r\n        catch {\r\n            return [System.IO.File]::ReadAllText($FilePath)\r\n        }\r\n    }\r\n    catch {\r\n        return \"[ERROR: Could not read file - $($_.Exception.Message)]\"\r\n    }\r\n}\r\n\r\n# Resolve repository path to absolute path\r\ntry {\r\n    $resolvedRepoPath = (Resolve-Path $RepositoryPath -ErrorAction Stop).Path\r\n}\r\ncatch {\r\n    Write-Host \"Error: Cannot resolve repository path '$RepositoryPath'\" -ForegroundColor Red\r\n    exit 1\r\n}\r\n\r\n# Auto-generate output path if not provided\r\nif ([string]::IsNullOrWhiteSpace($OutputPath)) {\r\n    $repoName = Split-Path $resolvedRepoPath -Leaf\r\n    $timestamp = Get-Date -Format \"yyyyMMdd_HHmmss\"\r\n    $OutputPath = Join-Path $resolvedRepoPath \"${repoName}_${timestamp}.json\"\r\n}\r\nelse {\r\n    # If OutputPath is relative, make it relative to the repository\r\n    if (-not [System.IO.Path]::IsPathRooted($OutputPath)) {\r\n        $OutputPath = Join-Path $resolvedRepoPath $OutputPath\r\n    }\r\n}\r\n\r\n# Ensure output directory exists\r\n$outputDir = Split-Path $OutputPath -Parent\r\nif (-not (Test-Path $outputDir)) {\r\n    try {\r\n        New-Item -ItemType Directory -Path $outputDir -Force -ErrorAction Stop | Out-Null\r\n        Write-Host \"Created output directory: $outputDir\" -ForegroundColor Cyan\r\n    }\r\n    catch {\r\n        Write-Host \"Error: Cannot create output directory '$outputDir': $($_.Exception.Message)\" -ForegroundColor Red\r\n        exit 1\r\n    }\r\n}\r\n\r\nWrite-Host \"Scanning repository: $resolvedRepoPath\" -ForegroundColor Green\r\nWrite-Host \"Output file: $OutputPath\" -ForegroundColor Green\r\n\r\n# Initialize the export structure\r\n$exportData = @{\r\n    metadata = @{\r\n        export_date        = Get-Date -Format \"yyyy-MM-dd HH:mm:ss\"\r\n        repository_path    = $resolvedRepoPath\r\n        repository_name    = Split-Path $resolvedRepoPath -Leaf\r\n        total_files        = 0\r\n        exported_files     = 0\r\n        skipped_files      = 0\r\n        script_version     = \"1.1\"\r\n        powershell_version = $PSVersionTable.PSVersion.ToString()\r\n    }\r\n    files    = @()\r\n}\r\n\r\n# Get all files recursively with improved exclusion logic\r\n$allFiles = Get-ChildItem -Path $resolvedRepoPath -Recurse -File -ErrorAction SilentlyContinue | Where-Object {\r\n    # Exclude files in excluded directories - improved logic\r\n    $relativePath = $_.FullName.Replace($resolvedRepoPath, \"\").TrimStart('\\', '/').Replace('\\', '/')\r\n    $inExcludedDir = $false\r\n\r\n    foreach ($excludedDir in $ExcludedDirs) {\r\n        # Check multiple patterns to catch all variations\r\n        $patterns = @(\r\n            \"$excludedDir/*\",           # Direct child: mypy_cache/file.txt\r\n            \"*/$excludedDir/*\",         # Nested: something/mypy_cache/file.txt\r\n            \"$excludedDir\\*\",           # Windows separator\r\n            \"*\\$excludedDir\\*\",         # Windows nested\r\n            \"*/$excludedDir\",           # Directory itself\r\n            \"*\\$excludedDir\"            # Directory itself (Windows)\r\n        )\r\n\r\n        foreach ($pattern in $patterns) {\r\n            if ($relativePath -like $pattern) {\r\n                if ($Debug) { Write-Host \"EXCLUDED: $relativePath (matched pattern: $pattern for dir: $excludedDir)\" -ForegroundColor Red }\r\n                elseif ($Verbose) { Write-Host \"Excluding: $relativePath\" -ForegroundColor Yellow }\r\n                $inExcludedDir = $true\r\n                break\r\n            }\r\n        }\r\n        if ($inExcludedDir) { break }\r\n    }\r\n\r\n    return -not $inExcludedDir\r\n}\r\n\r\n$exportData.metadata.total_files = $allFiles.Count\r\nWrite-Host \"Found $($allFiles.Count) files to process\" -ForegroundColor Cyan\r\n\r\nif ($Debug) {\r\n    Write-Host \"`nExcluded directories: $($ExcludedDirs -join ', ')\" -ForegroundColor Yellow\r\n    $testFiles = Get-ChildItem -Path $resolvedRepoPath -Recurse -File -ErrorAction SilentlyContinue | Where-Object { $_.FullName -like \"*mypy_cache*\" } | Select-Object -First 3\r\n    if ($testFiles) {\r\n        Write-Host \"`nFound mypy_cache files:\" -ForegroundColor Magenta\r\n        foreach ($tf in $testFiles) {\r\n            $testRelPath = $tf.FullName.Replace($resolvedRepoPath, \"\").TrimStart('\\', '/').Replace('\\', '/')\r\n            Write-Host \"  $testRelPath\" -ForegroundColor Magenta\r\n        }\r\n    }\r\n\r\n    # Count excluded vs included files\r\n    $allFilesDebug = Get-ChildItem -Path $resolvedRepoPath -Recurse -File -ErrorAction SilentlyContinue\r\n    Write-Host \"`nTotal files before filtering: $($allFilesDebug.Count)\" -ForegroundColor Cyan\r\n    Write-Host \"Files after exclusion filtering: $($allFiles.Count)\" -ForegroundColor Cyan\r\n    Write-Host \"Files filtered out: $($allFilesDebug.Count - $allFiles.Count)\" -ForegroundColor Yellow\r\n}\r\n\r\nforeach ($file in $allFiles) {\r\n    $relativePath = $file.FullName.Replace($resolvedRepoPath, \"\").TrimStart('\\', '/').Replace('\\', '/')\r\n    $extension = $file.Extension.ToLower()\r\n    $fileSizeKB = [Math]::Round($file.Length / 1KB, 2)\r\n\r\n    # Skip large files\r\n    if ($fileSizeKB -gt $MaxFileSizeKB) {\r\n        Write-Host \"Skipping large file: $relativePath ($fileSizeKB KB)\" -ForegroundColor Yellow\r\n        $exportData.metadata.skipped_files++\r\n        continue\r\n    }\r\n\r\n    # Check if file should be included\r\n    $shouldInclude = $IncludeAllFiles -or ($extension -in $IncludedExtensions) -or ($file.Name -in @('.gitignore', '.dockerignore', '.editorconfig', 'Dockerfile', 'Makefile', 'README'))\r\n\r\n    if (-not $shouldInclude) {\r\n        $exportData.metadata.skipped_files++\r\n        continue\r\n    }\r\n\r\n    Write-Host \"Processing: $relativePath\" -ForegroundColor Gray\r\n\r\n    # Get file content\r\n    $content = Get-FileContent -FilePath $file.FullName\r\n\r\n    # Create file object\r\n    $fileObject = @{\r\n        path            = $relativePath\r\n        directory       = Split-Path $relativePath -Parent\r\n        filename        = $file.Name\r\n        extension       = $extension\r\n        size_bytes      = $file.Length\r\n        size_kb         = $fileSizeKB\r\n        last_modified   = $file.LastWriteTime.ToString(\"yyyy-MM-dd HH:mm:ss\")\r\n        content         = $content\r\n        content_preview = if ($content.Length -gt 200) { $content.Substring(0, 200) + \"...\" } else { $content }\r\n        is_binary       = Test-IsBinaryFile -FilePath $file.FullName\r\n    }\r\n\r\n    $exportData.files += $fileObject\r\n    $exportData.metadata.exported_files++\r\n}\r\n\r\n# Sort files by path for better organization\r\n$exportData.files = $exportData.files | Sort-Object path\r\n\r\nWrite-Host \"`nExport Summary:\" -ForegroundColor Green\r\nWrite-Host \"Total files found: $($exportData.metadata.total_files)\" -ForegroundColor White\r\nWrite-Host \"Files exported: $($exportData.metadata.exported_files)\" -ForegroundColor Green\r\nWrite-Host \"Files skipped: $($exportData.metadata.skipped_files)\" -ForegroundColor Yellow\r\n\r\n# Export to JSON with enhanced error handling\r\ntry {\r\n    $jsonOutput = $exportData | ConvertTo-Json -Depth 10 -Compress:$false\r\n    [System.IO.File]::WriteAllText($OutputPath, $jsonOutput, [System.Text.Encoding]::UTF8)\r\n    Write-Host \"`nRepository exported successfully to: $OutputPath\" -ForegroundColor Green\r\n\r\n    # Show file size\r\n    $outputSize = [Math]::Round((Get-Item $OutputPath).Length / 1KB, 2)\r\n    Write-Host \"Output file size: $outputSize KB\" -ForegroundColor Cyan\r\n\r\n    # Return the output path for potential pipeline use\r\n    return $OutputPath\r\n}\r\ncatch {\r\n    Write-Host \"Error exporting to JSON: $($_.Exception.Message)\" -ForegroundColor Red\r\n    Write-Host \"Attempted output path: $OutputPath\" -ForegroundColor Yellow\r\n    exit 1\r\n}\r\n",
      "content_preview": "# Repository to JSON Exporter - Environment Agnostic\r\n# Usage: .\\Export-JsonRepo.ps1 -RepositoryPath \".\" -OutputPath \"backup.json\"\r\n# If no OutputPath specified, auto-generates: repo_export_{reponame}...",
      "directory": "adaptive_bayesian_driver\\utils",
      "size_bytes": 9751,
      "is_binary": false,
      "last_modified": "2025-08-02 09:48:03",
      "path": "adaptive_bayesian_driver/utils/Export-JsonRepo.ps1",
      "extension": ".ps1",
      "filename": "Export-JsonRepo.ps1"
    },
    {
      "size_kb": 0.34,
      "content": "\"\"\"\r\nVisualization utilities for adaptive Bayesian learning.\r\n\"\"\"\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport numpy as np\r\nfrom typing import Dict, List, Optional\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n# This file was restored after being corrupted\r\n# Content needs to be reimplemented based on project needs\r\n",
      "content_preview": "\"\"\"\r\nVisualization utilities for adaptive Bayesian learning.\r\n\"\"\"\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport numpy as np\r\nfrom typing import Dict, List, Optional\r\nimport logging...",
      "directory": "adaptive_bayesian_driver\\utils",
      "size_bytes": 352,
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "path": "adaptive_bayesian_driver/utils/visualization.py",
      "extension": ".py",
      "filename": "visualization.py"
    },
    {
      "size_kb": 0.26,
      "content": "# Changelog\r\n\r\n## [v0.10 - 2025-07-12]\r\n\r\n    Changed: Repo to with cleanup selections from in wip project \r\n    Fixed: Various minor adjustments in ported files, wip\r\n\r\n..\r\n\r\n## [0.0.01 2025-07-01] \r\nSkeletal prototype created via Ai Promptineering e.g. \"vibe coding\"\r\n",
      "content_preview": "# Changelog\r\n\r\n## [v0.10 - 2025-07-12]\r\n\r\n    Changed: Repo to with cleanup selections from in wip project \r\n    Fixed: Various minor adjustments in ported files, wip\r\n\r\n..\r\n\r\n## [0.0.01 2025-07-01] \r...",
      "directory": "",
      "size_bytes": 270,
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "path": "changelog.md",
      "extension": ".md",
      "filename": "changelog.md"
    },
    {
      "size_kb": 0.0,
      "content": "",
      "content_preview": "",
      "directory": "",
      "size_bytes": 0,
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "path": "CONTRIBUTING.md",
      "extension": ".md",
      "filename": "CONTRIBUTING.md"
    },
    {
      "size_kb": 1.37,
      "content": "# =============================================================================\r\n# Multi-stage Docker build for adaptive-bayesian-driver\r\n# Optimized for development and demonstration purposes\r\n# =============================================================================\r\n\r\n# Build stage - install dependencies and build tools\r\nFROM python:3.11-slim as builder\r\n\r\nWORKDIR /usr/src/app\r\n\r\n# Install system dependencies for PyTorch compilation\r\nRUN apt-get update && apt-get install -y \\\r\n    build-essential \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n# Copy requirements and install Python dependencies\r\nCOPY requirements.txt .\r\nRUN pip install --no-cache-dir --user -r requirements.txt\r\n\r\n# Production stage - minimal runtime environment\r\nFROM python:3.11-slim\r\n\r\nWORKDIR /usr/src/app\r\n\r\n# Create non-root user for security\r\nRUN groupadd --system app && useradd --system --group app\r\n\r\n# Copy installed packages from builder stage\r\nCOPY --from=builder /root/.local /home/app/.local\r\n\r\n# Copy application code\r\nCOPY ./adaptive_bayesian_driver ./adaptive_bayesian_driver\r\nCOPY ./config ./config\r\nCOPY ./demo.py .\r\n\r\n# Set ownership and switch to non-root user\r\nRUN chown -R app:app /usr/src/app\r\nUSER app\r\n\r\n# Add local Python packages to PATH\r\nENV PATH=/home/app/.local/bin:$PATH\r\n\r\n# Expose port for potential API serving\r\nEXPOSE 8000\r\n\r\n# Default command runs the demo\r\nCMD [\"python\", \"demo.py\"]\r\n",
      "content_preview": "# =============================================================================\r\n# Multi-stage Docker build for adaptive-bayesian-driver\r\n# Optimized for development and demonstration purposes\r\n# ====...",
      "directory": "",
      "size_bytes": 1399,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": "Dockerfile",
      "extension": "",
      "filename": "Dockerfile"
    },
    {
      "size_kb": 3.45,
      "content": "[build-system]\r\nrequires = [\"setuptools>=61.0\", \"wheel\"]\r\nbuild-backend = \"setuptools.build_meta\"\r\n\r\n[project]\r\nname = \"adaptive-bayesian-driver\"\r\nversion = \"0.2.0\"\r\ndescription = \"LC-NE inspired adaptive Bayesian learning for autonomous driving\"\r\nauthors = [{name = \"Azriel Ghadooshahy\", email = \"azriel.ghadooshahy@example.com\"}]\r\nreadme = \"README.md\"\r\nrequires-python = \">=3.11\"\r\nlicense = {text = \"MIT\"}\r\nkeywords = [\"machine-learning\", \"bayesian\", \"autonomous-driving\", \"neuroscience\", \"cuda\"]\r\nclassifiers = [\r\n    \"Development Status :: 4 - Beta\",\r\n    \"Intended Audience :: Science/Research\",\r\n    \"License :: OSI Approved :: MIT License\",\r\n    \"Programming Language :: Python :: 3.11\",\r\n    \"Programming Language :: Python :: 3.12\",\r\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\r\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\r\n]\r\n\r\ndependencies = [\r\n    \"torch>=2.2.0,<2.4.0\",\r\n    \"torchvision>=0.17.0,<0.19.0\",\r\n    \"numpy>=1.24.0,<1.27.0\",\r\n    \"matplotlib>=3.6.0,<3.9.0\",\r\n    \"pillow>=9.0.0,<11.0.0\",\r\n    \"pyyaml>=6.0.0,<7.0.0\",\r\n    \"tqdm>=4.66.0,<5.0.0\",\r\n]\r\n\r\n[project.optional-dependencies]\r\ndev = [\r\n    \"pytest>=8.0.0,<9.0.0\",\r\n    \"pytest-cov>=5.0.0,<6.0.0\",\r\n    \"ruff>=0.4.4,<0.5.0\",\r\n    \"mypy>=1.10.0,<2.0.0\",\r\n    \"pre-commit>=3.7.0,<4.0.0\",\r\n]\r\n\r\ncuda = [\r\n    \"cupy-cuda12x>=12.0.0,<13.0.0\",\r\n]\r\n\r\nproduction = [\r\n    \"mlflow>=2.13.0,<3.0.0\",\r\n    \"hydra-core>=1.3.0,<2.0.0\",\r\n    \"wandb>=0.17.0,<1.0.0\",\r\n    \"fastapi>=0.111.0,<1.0.0\",\r\n    \"uvicorn[standard]>=0.29.0,<1.0.0\",\r\n    \"onnx>=1.16.0,<2.0.0\",\r\n    \"onnxruntime-gpu>=1.18.0,<2.0.0\",\r\n]\r\n\r\n[project.scripts]\r\nadaptive-demo = \"adaptive_bayesian_driver.main:main\"\r\nadaptive-train = \"adaptive_bayesian_driver.applications.training:main\"\r\n\r\n[tool.ruff]\r\nline-length = 88\r\ntarget-version = \"py311\"\r\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\", \"UP\", \"B\", \"C4\", \"SIM\", \"TCH\"]\r\nignore = [\"E501\", \"N803\", \"N806\", \"B008\", \"UP007\"]\r\nexclude = [\".git\", \"__pycache__\", \"build\", \"dist\", \".eggs\", \"_chatbot\"]\r\n\r\n[tool.ruff.format]\r\nquote-style = \"double\"\r\nindent-style = \"space\"\r\nskip-magic-trailing-comma = false\r\n\r\n[tool.ruff.isort]\r\nknown-first-party = [\"adaptive_bayesian_driver\"]\r\nforce-single-line = false\r\n\r\n[tool.mypy]\r\npython_version = \"3.11\"\r\nwarn_return_any = true\r\nwarn_unused_configs = true\r\ndisallow_untyped_defs = true\r\nignore_missing_imports = true\r\nexclude = [\"_chatbot/\", \"build/\", \"dist/\", \"tests/\"]\r\n\r\n[tool.pytest.ini_options]\r\ntestpaths = [\"tests\"]\r\npython_files = [\"test_*.py\"]\r\npython_classes = [\"Test*\"]\r\npython_functions = [\"test_*\"]\r\naddopts = [\r\n    \"--strict-markers\",\r\n    \"--strict-config\",\r\n    \"--cov=adaptive_bayesian_driver\",\r\n    \"--cov-report=term-missing\",\r\n    \"--cov-report=xml\",\r\n    \"--cov-fail-under=80\"\r\n]\r\nmarkers = [\r\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\r\n    \"gpu: marks tests as requiring GPU\",\r\n    \"integration: marks tests as integration tests\",\r\n    \"unit: marks tests as unit tests\",\r\n]\r\n\r\n[tool.coverage.run]\r\nsource = [\"adaptive_bayesian_driver\"]\r\nomit = [\"*/tests/*\", \"*/_chatbot/*\", \"*/setup.py\", \"*/demo.py\"]\r\n\r\n[tool.coverage.report]\r\nexclude_lines = [\r\n    \"pragma: no cover\",\r\n    \"def __repr__\",\r\n    \"if self.debug:\",\r\n    \"if settings.DEBUG\",\r\n    \"raise AssertionError\",\r\n    \"raise NotImplementedError\",\r\n    \"if 0:\",\r\n    \"if __name__ == .__main__.:\",\r\n    \"class .*\\\\\\\\bProtocol\\\\$\\\\$:\",\r\n    \"@(abc\\\\.)?abstractmethod\",\r\n]\r\n\r\n[project.urls]\r\n\"Bug Reports\" = \"https://github.com/yourusername/adaptive-bayesian-driver/issues\"\r\n",
      "content_preview": "[build-system]\r\nrequires = [\"setuptools>=61.0\", \"wheel\"]\r\nbuild-backend = \"setuptools.build_meta\"\r\n\r\n[project]\r\nname = \"adaptive-bayesian-driver\"\r\nversion = \"0.2.0\"\r\ndescription = \"LC-NE inspired adap...",
      "directory": "",
      "size_bytes": 3533,
      "is_binary": false,
      "last_modified": "2025-07-16 21:42:12",
      "path": "pyproject.toml",
      "extension": ".toml",
      "filename": "pyproject.toml"
    },
    {
      "size_kb": 11.67,
      "content": "Adaptive Bayesian Driver: Bridging Neuroscience and Computer Vision\nProject Overview\nAdaptive Bayesian Driver is a demonstration project that bridges biological uncertainty modeling principles from neuroscience to computer vision applications in autonomous driving. This work translates dual-timescale uncertainty encoding from the locus coeruleus-norepinephrine (LC-NE) system into geometric priors for neural network perception, directly connecting to Helm.ai's Deep Teaching methodology.\n\nCore Innovation: From Biological Vision to Artificial Perception\nThis project demonstrates how biological uncertainty processing can inform artificial vision systems through:\n\nDual-Timescale Uncertainty Modeling\nBased on my COSYNE 2014 research, this implementation captures uncertainty at two distinct temporal scales:\n\nWithin-trial uncertainty: Real-time confidence in individual predictions\n\nAcross-trial uncertainty: Contextual adaptation based on environmental volatility\n\nGeometric Priors in Latent Space\nMoving beyond traditional classification, we implement:\n\nUncertainty-aware latent representations that encode geometric structure\n\nBayesian neural layers for principled uncertainty quantification\n\nAdaptive learning dynamics modulated by confidence estimates\n# Adaptive Bayesian Driver: From Neuroscience to Autonomous Driving\n\n## 🎯 Current Implementation Status\n\n### ✅ Phase 1: Proof of Concept (In Progress)\n- **MNIST uncertainty classifier** with LC-NE inspired dual-timescale dynamics\n- **Intel compute optimization** for 8-core development system\n- **Geometric priors** in latent space using manifold learning\n- **Professional UQ evaluation** using UNIQUE + NIST frameworks\n\n### 🔮 Phase 2: CARLA Simulation (DESIGNED)\n- Multi-modal sensor fusion architecture\n- Real-time uncertainty propagation\n- Safety-critical decision making framework\n- Integration with Helm.ai Deep Teaching methodology\n\n### 🏭 Phase 3: Production Deployment (PLANNED)\n- Edge optimization for vehicle compute platforms\n- ISO 26262 safety compliance\n- Fleet-scale monitoring and validation\n- Continuous learning from real-world driving data\n\n## 🚀 Quick Start (Current Demo)\n\nTechnical Architecture\nMinimum Viable Demo: MNIST Uncertainty Classification\ntext\nadaptive_bayesian_driver/\n├── models/\n│   ├── uncertainty_cnn.py      # Bayesian CNN with Monte Carlo Dropout\n│   ├── geometric_prior.py      # Latent space constraints\n│   └── dual_timescale.py       # LC-NE inspired uncertainty dynamics\n├── experiments/\n│   └── mnist_demo.ipynb        # Interactive demonstration notebook\n├── utils/\n│   ├── uncertainty_metrics.py  # KL divergence, Mahalanobis distance\n│   └── visualization.py        # Uncertainty evolution plots\n└── config/\n    └── experiment_config.yaml  # Hyperparameters and settings\nKey Components\n1. Uncertainty-Aware CNN Architecture\nMonte Carlo Dropout for epistemic uncertainty estimation\n\nBayesian layers for aleatoric uncertainty quantification\n\nConfidence-based learning rate adaptation\n\n2. Biological Inspiration Integration\nLC-NE dual-timescale dynamics applied to prediction confidence\n\nContextual priors that adapt based on prediction history\n\nExploration/exploitation balance informed by uncertainty estimates\n\n3. Geometric Latent Space\nVAE-style encoding with geometric constraints\n\nManifold learning for structured uncertainty representation\n\nPhysics-informed priors for autonomous driving relevance\n\nConnection to Helm.ai's Technology Stack\nDeep Teaching Methodology Alignment\nThis project directly connects to Helm.ai's core principles:\n\nUnsupervised Learning: Geometric priors enable learning without extensive labeled data\n\nGenerative Priors: Latent space structure encodes environmental understanding\n\nNon-convex Optimization: Bayesian inference provides guarantees in complex landscapes\n\nAutonomous Driving Applications\nThe MNIST demo serves as proof-of-concept for:\n\nMulti-modal sensor fusion (extending to LIDAR, camera, radar)\n\nReal-time uncertainty quantification for safety-critical decisions\n\nAdaptive perception in changing environmental conditions\n\nUnique Value Proposition\nBiological Systems Perspective\nUnlike traditional CS/ML approaches, this work brings:\n\nSystems neuroscience insights into adaptive behavior\n\nCybernetics principles for feedback-driven learning\n\nDual-timescale modeling from computational neuroscience\n\nResearch-to-Application Bridge\nTheoretical foundation: Bayesian inference and uncertainty encoding\n\nPractical implementation: PyTorch-based neural networks\n\nIndustrial relevance: Autonomous driving safety and perception\n\nFuture Roadmap\nPhase 1: MNIST Proof-of-Concept ✅\n Uncertainty-aware classification\n\n Geometric latent space implementation\n\n Biological dynamics integration\n\nPhase 2: Computer Vision Extension 🔄\n CARLA simulation environment integration\n\n Multi-modal sensor fusion architecture\n\n Real-time perception pipeline\n\nPhase 3: Production Deployment 🔮\n Hardware-agnostic implementation\n\n Edge computing optimization\n\n Vehicle platform integration\n\nTechnical Implementation Notes\nMathematical Foundation\npython\n# Dual-timescale uncertainty update (inspired by LC-NE dynamics)\nwithin_trial_uncertainty = monte_carlo_dropout(model, x)\nacross_trial_uncertainty = bayesian_update(prior_context, prediction_history)\ncombined_uncertainty = geometric_prior_fusion(within_trial, across_trial)\nKey Dependencies\nPyTorch: Neural network implementation and training\n\nNumPy: Mathematical operations and array handling\n\nSciPy: Statistical distributions and optimization\n\nMatplotlib/Seaborn: Uncertainty visualization\n\nInstallation & Usage\nbash\n# Clone repository\ngit clone https://github.com/yourusername/adaptive-bayesian-driver.git\ncd adaptive-bayesian-driver\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run MNIST demonstration\njupyter notebook experiments/mnist_demo.ipynb\nResearch Background\nThis project builds on my published work in computational neuroscience, specifically:\n\nContextual uncertainty modeling in primate locus coeruleus\n\nDual-timescale Bayesian inference for perceptual decision-making\n\nBiological vision principles applied to artificial systems\n\nThe connection to Helm.ai's technology philosophy stems from shared interests in uncertainty-aware perception, biological inspiration, and principled approaches to autonomous driving challenges.\n\n\nAdaptive Bayesian Driver: Bridging Neuroscience and Computer Vision\nProject Overview\nAdaptive Bayesian Driver is a demonstration project that bridges biological uncertainty modeling principles from neuroscience to computer vision applications in autonomous driving. This work translates dual-timescale uncertainty encoding from the locus coeruleus-norepinephrine (LC-NE) system into geometric priors for neural network perception, directly connecting to Helm.ai's Deep Teaching methodology.\n\nCore Innovation: From Biological Vision to Artificial Perception\nThis project demonstrates how biological uncertainty processing can inform artificial vision systems through:\n\nDual-Timescale Uncertainty Modeling\nBased on my COSYNE 2014 research, this implementation captures uncertainty at two distinct temporal scales:\n\nWithin-trial uncertainty: Real-time confidence in individual predictions\n\nAcross-trial uncertainty: Contextual adaptation based on environmental volatility\n\nGeometric Priors in Latent Space\nMoving beyond traditional classification, we implement:\n\nUncertainty-aware latent representations that encode geometric structure\n\nBayesian neural layers for principled uncertainty quantification\n\nAdaptive learning dynamics modulated by confidence estimates\n\nTechnical Architecture\nMinimum Viable Demo: MNIST Uncertainty Classification\ntext\nadaptive_bayesian_driver/\n├── models/\n│   ├── uncertainty_cnn.py      # Bayesian CNN with Monte Carlo Dropout\n│   ├── geometric_prior.py      # Latent space constraints\n│   └── dual_timescale.py       # LC-NE inspired uncertainty dynamics\n├── experiments/\n│   └── mnist_demo.ipynb        # Interactive demonstration notebook\n├── utils/\n│   ├── uncertainty_metrics.py  # KL divergence, Mahalanobis distance\n│   └── visualization.py        # Uncertainty evolution plots\n└── config/\n    └── experiment_config.yaml  # Hyperparameters and settings\nKey Components\n1. Uncertainty-Aware CNN Architecture\nMonte Carlo Dropout for epistemic uncertainty estimation\n\nBayesian layers for aleatoric uncertainty quantification\n\nConfidence-based learning rate adaptation\n\n2. Biological Inspiration Integration\nLC-NE dual-timescale dynamics applied to prediction confidence\n\nContextual priors that adapt based on prediction history\n\nExploration/exploitation balance informed by uncertainty estimates\n\n3. Geometric Latent Space\nVAE-style encoding with geometric constraints\n\nManifold learning for structured uncertainty representation\n\nPhysics-informed priors for autonomous driving relevance\n\nConnection to Helm.ai's Technology Stack\nDeep Teaching Methodology Alignment\nThis project directly connects to Helm.ai's core principles:\n\nUnsupervised Learning: Geometric priors enable learning without extensive labeled data\n\nGenerative Priors: Latent space structure encodes environmental understanding\n\nNon-convex Optimization: Bayesian inference provides guarantees in complex landscapes\n\nAutonomous Driving Applications\nThe MNIST demo serves as proof-of-concept for:\n\nMulti-modal sensor fusion (extending to LIDAR, camera, radar)\n\nReal-time uncertainty quantification for safety-critical decisions\n\nAdaptive perception in changing environmental conditions\n\nUnique Value Proposition\nBiological Systems Perspective\nUnlike traditional CS/ML approaches, this work brings:\n\nSystems neuroscience insights into adaptive behavior\n\nCybernetics principles for feedback-driven learning\n\nDual-timescale modeling from computational neuroscience\n\nResearch-to-Application Bridge\nTheoretical foundation: Bayesian inference and uncertainty encoding\n\nPractical implementation: PyTorch-based neural networks\n\nIndustrial relevance: Autonomous driving safety and perception\n\nFuture Roadmap\nPhase 1: MNIST Proof-of-Concept ✅\n Uncertainty-aware classification\n\n Geometric latent space implementation\n\n Biological dynamics integration\n\nPhase 2: Computer Vision Extension 🔄\n CARLA simulation environment integration\n\n Multi-modal sensor fusion architecture\n\n Real-time perception pipeline\n\nPhase 3: Production Deployment 🔮\n Hardware-agnostic implementation\n\n Edge computing optimization\n\n Vehicle platform integration\n\nTechnical Implementation Notes\nMathematical Foundation\npython\n# Dual-timescale uncertainty update (inspired by LC-NE dynamics)\nwithin_trial_uncertainty = monte_carlo_dropout(model, x)\nacross_trial_uncertainty = bayesian_update(prior_context, prediction_history)\ncombined_uncertainty = geometric_prior_fusion(within_trial, across_trial)\nKey Dependencies\nPyTorch: Neural network implementation and training\n\nNumPy: Mathematical operations and array handling\n\nSciPy: Statistical distributions and optimization\n\nMatplotlib/Seaborn: Uncertainty visualization\n\nInstallation & Usage\nbash\n# Clone repository\ngit clone https://github.com/yourusername/adaptive-bayesian-driver.git\ncd adaptive-bayesian-driver\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run MNIST demonstration\njupyter notebook experiments/mnist_demo.ipynb\nResearch Background\nThis project builds on my published work in computational neuroscience, specifically:\n\nContextual uncertainty modeling in primate locus coeruleus\n\nDual-timescale Bayesian inference for perceptual decision-making\n\nBiological vision principles applied to artificial systems\n\nThe connection to Helm.ai's technology philosophy stems from shared interests in uncertainty-aware perception, biological inspiration, and principled approaches to autonomous driving challenges.\n",
      "content_preview": "Adaptive Bayesian Driver: Bridging Neuroscience and Computer Vision\nProject Overview\nAdaptive Bayesian Driver is a demonstration project that bridges biological uncertainty modeling principles from ne...",
      "directory": "",
      "size_bytes": 11946,
      "is_binary": false,
      "last_modified": "2025-08-04 23:52:42",
      "path": "README.md",
      "extension": ".md",
      "filename": "README.md"
    },
    {
      "size_kb": 1.18,
      "content": "[metadata]\r\nname = adaptive-bayesian-driver\r\ndescription = LC-NE inspired adaptive Bayesian learning for autonomous driving\r\nlong_description = file: README.md\r\nlong_description_content_type = text/markdown\r\nlicense = MIT\r\nauthor = Your Name\r\nauthor_email = your.email@example.com\r\nclassifiers =\r\n    Development Status :: 3 - Alpha\r\n    Intended Audience :: Science/Research\r\n    License :: OSI Approved :: MIT License\r\n    Programming Language :: Python :: 3.11\r\n    Topic :: Scientific/Engineering :: Artificial Intelligence\r\n\r\n[options]\r\npackages = find:\r\npython_requires = >=3.11\r\ninclude_package_data = True\r\nzip_safe = False\r\n\r\n[options.packages.find]\r\nwhere = .\r\ninclude = adaptive_bayesian_driver*\r\nexclude = tests*\r\n\r\n[options.extras_require]\r\ndev =\r\n    pytest>=8.0.0\r\n    pytest-cov>=5.0.0\r\n    ruff>=0.4.4\r\n    mypy>=1.10.0\r\n    pre-commit>=3.7.0\r\n\r\n[flake8]\r\nmax-line-length = 88\r\nextend-ignore = E203, W503\r\nexclude = .git,__pycache__,build,dist,.eggs\r\n\r\n[coverage:run]\r\nsource = adaptive_bayesian_driver\r\nomit =\r\n    */tests/*\r\n    */test_*\r\n    setup.py\r\n\r\n[coverage:report]\r\nexclude_lines =\r\n    pragma: no cover\r\n    def __repr__\r\n    raise AssertionError\r\n    raise NotImplementedError\r\n",
      "content_preview": "[metadata]\r\nname = adaptive-bayesian-driver\r\ndescription = LC-NE inspired adaptive Bayesian learning for autonomous driving\r\nlong_description = file: README.md\r\nlong_description_content_type = text/ma...",
      "directory": "",
      "size_bytes": 1207,
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "path": "setup.cfg",
      "extension": ".cfg",
      "filename": "setup.cfg"
    },
    {
      "size_kb": 0.35,
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nSetup script for adaptive-bayesian-driver package.\r\nModern Python packaging using setuptools with pyproject.toml configuration.\r\n\"\"\"\r\n\r\nfrom setuptools import setup\r\n\r\n# Configuration is now in pyproject.toml\r\n# This file maintained for backwards compatibility and editable installs\r\nif __name__ == \"__main__\":\r\n    setup()\r\n",
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nSetup script for adaptive-bayesian-driver package.\r\nModern Python packaging using setuptools with pyproject.toml configuration.\r\n\"\"\"\r\n\r\nfrom setuptools import setup\r\n\r\n# C...",
      "directory": "",
      "size_bytes": 354,
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "path": "setup.py",
      "extension": ".py",
      "filename": "setup.py"
    },
    {
      "size_kb": 0.55,
      "content": "# Version control\r\n.git/\r\n.gitignore\r\n\r\n# Python artifacts\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.so\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\n*.egg-info/\r\n.installed.cfg\r\n*.egg\r\n\r\n# Development environments\r\n.env\r\n.venv/\r\nenv/\r\nvenv/\r\nENV/\r\n\r\n# Testing and coverage\r\n.pytest_cache/\r\n.coverage\r\nhtmlcov/\r\n.tox/\r\n\r\n# Documentation\r\ndocs/_build/\r\nnotebooks/\r\n\r\n# IDE files\r\n.vscode/\r\n.idea/\r\n*.swp\r\n*.swo\r\n*~\r\n\r\n# OS files\r\n.DS_Store\r\nThumbs.db\r\n\r\n# Project-specific\r\n_chatbot/\r\n_reports/\r\n*.log\r\n",
      "content_preview": "# Version control\r\n.git/\r\n.gitignore\r\n\r\n# Python artifacts\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.so\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nv...",
      "directory": "tests",
      "size_bytes": 566,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": "tests/.dockerignore",
      "extension": ".dockerignore",
      "filename": ".dockerignore"
    },
    {
      "size_kb": 2.25,
      "content": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\npip-wheel-metadata/\r\nshare/python-wheels/\r\n.installed.cfg\r\n*.egg-info/\r\n*.egg\r\n\r\n# PyInstaller\r\n#  Usually these files are written by a python script from a template\r\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\r\n*.manifest\r\n*.spec\r\n\r\n# Installer logs\r\npip-log.txt\r\npip-delete-this-directory.txt\r\n\r\n# Unit test / coverage reports\r\nhtmlcov/\r\n.tox/\r\n.nox/\r\n.coverage\r\n.coverage.*\r\n.cache\r\nnosetests.xml\r\ncoverage.xml\r\n*.cover\r\n*.py,cover\r\n.hypothesis/\r\n.pytest_cache/\r\n\r\n# Translations\r\n*.mo\r\n*.pot\r\n\r\n# Django stuff:\r\n*.log\r\nlocal_settings.py\r\ndb.sqlite3\r\ndb.sqlite3-journal\r\n\r\n# Flask stuff:\r\ninstance/\r\n.webassets-cache\r\n\r\n# Scrapy stuff:\r\n.scrapy\r\n\r\n# Sphinx documentation\r\ndocs/_build/\r\n\r\n# PyBuilder\r\ntarget/\r\n\r\n# Jupyter Notebook\r\n.ipynb_checkpoints\r\n\r\n# IPython\r\nprofile_default/\r\nipython_config.py\r\n\r\n# pyenv\r\n.python-version\r\n\r\n# pipenv\r\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\r\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\r\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\r\n#   install all needed dependencies.\r\n#Pipfile.lock\r\n\r\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\r\n__pypackages__/\r\n\r\n# Celery stuff\r\ncelerybeat-schedule\r\ncelerybeat.pid\r\n\r\n# SageMath parsed files\r\n*.sage.py\r\n\r\n# Environments\r\n# .env\r\n.venv\r\nenv/\r\nvenv/\r\nENV/\r\nenv.bak/\r\nvenv.bak/\r\n\r\n# Spyder project settings\r\n.spyderproject\r\n.spyproject\r\n\r\n# Rope project settings\r\n.ropeproject\r\n\r\n# mkdocs documentation\r\n/site\r\n\r\n# mypy\r\n.mypy_cache/\r\n.dmypy.json\r\ndmypy.json\r\n\r\n# Pyre type checker\r\n.pyre/\r\n\r\n# Project-specific\r\ndebug_scripts/\r\n*_debug.py\r\n*test_debug.py\r\nrun_specific_tests.py\r\ntest_particle_filter.py\r\ncomprehensive_test.py\r\ntorch_cache/\r\n.ci_env\r\n*.tmp\r\n*.bak\r\n\r\n# Chatbot assistant tools\r\n# _chatbot/\r\n# !_chatbot/README.md\r\n\r\n# VS Code\r\n.vscode/\r\n*.code-workspace\r\n\r\n# PyCharm\r\n.idea/\r\n\r\n# MacOS\r\n.DS_Store\r\n\r\n# Windows\r\nThumbs.db\r\nehthumbs.db\r\nDesktop.ini\r\n",
      "content_preview": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n...",
      "directory": "tests",
      "size_bytes": 2303,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": "tests/.gitignore",
      "extension": ".gitignore",
      "filename": ".gitignore"
    },
    {
      "size_kb": 1.37,
      "content": "# =============================================================================\r\n# Multi-stage Docker build for adaptive-bayesian-driver\r\n# Optimized for development and demonstration purposes\r\n# =============================================================================\r\n\r\n# Build stage - install dependencies and build tools\r\nFROM python:3.11-slim as builder\r\n\r\nWORKDIR /usr/src/app\r\n\r\n# Install system dependencies for PyTorch compilation\r\nRUN apt-get update && apt-get install -y \\\r\n    build-essential \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n# Copy requirements and install Python dependencies\r\nCOPY requirements.txt .\r\nRUN pip install --no-cache-dir --user -r requirements.txt\r\n\r\n# Production stage - minimal runtime environment\r\nFROM python:3.11-slim\r\n\r\nWORKDIR /usr/src/app\r\n\r\n# Create non-root user for security\r\nRUN groupadd --system app && useradd --system --group app\r\n\r\n# Copy installed packages from builder stage\r\nCOPY --from=builder /root/.local /home/app/.local\r\n\r\n# Copy application code\r\nCOPY ./adaptive_bayesian_driver ./adaptive_bayesian_driver\r\nCOPY ./config ./config\r\nCOPY ./demo.py .\r\n\r\n# Set ownership and switch to non-root user\r\nRUN chown -R app:app /usr/src/app\r\nUSER app\r\n\r\n# Add local Python packages to PATH\r\nENV PATH=/home/app/.local/bin:$PATH\r\n\r\n# Expose port for potential API serving\r\nEXPOSE 8000\r\n\r\n# Default command runs the demo\r\nCMD [\"python\", \"demo.py\"]\r\n",
      "content_preview": "# =============================================================================\r\n# Multi-stage Docker build for adaptive-bayesian-driver\r\n# Optimized for development and demonstration purposes\r\n# ====...",
      "directory": "tests",
      "size_bytes": 1399,
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "path": "tests/Dockerfile",
      "extension": "",
      "filename": "Dockerfile"
    },
    {
      "size_kb": 6.39,
      "content": "\"\"\"\r\nTest suite for environment components.\r\n\"\"\"\r\n\r\nimport unittest\r\nimport numpy as np\r\nimport torch\r\nfrom adaptive_bayesian_driver.environment import (\r\n    VolatilityController, VolatilityRegime,\r\n    SceneRenderer, TaskHMM, TaskState\r\n)\r\nfrom adaptive_bayesian_driver.config import load_config\r\n\r\nclass TestVolatilityController(unittest.TestCase):\r\n    \"\"\"Test VolatilityController functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'volatility_parameters': {\r\n                'hazard_rate': 0.01,\r\n                'tau_drift': 30,\r\n                'bias_extremes': [0.2, 0.8]\r\n            }\r\n        }\r\n        self.controller = VolatilityController(self.config)\r\n\r\n    def test_initialization(self):\r\n        \"\"\"Test proper initialization.\"\"\"\r\n        self.assertEqual(self.controller.current_bias, 0.5)\r\n        self.assertEqual(self.controller.target_bias, 0.5)\r\n        self.assertEqual(self.controller.volatility_regime, VolatilityRegime.STABLE)\r\n\r\n    def test_update(self):\r\n        \"\"\"Test update functionality.\"\"\"\r\n        result = self.controller.update(0)\r\n\r\n        self.assertIn('current_bias', result)\r\n        self.assertIn('target_bias', result)\r\n        self.assertIn('volatility_regime', result)\r\n        self.assertIn('context_switched', result)\r\n\r\n    def test_bias_bounds(self):\r\n        \"\"\"Test bias stays within bounds.\"\"\"\r\n        for _ in range(100):\r\n            self.controller.update(0)\r\n            self.assertGreaterEqual(self.controller.current_bias, 0.0)\r\n            self.assertLessEqual(self.controller.current_bias, 1.0)\r\n\r\n    def test_reset(self):\r\n        \"\"\"Test reset functionality.\"\"\"\r\n        # Change state\r\n        for _ in range(10):\r\n            self.controller.update(0)\r\n\r\n        # Reset\r\n        self.controller.reset()\r\n\r\n        self.assertEqual(self.controller.current_bias, 0.5)\r\n        self.assertEqual(len(self.controller.bias_history), 0)\r\n\r\nclass TestSceneRenderer(unittest.TestCase):\r\n    \"\"\"Test SceneRenderer functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'visual_parameters': {\r\n                'scene_size': [64, 64],\r\n                'safe_background_color': [50, 50, 50],\r\n                'danger_background_color': [255, 215, 0]\r\n            }\r\n        }\r\n        self.renderer = SceneRenderer(self.config)\r\n\r\n    def test_initialization(self):\r\n        \"\"\"Test proper initialization.\"\"\"\r\n        self.assertEqual(self.renderer.scene_size, [64, 64])\r\n        self.assertEqual(self.renderer.safe_color, [50, 50, 50])\r\n\r\n    def test_render_scene(self):\r\n        \"\"\"Test scene rendering.\"\"\"\r\n        params = {\r\n            'has_danger': True,\r\n            'orientation_angle': 45.0,\r\n            'difficulty': 0.2,\r\n            'correct_direction': 'right'\r\n        }\r\n\r\n        scene, metadata = self.renderer.render_scene(params)\r\n\r\n        # Check output format\r\n        self.assertIsInstance(scene, torch.Tensor)\r\n        self.assertEqual(scene.shape, (1, 64, 64, 3))  # RGB channels\r\n        self.assertIsInstance(metadata, dict)\r\n\r\n        # Check metadata\r\n        self.assertEqual(metadata['has_danger'], True)\r\n        self.assertEqual(metadata['correct_direction'], 'right')\r\n\r\n    def test_scene_sequence(self):\r\n        \"\"\"Test scene sequence generation.\"\"\"\r\n        scenes, metadata_list = self.renderer.create_scene_sequence(\r\n            n_scenes=10, bias=0.7, danger_probability=0.8\r\n        )\r\n\r\n        self.assertEqual(scenes.shape[0], 10)  # Batch size\r\n        self.assertEqual(len(metadata_list), 10)\r\n\r\n    def test_difficulty_application(self):\r\n        \"\"\"Test difficulty effects.\"\"\"\r\n        params_easy = {\r\n            'has_danger': False,\r\n            'orientation_angle': 0.0,\r\n            'difficulty': 0.0,\r\n            'correct_direction': 'right'\r\n        }\r\n\r\n        params_hard = {\r\n            'has_danger': False,\r\n            'orientation_angle': 0.0,\r\n            'difficulty': 0.8,\r\n            'correct_direction': 'right'\r\n        }\r\n\r\n        scene_easy, _ = self.renderer.render_scene(params_easy)\r\n        scene_hard, _ = self.renderer.render_scene(params_hard)\r\n\r\n        # Hard scene should be different due to noise/occlusion\r\n        self.assertFalse(torch.equal(scene_easy, scene_hard))\r\n\r\nclass TestTaskHMM(unittest.TestCase):\r\n    \"\"\"Test TaskHMM functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'hmm_learning_rate': 0.01,\r\n            'hmm_adaptation': True\r\n        }\r\n        self.hmm = TaskHMM(self.config)\r\n\r\n    def test_initialization(self):\r\n        \"\"\"Test proper initialization.\"\"\"\r\n        self.assertEqual(self.hmm.n_states, len(TaskState))\r\n        self.assertEqual(self.hmm.current_state, 0)\r\n        self.assertEqual(self.hmm.transition_matrix.shape, (self.hmm.n_states, self.hmm.n_states))\r\n\r\n    def test_state_update(self):\r\n        \"\"\"Test state update.\"\"\"\r\n        observation = {\r\n            'has_danger': True,\r\n            'orientation_angle': 90.0,\r\n            'correct_direction': 'left'\r\n        }\r\n\r\n        result = self.hmm.update_state(observation)\r\n\r\n        self.assertIn('current_state', result)\r\n        self.assertIn('state_name', result)\r\n        self.assertIn('state_probabilities', result)\r\n\r\n    def test_observation_generation(self):\r\n        \"\"\"Test observation generation.\"\"\"\r\n        observation = self.hmm.generate_observation()\r\n\r\n        self.assertIn('has_danger', observation)\r\n        self.assertIn('orientation_angle', observation)\r\n        self.assertIn('correct_direction', observation)\r\n\r\n    def test_likelihood_computation(self):\r\n        \"\"\"Test likelihood computation.\"\"\"\r\n        observations = [\r\n            {'has_danger': True, 'orientation_angle': 90.0, 'correct_direction': 'left'},\r\n            {'has_danger': False, 'orientation_angle': 0.0, 'correct_direction': 'right'}\r\n        ]\r\n\r\n        likelihood = self.hmm.compute_likelihood(observations)\r\n        self.assertIsInstance(likelihood, float)\r\n\r\n    def test_reset(self):\r\n        \"\"\"Test reset functionality.\"\"\"\r\n        # Process some observations\r\n        for i in range(5):\r\n            obs = self.hmm.generate_observation()\r\n            self.hmm.update_state(obs)\r\n\r\n        # Reset\r\n        self.hmm.reset()\r\n\r\n        self.assertEqual(self.hmm.current_state, 0)\r\n        self.assertEqual(len(self.hmm.state_history), 0)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n",
      "content_preview": "\"\"\"\r\nTest suite for environment components.\r\n\"\"\"\r\n\r\nimport unittest\r\nimport numpy as np\r\nimport torch\r\nfrom adaptive_bayesian_driver.environment import (\r\n    VolatilityController, VolatilityRegime,\r\n...",
      "directory": "tests",
      "size_bytes": 6546,
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "path": "tests/test_environment.py",
      "extension": ".py",
      "filename": "test_environment.py"
    },
    {
      "size_kb": 8.24,
      "content": "\"\"\"\r\nTest suite for recursive Bayesian learning components.\r\n\"\"\"\r\n\r\nimport unittest\r\nimport numpy as np\r\nimport torch\r\nfrom adaptive_bayesian_driver.models import (\r\n    RecursiveBayesianLearner, InferenceMode,\r\n    SurpriseMeter, SurpriseType,\r\n    AdaptiveParticleFilter\r\n)\r\nfrom adaptive_bayesian_driver.config import load_config\r\n\r\nclass TestRecursiveBayesianLearner(unittest.TestCase):\r\n    \"\"\"Test RecursiveBayesianLearner functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'model_parameters': {\r\n                'learning_rate': 0.01,\r\n                'adaptation_rate': 0.1,\r\n                'state_dim': 4,\r\n                'obs_dim': 2,\r\n                'baseline_window': 20\r\n            },\r\n            'experiment': {'seed': 42}\r\n        }\r\n\r\n    def test_gaussian_initialization(self):\r\n        \"\"\"Test Gaussian inference mode initialization.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        self.assertEqual(learner.inference_mode, InferenceMode.GAUSSIAN)\r\n        self.assertEqual(learner.state_mean.shape, (4,))\r\n        self.assertEqual(learner.state_cov.shape, (4, 4))\r\n\r\n    def test_particle_initialization(self):\r\n        \"\"\"Test particle filter initialization.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.PARTICLE)\r\n\r\n        self.assertEqual(learner.inference_mode, InferenceMode.PARTICLE)\r\n        self.assertEqual(learner.particles.shape, (100, 4))  # Default n_particles\r\n        self.assertEqual(len(learner.weights), 100)\r\n\r\n    def test_gaussian_prediction(self):\r\n        \"\"\"Test Gaussian prediction step.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        prediction = learner.predict()\r\n\r\n        self.assertIn('state_mean', prediction)\r\n        self.assertIn('state_cov', prediction)\r\n        self.assertIn('obs_mean', prediction)\r\n\r\n    def test_gaussian_update(self):\r\n        \"\"\"Test Gaussian update step.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        observation = np.array([1.0, 2.0])\r\n        result = learner.update(observation)\r\n\r\n        self.assertIn('primary_surprise', result)\r\n        self.assertIn('context_beliefs', result)\r\n        self.assertIn('adaptive_learning_rate', result)\r\n\r\n    def test_particle_update(self):\r\n        \"\"\"Test particle filter update step.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.PARTICLE)\r\n\r\n        observation = np.array([1.0, 2.0])\r\n        result = learner.update(observation)\r\n\r\n        self.assertIn('primary_surprise', result)\r\n        self.assertIn('effective_sample_size', result)\r\n\r\n    def test_decision_making(self):\r\n        \"\"\"Test decision making.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        observation = np.array([1.0, 2.0])\r\n        decision = learner.make_decision(observation, ['left', 'right'])\r\n\r\n        self.assertIn('decision', decision)\r\n        self.assertIn('confidence', decision)\r\n        self.assertIn(decision['decision'], ['left', 'right'])\r\n\r\n    def test_reset(self):\r\n        \"\"\"Test reset functionality.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        # Process some data\r\n        for _ in range(5):\r\n            observation = np.random.randn(2)\r\n            learner.update(observation)\r\n\r\n        # Reset\r\n        learner.reset()\r\n\r\n        self.assertEqual(len(learner.surprise_history), 0)\r\n        self.assertEqual(learner.context_uncertainty, 0.5)\r\n\r\nclass TestSurpriseMeter(unittest.TestCase):\r\n    \"\"\"Test SurpriseMeter functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.meter = SurpriseMeter(mode=SurpriseType.CHI2)\r\n\r\n    def test_chi2_surprise(self):\r\n        \"\"\"Test chi-squared surprise computation.\"\"\"\r\n        innovation = np.array([1.0, 2.0])\r\n        covariance = np.eye(2) * 0.5\r\n\r\n        surprise = self.meter.compute_surprise(innov=innovation, S=covariance)\r\n\r\n        self.assertIsInstance(surprise, float)\r\n        self.assertGreaterEqual(surprise, 0.0)\r\n\r\n    def test_reconstruction_surprise(self):\r\n        \"\"\"Test reconstruction error surprise.\"\"\"\r\n        meter = SurpriseMeter(mode=SurpriseType.RECONSTRUCTION)\r\n\r\n        x = torch.randn(1, 10)\r\n        x_recon = x + torch.randn(1, 10) * 0.1\r\n\r\n        surprise = meter.compute_surprise(x=x, x_recon=x_recon)\r\n\r\n        self.assertIsInstance(surprise, float)\r\n        self.assertGreaterEqual(surprise, 0.0)\r\n\r\n    def test_baseline_update(self):\r\n        \"\"\"Test baseline statistics update.\"\"\"\r\n        meter = SurpriseMeter(mode=SurpriseType.CHI2, baseline_window=5)\r\n\r\n        # Add some errors\r\n        for i in range(10):\r\n            meter.update_baseline(float(i))\r\n\r\n        # Check baseline stats updated\r\n        self.assertGreater(meter.baseline_stats['mean'], 0)\r\n        self.assertEqual(len(meter.recent_errors), 5)  # Window size\r\n\r\n    def test_normalized_surprise(self):\r\n        \"\"\"Test normalized surprise computation.\"\"\"\r\n        meter = SurpriseMeter(mode=SurpriseType.CHI2)\r\n\r\n        # Add baseline data\r\n        for _ in range(20):\r\n            innovation = np.random.randn(2)\r\n            covariance = np.eye(2)\r\n            meter.compute_normalized_surprise(innov=innovation, S=covariance)\r\n\r\n        # Compute normalized surprise\r\n        innovation = np.array([5.0, 5.0])  # Large innovation\r\n        covariance = np.eye(2)\r\n\r\n        surprise = meter.compute_normalized_surprise(innov=innovation, S=covariance)\r\n\r\n        self.assertGreaterEqual(surprise, 0.0)\r\n\r\nclass TestAdaptiveParticleFilter(unittest.TestCase):\r\n    \"\"\"Test AdaptiveParticleFilter functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'n_particles': 50,\r\n            'state_dim': 3,\r\n            'observation_dim': 2,\r\n            'process_noise_std': 0.1,\r\n            'observation_noise_std': 0.1,\r\n            'adaptive_noise': True,\r\n            'adaptive_resampling': True\r\n        }\r\n        self.pf = AdaptiveParticleFilter(self.config)\r\n\r\n    def test_initialization(self):\r\n        \"\"\"Test proper initialization.\"\"\"\r\n        self.assertEqual(self.pf.particles.shape, (50, 3))\r\n        self.assertEqual(len(self.pf.weights), 50)\r\n        self.assertAlmostEqual(np.sum(self.pf.weights), 1.0, places=6)\r\n\r\n    def test_prediction(self):\r\n        \"\"\"Test prediction step.\"\"\"\r\n        result = self.pf.predict()\r\n\r\n        self.assertIn('mean', result)\r\n        self.assertIn('covariance', result)\r\n        self.assertEqual(result['mean'].shape, (3,))\r\n\r\n    def test_update(self):\r\n        \"\"\"Test update step.\"\"\"\r\n        observation = np.array([1.0, 2.0])\r\n\r\n        result = self.pf.update(observation)\r\n\r\n        self.assertIn('mean', result)\r\n        self.assertIn('effective_sample_size', result)\r\n        self.assertIn('resampled', result)\r\n\r\n    def test_resampling(self):\r\n        \"\"\"Test resampling functionality.\"\"\"\r\n        # Set uneven weights to trigger resampling\r\n        self.pf.weights[0] = 0.9\r\n        self.pf.weights[1:] = 0.1 / (len(self.pf.weights) - 1)\r\n\r\n        observation = np.array([1.0, 2.0])\r\n        result = self.pf.update(observation)\r\n\r\n        # Should have resampled due to low effective sample size\r\n        # (This might not always trigger, but test structure is correct)\r\n        self.assertIn('resampled', result)\r\n\r\n    def test_state_estimate(self):\r\n        \"\"\"Test state estimation.\"\"\"\r\n        estimate = self.pf.get_state_estimate()\r\n\r\n        self.assertEqual(estimate.shape, (3,))\r\n        self.assertIsInstance(estimate, np.ndarray)\r\n\r\n    def test_uncertainty(self):\r\n        \"\"\"Test uncertainty computation.\"\"\"\r\n        uncertainty = self.pf.get_uncertainty()\r\n\r\n        self.assertIsInstance(uncertainty, float)\r\n        self.assertGreaterEqual(uncertainty, 0.0)\r\n\r\n    def test_reset(self):\r\n        \"\"\"Test reset functionality.\"\"\"\r\n        # Run some updates\r\n        for _ in range(5):\r\n            observation = np.random.randn(2)\r\n            self.pf.update(observation)\r\n\r\n        # Reset\r\n        self.pf.reset()\r\n\r\n        self.assertEqual(len(self.pf.state_history), 0)\r\n        self.assertAlmostEqual(np.sum(self.pf.weights), 1.0, places=6)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n",
      "content_preview": "\"\"\"\r\nTest suite for recursive Bayesian learning components.\r\n\"\"\"\r\n\r\nimport unittest\r\nimport numpy as np\r\nimport torch\r\nfrom adaptive_bayesian_driver.models import (\r\n    RecursiveBayesianLearner, Infe...",
      "directory": "tests",
      "size_bytes": 8440,
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "path": "tests/test_recursive_bayes.py",
      "extension": ".py",
      "filename": "test_recursive_bayes.py"
    }
  ],
  "metadata": {
    "skipped_files": 25,
    "powershell_version": "7.5.2",
    "export_date": "2025-08-08 02:38:04",
    "script_version": "1.1",
    "total_files": 74,
    "repository_path": "C:\\Users\\azriy\\PortDenv\\Projects\\SurpriseLearner",
    "repository_name": "SurpriseLearner",
    "exported_files": 49
  }
}