{
  "files": [
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": ".dockerignore",
      "filename": ".dockerignore",
      "content": "# Version control\r\n.git/\r\n.gitignore\r\n\r\n# Python artifacts\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.so\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\n*.egg-info/\r\n.installed.cfg\r\n*.egg\r\n\r\n# Development environments\r\n.env\r\n.venv/\r\nenv/\r\nvenv/\r\nENV/\r\n\r\n# Testing and coverage\r\n.pytest_cache/\r\n.coverage\r\nhtmlcov/\r\n.tox/\r\n\r\n# Documentation\r\ndocs/_build/\r\nnotebooks/\r\n\r\n# IDE files\r\n.vscode/\r\n.idea/\r\n*.swp\r\n*.swo\r\n*~\r\n\r\n# OS files\r\n.DS_Store\r\nThumbs.db\r\n\r\n# Project-specific\r\n_chatbot/\r\n_reports/\r\n*.log\r\n",
      "size_bytes": 566,
      "size_kb": 0.55,
      "content_preview": "# Version control\r\n.git/\r\n.gitignore\r\n\r\n# Python artifacts\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.so\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nv...",
      "directory": "",
      "path": ".dockerignore"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": ".gitignore",
      "filename": ".gitignore",
      "content": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\npip-wheel-metadata/\r\nshare/python-wheels/\r\n.installed.cfg\r\n*.egg-info/\r\n*.egg\r\n\r\n# PyInstaller\r\n#  Usually these files are written by a python script from a template\r\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\r\n*.manifest\r\n*.spec\r\n\r\n# Installer logs\r\npip-log.txt\r\npip-delete-this-directory.txt\r\n\r\n# Unit test / coverage reports\r\nhtmlcov/\r\n.tox/\r\n.nox/\r\n.coverage\r\n.coverage.*\r\n.cache\r\nnosetests.xml\r\ncoverage.xml\r\n*.cover\r\n*.py,cover\r\n.hypothesis/\r\n.pytest_cache/\r\n\r\n# Translations\r\n*.mo\r\n*.pot\r\n\r\n# Django stuff:\r\n*.log\r\nlocal_settings.py\r\ndb.sqlite3\r\ndb.sqlite3-journal\r\n\r\n# Flask stuff:\r\ninstance/\r\n.webassets-cache\r\n\r\n# Scrapy stuff:\r\n.scrapy\r\n\r\n# Sphinx documentation\r\ndocs/_build/\r\n\r\n# PyBuilder\r\ntarget/\r\n\r\n# Jupyter Notebook\r\n.ipynb_checkpoints\r\n\r\n# IPython\r\nprofile_default/\r\nipython_config.py\r\n\r\n# pyenv\r\n.python-version\r\n\r\n# pipenv\r\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\r\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\r\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\r\n#   install all needed dependencies.\r\n#Pipfile.lock\r\n\r\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\r\n__pypackages__/\r\n\r\n# Celery stuff\r\ncelerybeat-schedule\r\ncelerybeat.pid\r\n\r\n# SageMath parsed files\r\n*.sage.py\r\n\r\n# Environments\r\n# .env\r\n.venv\r\nenv/\r\nvenv/\r\nENV/\r\nenv.bak/\r\nvenv.bak/\r\n\r\n# Spyder project settings\r\n.spyderproject\r\n.spyproject\r\n\r\n# Rope project settings\r\n.ropeproject\r\n\r\n# mkdocs documentation\r\n/site\r\n\r\n# mypy\r\n.mypy_cache/\r\n.dmypy.json\r\ndmypy.json\r\n\r\n# Pyre type checker\r\n.pyre/\r\n\r\n# Project-specific\r\ndebug_scripts/\r\n*_debug.py\r\n*test_debug.py\r\nrun_specific_tests.py\r\ntest_particle_filter.py\r\ncomprehensive_test.py\r\ntorch_cache/\r\n.ci_env\r\n*.tmp\r\n*.bak\r\n\r\n# Chatbot assistant tools\r\n# _chatbot/\r\n# !_chatbot/README.md\r\n\r\n# VS Code\r\n.vscode/\r\n*.code-workspace\r\n\r\n# PyCharm\r\n.idea/\r\n\r\n# MacOS\r\n.DS_Store\r\n\r\n# Windows\r\nThumbs.db\r\nehthumbs.db\r\nDesktop.ini\r\n",
      "size_bytes": 2303,
      "size_kb": 2.25,
      "content_preview": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n...",
      "directory": "",
      "path": ".gitignore"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": ".py",
      "filename": "__init__.py",
      "content": "\"\"\"\nAdaptive Bayesian Driver - Production ML Package\n\"\"\"\n\n__version__ = \"0.2.0\"\n__author__ = \"Azriel Ghadooshahy\"\n\n# Core system imports with robust error handling\ntry:\n    import torch\n    import numpy as np\n\n    CUDA_AVAILABLE = torch.cuda.is_available()\n    DEVICE = torch.device(\"cuda\" if CUDA_AVAILABLE else \"cpu\")\n\n    if CUDA_AVAILABLE:\n        GPU_COUNT = torch.cuda.device_count()\n        GPU_NAME = torch.cuda.get_device_name(0)\n    else:\n        GPU_COUNT = 0\n        GPU_NAME = \"No GPU Available\"\n\nexcept ImportError as e:\n    print(f\"⚠️  Critical dependency missing: {e}\")\n    CUDA_AVAILABLE = False\n    DEVICE = \"cpu\"\n    GPU_COUNT = 0\n    GPU_NAME = \"Dependencies Missing\"\n\nfrom .config import load_config\nfrom .utils.device import get_device, setup_cuda_environment\n\n__all__ = [\n    \"__version__\",\n    \"CUDA_AVAILABLE\",\n    \"DEVICE\",\n    \"GPU_COUNT\",\n    \"GPU_NAME\",\n    \"load_config\",\n    \"get_device\",\n    \"setup_cuda_environment\"\n]\n",
      "size_bytes": 955,
      "size_kb": 0.93,
      "content_preview": "\"\"\"\nAdaptive Bayesian Driver - Production ML Package\n\"\"\"\n\n__version__ = \"0.2.0\"\n__author__ = \"Azriel Ghadooshahy\"\n\n# Core system imports with robust error handling\ntry:\n    import torch\n    import num...",
      "directory": "adaptive_bayesian_driver",
      "path": "adaptive_bayesian_driver/__init__.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": ".py",
      "filename": "__init__.py",
      "content": "\"\"\"Applications module for Adaptive Bayesian Driver.\"\"\"\n\n__all__ = []\n",
      "size_bytes": 70,
      "size_kb": 0.07,
      "content_preview": "\"\"\"Applications module for Adaptive Bayesian Driver.\"\"\"\n\n__all__ = []\n",
      "directory": "adaptive_bayesian_driver\\applications",
      "path": "adaptive_bayesian_driver/applications/__init__.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": ".py",
      "filename": "config.py",
      "content": "\"\"\"Production-grade configuration management.\"\"\"\n\nimport logging\nimport sys\nfrom pathlib import Path\nfrom typing import Any\n\nimport torch\nimport yaml  # type: ignore\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConfigurationError(Exception):\n    \"\"\"Configuration-related errors.\"\"\"\n\n    pass\n\n\ndef load_config(config_path: str = \"config/experiment.yaml\") -> dict[str, Any]:\n    \"\"\"\n    Load and validate configuration from YAML file.\n\n    Args:\n        config_path: Path to YAML configuration file\n\n    Returns:\n        Validated configuration dictionary\n\n    Raises:\n        ConfigurationError: If configuration is invalid or missing\n    \"\"\"\n    config_file = Path(config_path)\n\n    if not config_file.exists():\n        alternative_paths = [\n            \"config/default.yaml\",\n            \"adaptive_bayesian_driver/config/default.yaml\",\n            \"experiments/config.yaml\",\n        ]\n\n        for alt_path in alternative_paths:\n            if Path(alt_path).exists():\n                config_file = Path(alt_path)\n                logger.warning(f\"Using alternative config: {alt_path}\")\n                break\n        else:\n            raise ConfigurationError(\n                f\"No configuration file found. \"\n                f\"Tried: {config_path}, {alternative_paths}\"\n            )\n\n    try:\n        with open(config_file, encoding=\"utf-8\") as f:\n            config_data = yaml.safe_load(f)\n    except yaml.YAMLError as e:\n        raise ConfigurationError(f\"Invalid YAML in {config_path}: {e}\") from e\n\n    # Ensure config is a dictionary\n    if not isinstance(config_data, dict):\n        raise ConfigurationError(\n            f\"Configuration must be a dictionary, got {type(config_data)}\"\n        )\n\n    config: dict[str, Any] = config_data\n\n    # Validate and enhance configuration\n    config = _validate_and_enhance_config(config)\n    return config\n\n\ndef _validate_and_enhance_config(config: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Validate and enhance configuration with system information.\"\"\"\n\n    # Add device configuration\n    cuda_available = torch.cuda.is_available()\n    device_name = str(torch.device(\"cuda\" if cuda_available else \"cpu\"))\n    gpu_count = torch.cuda.device_count() if cuda_available else 0\n\n    config[\"device\"] = {\n        \"cuda_available\": cuda_available,\n        \"device_name\": device_name,\n        \"gpu_count\": gpu_count,\n    }\n\n    # Add system information\n    python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n    cuda_version = torch.version.cuda if cuda_available else None\n\n    config[\"system\"] = {\n        \"python_version\": python_version,\n        \"torch_version\": torch.__version__,\n        \"cuda_version\": cuda_version,\n    }\n\n    # Validate critical sections\n    required_sections = [\"model\", \"training\", \"environment\"]\n    for section in required_sections:\n        if section not in config:\n            logger.warning(f\"Missing configuration section: {section}\")\n            config[section] = {}\n\n    return config\n",
      "size_bytes": 2983,
      "size_kb": 2.91,
      "content_preview": "\"\"\"Production-grade configuration management.\"\"\"\n\nimport logging\nimport sys\nfrom pathlib import Path\nfrom typing import Any\n\nimport torch\nimport yaml  # type: ignore\n\nlogger = logging.getLogger(__name...",
      "directory": "adaptive_bayesian_driver",
      "path": "adaptive_bayesian_driver/config.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": ".py",
      "filename": "__init__.py",
      "content": "\"\"\"Environment package for adaptive Bayesian driver.\"\"\"\n\n# Import key environment classes here when they are ready\n\n__all__ = []\n",
      "size_bytes": 129,
      "size_kb": 0.13,
      "content_preview": "\"\"\"Environment package for adaptive Bayesian driver.\"\"\"\n\n# Import key environment classes here when they are ready\n\n__all__ = []\n",
      "directory": "adaptive_bayesian_driver\\environment",
      "path": "adaptive_bayesian_driver/environment/__init__.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "extension": ".py",
      "filename": "__init__.py",
      "content": "\"\"\"Models package for adaptive Bayesian driver.\"\"\"\r\n\r\n# Import key model classes here when they are ready\r\n# from .RecursiveBayesianLearner import RecursiveBayesianLearner\r\n\r\n__all__ = []\r\n",
      "size_bytes": 189,
      "size_kb": 0.18,
      "content_preview": "\"\"\"Models package for adaptive Bayesian driver.\"\"\"\r\n\r\n# Import key model classes here when they are ready\r\n# from .RecursiveBayesianLearner import RecursiveBayesianLearner\r\n\r\n__all__ = []\r\n",
      "directory": "adaptive_bayesian_driver\\models",
      "path": "adaptive_bayesian_driver/models/__init__.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "extension": ".py",
      "filename": "RecursiveBayesianLearner.py",
      "content": "# src/models/bayesian_learner.py\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom typing import Dict, List, Tuple, Optional\r\nfrom .generator import Simple2LayerReLU\r\nfrom .surprise_detector import SurpriseDetector\r\n\r\nclass RecursiveBayesianLearner:\r\n    \"\"\"\r\n    Complete implementation of adaptive recursive Bayesian learning framework\r\n    \r\n    Key Features:\r\n    - LC-NE inspired surprise detection with adaptive thresholds\r\n    - Ego vehicle embodied learning for rule acquisition\r\n    - Two-timescale dynamics (fast latent, slow generator updates)\r\n    - Emergent bias tracking through reconstruction error patterns\r\n    - Computer vision processing of fork intersection scenes\r\n    - Danger avoidance task with yellow/white background detection\r\n    \"\"\"\r\n    \r\n    def __init__(self, config: Dict):\r\n        self.config = config\r\n        \r\n        # Initialize generative prior (2-layer ReLU)\r\n        scene_size = config['visual_parameters']['scene_size']\r\n        self.generator = Simple2LayerReLU(\r\n            latent_dim=config['model_parameters']['latent_dim'],\r\n            output_dim=np.prod(scene_size),\r\n            hidden_dim=config['model_parameters']['generator_hidden']\r\n        )\r\n        \r\n        # Initialize surprise detection system\r\n        self.surprise_detector = SurpriseDetector(\r\n            baseline_window=config['model_parameters']['reconstruction_baseline_window']\r\n        )\r\n        \r\n        # Learning state tracking\r\n        self.reconstruction_history = []\r\n        self.context_uncertainty_history = []\r\n        self.performance_history = []\r\n        self.surprise_history = []\r\n        self.action_history = []\r\n        \r\n        # Ego vehicle state for embodied learning\r\n        self.ego_vehicle_state = {\r\n            'recent_danger_encounters': [],\r\n            'rule_confidence': 0.5,\r\n            'conservative_bias': 0.0,\r\n            'volatility_estimate': 0.5\r\n        }\r\n        \r\n        # Rule learning system\r\n        self.danger_avoidance_strength = 0.5\r\n        self.rule_violation_memory = []\r\n        \r\n        # Adaptive learning parameters\r\n        self.base_learning_rate = config['model_parameters']['learning_rate']\r\n        self.adaptation_rate = config['biological_parameters']['adaptation_rate']\r\n        \r\n    def process_scene_sequence(self, scenes: torch.Tensor, \r\n                             metadata: List[Dict]) -> Dict:\r\n        \"\"\"Process continuous sequence of fork intersection scenes\"\"\"\r\n        results = {\r\n            'surprise_signals': [],\r\n            'adaptive_thresholds': [],\r\n            'reconstruction_errors': [],\r\n            'actions': [],\r\n            'correct_actions': [],\r\n            'context_uncertainty': [],\r\n            'performance': [],\r\n            'ego_states': [],\r\n            'rule_violations': [],\r\n            'learning_rates': []\r\n        }\r\n        \r\n        for step, (scene, meta) in enumerate(zip(scenes, metadata)):\r\n            # Process single scene through complete framework\r\n            step_results = self._process_single_scene(scene, meta, step)\r\n            \r\n            # Store all results for analysis\r\n            for key, value in step_results.items():\r\n                results[key].append(value)\r\n                \r\n        return results\r\n    \r\n    def _process_single_scene(self, scene: torch.Tensor, metadata: Dict, \r\n                            step: int) -> Dict:\r\n        \"\"\"\r\n        Process single scene through recursive Bayesian framework with ego vehicle perspective\r\n        \"\"\"\r\n        \r\n        # 1. Establish ego vehicle context (enables rule understanding)\r\n        ego_state = self._update_ego_vehicle_state(scene, metadata, step)\r\n        \r\n        # 2. Compute reconstruction error via generative prior\r\n        reconstruction_error = self.generator.compute_reconstruction_error(scene)\r\n        self.reconstruction_history.append(reconstruction_error)\r\n        \r\n        # 3. Compute emergent context uncertainty\r\n        context_uncertainty = self._compute_context_uncertainty()\r\n        prediction_confidence = self._compute_prediction_confidence()\r\n        self.context_uncertainty_history.append(context_uncertainty)\r\n        \r\n        # 4. Adaptive surprise threshold (continuous function of uncertainty)\r\n        adaptive_threshold = self.surprise_detector.adaptive_surprise_threshold(\r\n            context_uncertainty, prediction_confidence, ego_state['volatility_regime']\r\n        )\r\n        \r\n        # 5. Compute surprise signal with adaptive threshold\r\n        surprise = self.surprise_detector.compute_surprise(\r\n            reconstruction_error, \r\n            self.reconstruction_history,\r\n            context_uncertainty=context_uncertainty,\r\n            prediction_confidence=prediction_confidence\r\n        )\r\n        self.surprise_history.append(surprise)\r\n        \r\n        # 6. Make ego vehicle action decision with rule learning\r\n        action = self._make_ego_vehicle_decision(scene, metadata, surprise, ego_state)\r\n        self.action_history.append(action)\r\n        \r\n        # 7. Get feedback and experience consequences\r\n        feedback = self._get_feedback(action, metadata['correct_action'])\r\n        self.performance_history.append(feedback)\r\n        \r\n        # 8. Detect rule violations for learning\r\n        rule_violation = self._detect_rule_violation(action, metadata, ego_state)\r\n        \r\n        # 9. Update beliefs based on embodied experience\r\n        learning_rate = self._update_beliefs_with_ego_experience(\r\n            surprise, feedback, reconstruction_error, ego_state, adaptive_threshold, rule_violation\r\n        )\r\n        \r\n        return {\r\n            'surprise_signal': surprise,\r\n            'adaptive_threshold': adaptive_threshold,\r\n            'reconstruction_error': reconstruction_error,\r\n            'action': action,\r\n            'correct_action': metadata['correct_action'],\r\n            'context_uncertainty': context_uncertainty,\r\n            'performance': feedback,\r\n            'ego_state': ego_state.copy(),\r\n            'rule_violation': rule_violation,\r\n            'learning_rate': learning_rate\r\n        }\r\n    \r\n    def _update_ego_vehicle_state(self, scene: torch.Tensor, metadata: Dict, step: int) -> Dict:\r\n        \"\"\"Update ego vehicle state for embodied rule learning\"\"\"\r\n        \r\n        # Detect current scene characteristics\r\n        danger_detected = self._detect_danger_background(scene)\r\n        scene_difficulty = self._assess_scene_difficulty(scene)\r\n        \r\n        # Update danger encounter history\r\n        if danger_detected:\r\n            self.ego_vehicle_state['recent_danger_encounters'].append(step)\r\n            # Keep only recent encounters (last 100 steps)\r\n            self.ego_vehicle_state['recent_danger_encounters'] = \\\r\n                self.ego_vehicle_state['recent_danger_encounters'][-100:]\r\n        \r\n        # Estimate current volatility regime\r\n        volatility_regime = self._estimate_volatility_regime()\r\n        \r\n        # Update rule confidence based on recent performance\r\n        self._update_rule_confidence()\r\n        \r\n        # Update conservative bias based on recent rule violations\r\n        self._update_conservative_bias()\r\n        \r\n        return {\r\n            'position': 'intersection_approach',\r\n            'speed': 'moderate',\r\n            'visibility': scene_difficulty,\r\n            'volatility_regime': volatility_regime,\r\n            'danger_frequency': len(self.ego_vehicle_state['recent_danger_encounters']) / 100,\r\n            'rule_confidence': self.ego_vehicle_state['rule_confidence'],\r\n            'conservative_bias': self.ego_vehicle_state['conservative_bias'],\r\n            'recent_performance': np.mean(self.performance_history[-10:]) if len(self.performance_history) >= 10 else 0.5\r\n        }\r\n    \r\n    def _compute_context_uncertainty(self) -> float:\r\n        \"\"\"Compute emergent context uncertainty from reconstruction error patterns\"\"\"\r\n        if len(self.reconstruction_history) < 10:\r\n            return 0.5  # Initial uncertainty\r\n        \r\n        # Use variance in recent reconstruction errors as uncertainty proxy\r\n        recent_errors = self.reconstruction_history[-20:]\r\n        recent_surprises = self.surprise_history[-20:] if len(self.surprise_history) >= 20 else [0.0] * 20\r\n        \r\n        # Combine reconstruction error variance with surprise volatility\r\n        error_uncertainty = np.var(recent_errors) / (np.mean(recent_errors) + 1e-6)\r\n        surprise_uncertainty = np.var(recent_surprises) if len(recent_surprises) > 1 else 0.0\r\n        \r\n        # Weight combination\r\n        uncertainty = 0.7 * error_uncertainty + 0.3 * surprise_uncertainty\r\n        \r\n        return np.clip(uncertainty, 0.0, 1.0)\r\n    \r\n    def _compute_prediction_confidence(self) -> float:\r\n        \"\"\"Compute prediction confidence from recent performance\"\"\"\r\n        if len(self.performance_history) < 5:\r\n            return 0.5\r\n        \r\n        recent_performance = self.performance_history[-10:]\r\n        confidence = np.mean(recent_performance)\r\n        \r\n        # Adjust for consistency\r\n        consistency = 1.0 - np.var(recent_performance)\r\n        confidence = 0.8 * confidence + 0.2 * consistency\r\n        \r\n        return np.clip(confidence, 0.0, 1.0)\r\n    \r\n    def _estimate_volatility_regime(self) -> str:\r\n        \"\"\"Estimate current environmental volatility regime\"\"\"\r\n        if len(self.surprise_history) < 20:\r\n            return 'moderate'\r\n        \r\n        recent_surprise_rate = np.mean([abs(s) > 0.5 for s in self.surprise_history[-20:]])\r\n        \r\n        if recent_surprise_rate > 0.4:\r\n            return 'volatile'\r\n        elif recent_surprise_rate < 0.1:\r\n            return 'stable'\r\n        else:\r\n            return 'moderate'\r\n    \r\n    def _make_ego_vehicle_decision(self, scene: torch.Tensor, metadata: Dict, \r\n                                 surprise: float, ego_state: Dict) -> str:\r\n        \"\"\"\r\n        Make ego vehicle decision with learned rule understanding\r\n        \"\"\"\r\n        \r\n        # Detect visual features\r\n        danger_detected = self._detect_danger_background(scene)\r\n        left_cue_detected = self._detect_left_orientation(scene)\r\n        \r\n        # Apply rule hierarchy with uncertainty modulation\r\n        if danger_detected:\r\n            # Danger avoidance rule with confidence modulation\r\n            base_action = 'right' if left_cue_detected else 'left'\r\n            \r\n            # Modulate based on rule confidence and surprise\r\n            rule_strength = self.ego_vehicle_state['rule_confidence']\r\n            if surprise > ego_state.get('surprise_threshold', 0.5):\r\n                # High surprise → more conservative (stronger rule application)\r\n                rule_strength = min(1.0, rule_strength * 1.2)\r\n            \r\n            # Apply conservative bias when uncertain\r\n            if ego_state['rule_confidence'] < 0.7:\r\n                conservative_action = self._apply_conservative_strategy(base_action, ego_state)\r\n                action = conservative_action\r\n            else:\r\n                action = base_action\r\n                \r\n        else:\r\n            # Safe condition: follow bar direction with bias consideration\r\n            base_action = 'left' if left_cue_detected else 'right'\r\n            \r\n            # Apply learned biases from reconstruction error patterns\r\n            action = self._apply_emergent_bias(base_action, ego_state)\r\n        \r\n        return action\r\n    \r\n    def _detect_danger_background(self, scene: torch.Tensor) -> bool:\r\n        \"\"\"Detect danger sign from yellow background vs white/gray\"\"\"\r\n        mean_intensity = torch.mean(scene).item()\r\n        # Yellow background has higher intensity than gray/white\r\n        return mean_intensity > 0.6\r\n    \r\n    def _detect_left_orientation(self, scene: torch.Tensor) -> bool:\r\n        \"\"\"Detect left-pointing vs right-pointing oriented bars\"\"\"\r\n        h, w = scene.shape\r\n        center_region = scene[h//2-8:h//2+8, w//2-8:w//2+8]\r\n        \r\n        if center_region.numel() == 0:\r\n            return False\r\n        \r\n        # Gradient-based orientation detection\r\n        grad_x = torch.diff(center_region, dim=1)\r\n        grad_y = torch.diff(center_region, dim=0)\r\n        \r\n        if grad_x.numel() == 0 or grad_y.numel() == 0:\r\n            return False\r\n        \r\n        # Left-pointing bars have specific gradient correlation pattern\r\n        try:\r\n            # Align dimensions for correlation\r\n            min_size = min(grad_x.shape[0], grad_y.shape[0])\r\n            grad_x_flat = grad_x[:min_size, :].flatten()\r\n            grad_y_flat = grad_y[:min_size, :].flatten()\r\n            \r\n            if len(grad_x_flat) > 1 and len(grad_y_flat) > 1:\r\n                correlation = torch.corrcoef(torch.stack([grad_x_flat, grad_y_flat]))[0,1]\r\n                return correlation.item() > 0\r\n            else:\r\n                return False\r\n        except:\r\n            return False\r\n    \r\n    def _assess_scene_difficulty(self, scene: torch.Tensor) -> float:\r\n        \"\"\"Assess visual difficulty of current scene\"\"\"\r\n        # Use variance and edge density as difficulty proxy\r\n        intensity_var = torch.var(scene).item()\r\n        edges = torch.abs(torch.diff(scene, dim=0)).sum() + torch.abs(torch.diff(scene, dim=1)).sum()\r\n        edge_density = edges.item() / scene.numel()\r\n        \r\n        difficulty = 1.0 - (intensity_var + edge_density) / 2.0\r\n        return np.clip(difficulty, 0.0, 1.0)\r\n    \r\n    def _apply_conservative_strategy(self, base_action: str, ego_state: Dict) -> str:\r\n        \"\"\"Apply conservative bias when rule confidence is low\"\"\"\r\n        conservative_bias = ego_state['conservative_bias']\r\n        \r\n        # When uncertain, prefer right turns (arbitrary but consistent choice)\r\n        if conservative_bias > 0.3:\r\n            return 'right'\r\n        else:\r\n            return base_action\r\n    \r\n    def _apply_emergent_bias(self, base_action: str, ego_state: Dict) -> str:\r\n        \"\"\"Apply emergent directional bias learned from reconstruction patterns\"\"\"\r\n        \r\n        # Extract bias from recent reconstruction error patterns\r\n        if len(self.reconstruction_history) < 20:\r\n            return base_action\r\n        \r\n        # Analyze correlation between actions and reconstruction errors\r\n        recent_actions = self.action_history[-20:]\r\n        recent_errors = self.reconstruction_history[-20:]\r\n        \r\n        if len(recent_actions) < 20:\r\n            return base_action\r\n        \r\n        # Simple bias estimation\r\n        left_errors = [e for a, e in zip(recent_actions, recent_errors) if a == 'left']\r\n        right_errors = [e for a, e in zip(recent_actions, recent_errors) if a == 'right']\r\n        \r\n        if len(left_errors) > 0 and len(right_errors) > 0:\r\n            left_avg_error = np.mean(left_errors)\r\n            right_avg_error = np.mean(right_errors)\r\n            \r\n            # Prefer direction with lower average reconstruction error\r\n            if left_avg_error < right_avg_error * 0.9:  # 10% preference threshold\r\n                bias_action = 'left'\r\n            elif right_avg_error < left_avg_error * 0.9:\r\n                bias_action = 'right'\r\n            else:\r\n                bias_action = base_action\r\n        else:\r\n            bias_action = base_action\r\n        \r\n        return bias_action\r\n    \r\n    def _get_feedback(self, action: str, correct_action: str) -> int:\r\n        \"\"\"Get binary feedback signal (1 = correct, 0 = incorrect)\"\"\"\r\n        return 1 if action == correct_action else 0\r\n    \r\n    def _detect_rule_violation(self, action: str, metadata: Dict, ego_state: Dict) -> bool:\r\n        \"\"\"Detect if action violates the danger avoidance rule\"\"\"\r\n        danger_present = metadata['danger_present']\r\n        left_cue = metadata['left_cue']\r\n        \r\n        if danger_present:\r\n            # Rule: danger + left cue → turn right, danger + right cue → turn left\r\n            correct_avoidance = 'right' if left_cue else 'left'\r\n            return action != correct_avoidance\r\n        \r\n        return False\r\n    \r\n    def _update_beliefs_with_ego_experience(self, surprise: float, feedback: int, \r\n                                          reconstruction_error: float, ego_state: Dict,\r\n                                          adaptive_threshold: float, rule_violation: bool) -> float:\r\n        \"\"\"\r\n        Update beliefs based on embodied experience with adaptive learning rate\r\n        \"\"\"\r\n        \r\n        # Calculate learning signal strength\r\n        performance_error = 1 - feedback  # 1 if wrong, 0 if correct\r\n        surprise_magnitude = max(0, surprise - adaptive_threshold)\r\n        \r\n        # Combine multiple error sources\r\n        total_learning_signal = (\r\n            0.4 * performance_error +           # Task performance\r\n            0.3 * surprise_magnitude +          # Prediction surprise\r\n            0.2 * float(rule_violation) +       # Rule violation\r\n            0.1 * reconstruction_error          # Perceptual error\r\n        )\r\n        \r\n        # Adaptive learning rate based on uncertainty and ego state\r\n        base_rate = self.adaptation_rate\r\n        uncertainty_modulation = 1.0 + ego_state['context_uncertainty']\r\n        confidence_modulation = 2.0 - ego_state['rule_confidence']\r\n        \r\n        adaptive_learning_rate = base_rate * uncertainty_modulation * confidence_modulation\r\n        \r\n        # Apply learning if signal is significant\r\n        if total_learning_signal > 0.1:\r\n            # Update generator parameters (slow timescale)\r\n            self._update_generator_weights(total_learning_signal, adaptive_learning_rate)\r\n            \r\n            # Update rule knowledge (ego-specific learning)\r\n            self._update_rule_knowledge(ego_state, feedback, rule_violation, total_learning_signal)\r\n            \r\n            # Update context beliefs (emergent from patterns)\r\n            self._update_context_beliefs(surprise, feedback, reconstruction_error)\r\n        \r\n        return adaptive_learning_rate\r\n    \r\n    def _update_generator_weights(self, learning_signal: float, learning_rate: float):\r\n        \"\"\"Update generative prior weights based on surprise-driven adaptation\"\"\"\r\n        \r\n        # Implement simplified weight update (in practice, would use proper gradients)\r\n        adaptation_strength = learning_rate * learning_signal\r\n        \r\n        # Simple perturbation-based update for demonstration\r\n        with torch.no_grad():\r\n            for param in self.generator.parameters():\r\n                if param.grad is not None:\r\n                    # Apply adaptation with momentum\r\n                    param.data -= adaptation_strength * param.grad\r\n                else:\r\n                    # Small random perturbation when no gradient available\r\n                    noise = torch.randn_like(param) * adaptation_strength * 0.01\r\n                    param.data += noise\r\n    \r\n    def _update_rule_knowledge(self, ego_state: Dict, feedback: int, \r\n                             rule_violation: bool, learning_signal: float):\r\n        \"\"\"Update internal rule representations based on ego vehicle experience\"\"\"\r\n        \r\n        # Update rule confidence based on performance\r\n        if rule_violation and feedback == 0:\r\n            # Rule violation led to bad outcome → strengthen rule\r\n            self.ego_vehicle_state['rule_confidence'] = min(1.0, \r\n                self.ego_vehicle_state['rule_confidence'] + 0.1 * learning_signal)\r\n        elif not rule_violation and feedback == 1:\r\n            # Following rule led to good outcome → reinforce rule\r\n            self.ego_vehicle_state['rule_confidence'] = min(1.0,\r\n                self.ego_vehicle_state['rule_confidence'] + 0.05 * learning_signal)\r\n        elif rule_violation and feedback == 1:\r\n            # Rule violation led to good outcome → weaken rule slightly\r\n            self.ego_vehicle_state['rule_confidence'] = max(0.0,\r\n                self.ego_vehicle_state['rule_confidence'] - 0.02 * learning_signal)\r\n        \r\n        # Track rule violations for pattern learning\r\n        self.rule_violation_memory.append({\r\n            'violation': rule_violation,\r\n            'feedback': feedback,\r\n            'learning_signal': learning_signal\r\n        })\r\n        \r\n        # Keep only recent memory\r\n        self.rule_violation_memory = self.rule_violation_memory[-50:]\r\n    \r\n    def _update_rule_confidence(self):\r\n        \"\"\"Update rule confidence based on recent performance patterns\"\"\"\r\n        if len(self.performance_history) < 10:\r\n            return\r\n        \r\n        recent_performance = self.performance_history[-10:]\r\n        recent_violations = self.rule_violation_memory[-10:] if len(self.rule_violation_memory) >= 10 else []\r\n        \r\n        # Higher performance → higher rule confidence\r\n        performance_factor = np.mean(recent_performance)\r\n        \r\n        # Fewer violations → higher rule confidence\r\n        violation_factor = 1.0 - np.mean([v['violation'] for v in recent_violations]) if recent_violations else 1.0\r\n        \r\n        # Update with exponential moving average\r\n        alpha = 0.1\r\n        new_confidence = 0.6 * performance_factor + 0.4 * violation_factor\r\n        self.ego_vehicle_state['rule_confidence'] = (\r\n            (1 - alpha) * self.ego_vehicle_state['rule_confidence'] + \r\n            alpha * new_confidence\r\n        )\r\n    \r\n    def _update_conservative_bias(self):\r\n        \"\"\"Update conservative bias based on recent rule violations and uncertainty\"\"\"\r\n        if len(self.context_uncertainty_history) < 5:\r\n            return\r\n        \r\n        recent_uncertainty = np.mean(self.context_uncertainty_history[-5:])\r\n        recent_violations = len([v for v in self.rule_violation_memory[-10:] if v['violation']]) if len(self.rule_violation_memory) >= 10 else 0\r\n        \r\n        # Higher uncertainty → more conservative\r\n        # More violations → more conservative\r\n        target_bias = 0.5 * recent_uncertainty + 0.3 * (recent_violations / 10.0)\r\n        \r\n        # Smooth update\r\n        alpha = 0.05\r\n        self.ego_vehicle_state['conservative_bias'] = (\r\n            (1 - alpha) * self.ego_vehicle_state['conservative_bias'] + \r\n            alpha * target_bias\r\n        )\r\n    \r\n    def _update_context_beliefs(self, surprise: float, feedback: int, reconstruction_error: float):\r\n        \"\"\"Update context beliefs based on surprise and performance patterns\"\"\"\r\n        \r\n        # This implements emergent bias tracking through error patterns\r\n        # The bias emerges from the patterns of reconstruction errors and surprise signals\r\n        # rather than being explicitly programmed\r\n        \r\n        # Track statistical patterns in reconstruction errors\r\n        if len(self.reconstruction_history) >= 20:\r\n            # Analyze recent error patterns for context switching\r\n            recent_errors = self.reconstruction_history[-20:]\r\n            error_trend = np.diff(recent_errors)\r\n            \r\n            # Detect potential context switches from error dynamics\r\n            if np.std(error_trend) > np.mean(recent_errors) * 0.1:\r\n                # High variability suggests context instability\r\n                self.ego_vehicle_state['volatility_estimate'] = min(1.0,\r\n                    self.ego_vehicle_state['volatility_estimate'] + 0.05)\r\n            else:\r\n                # Low variability suggests stable context\r\n                self.ego_vehicle_state['volatility_estimate'] = max(0.0,\r\n                    self.ego_vehicle_state['volatility_estimate'] - 0.02)\r\n    \r\n    def get_learning_metrics(self) -> Dict:\r\n        \"\"\"Get comprehensive learning performance metrics\"\"\"\r\n        if len(self.performance_history) < 10:\r\n            return {\r\n                'performance': 0.5, \r\n                'surprise_rate': 0.5,\r\n                'context_uncertainty': 0.5,\r\n                'rule_confidence': 0.5\r\n            }\r\n        \r\n        recent_performance = np.mean(self.performance_history[-50:])\r\n        recent_surprise = np.mean([abs(s) for s in self.surprise_history[-50:]])\r\n        recent_uncertainty = np.mean(self.context_uncertainty_history[-20:])\r\n        \r\n        return {\r\n            'performance': recent_performance,\r\n            'surprise_rate': recent_surprise,\r\n            'context_uncertainty': recent_uncertainty,\r\n            'rule_confidence': self.ego_vehicle_state['rule_confidence'],\r\n            'conservative_bias': self.ego_vehicle_state['conservative_bias'],\r\n            'volatility_estimate': self.ego_vehicle_state['volatility_estimate'],\r\n            'total_rule_violations': len([v for v in self.rule_violation_memory if v['violation']]),\r\n            'adaptation_rate': np.mean([abs(s) > 0.5 for s in self.surprise_history[-20:]]) if len(self.surprise_history) >= 20 else 0.0\r\n        }\r\n    \r\n    def get_system_state(self) -> Dict:\r\n        \"\"\"Get current complete system state for analysis\"\"\"\r\n        return {\r\n            'ego_vehicle_state': self.ego_vehicle_state.copy(),\r\n            'recent_performance': self.performance_history[-10:] if len(self.performance_history) >= 10 else [],\r\n            'recent_surprises': self.surprise_history[-10:] if len(self.surprise_history) >= 10 else [],\r\n            'recent_uncertainties': self.context_uncertainty_history[-10:] if len(self.context_uncertainty_history) >= 10 else [],\r\n            'generator_state': 'pretrained',  # Could add actual generator analysis\r\n            'learning_metrics': self.get_learning_metrics()\r\n        }\r\n",
      "size_bytes": 25597,
      "size_kb": 25.0,
      "content_preview": "# src/models/bayesian_learner.py\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom typing import Dict, List, Tuple, Optional\r\nfrom .generator import Simple2LayerReLU\r\nfrom .surprise_detec...",
      "directory": "adaptive_bayesian_driver\\models",
      "path": "adaptive_bayesian_driver/models/RecursiveBayesianLearner.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "extension": ".py",
      "filename": "utils_particle.py",
      "content": "\"\"\"\r\nParticle filter utilities for adaptive Bayesian learning.\r\n\"\"\"\r\n\r\nimport numpy as np\r\nimport torch\r\nfrom typing import Dict, List, Tuple, Optional, Callable\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n# This file was restored after being corrupted\r\n# Content needs to be reimplemented based on project needs\r\n",
      "size_bytes": 329,
      "size_kb": 0.32,
      "content_preview": "\"\"\"\r\nParticle filter utilities for adaptive Bayesian learning.\r\n\"\"\"\r\n\r\nimport numpy as np\r\nimport torch\r\nfrom typing import Dict, List, Tuple, Optional, Callable\r\nimport logging\r\n\r\nlogger = logging.ge...",
      "directory": "adaptive_bayesian_driver\\models",
      "path": "adaptive_bayesian_driver/models/utils_particle.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "extension": ".py",
      "filename": "__init__.py",
      "content": "\"\"\"Utilities package for adaptive Bayesian driver.\"\"\"\r\n\r\nfrom .device import get_device\r\n\r\n__all__ = [\"get_device\"]\r\n",
      "size_bytes": 117,
      "size_kb": 0.11,
      "content_preview": "\"\"\"Utilities package for adaptive Bayesian driver.\"\"\"\r\n\r\nfrom .device import get_device\r\n\r\n__all__ = [\"get_device\"]\r\n",
      "directory": "adaptive_bayesian_driver\\utils",
      "path": "adaptive_bayesian_driver/utils/__init__.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-08-02 09:07:28",
      "extension": ".py",
      "filename": "compute_optimizations.py",
      "content": "#  compute_optimizer.py\nimport torch\nimport os\nimport psutil\nimport platform\nfrom typing import Dict, Tuple, Any\nimport warnings\n\nclass IntelComputeOptimizer:\n    \"\"\"\n    Hardware-aware compute optimization for Intel 8-core + 32GB RAM system.\n    Optimizes PyTorch performance for uncertainty-aware computer vision demo.\n    \"\"\"\n\n    def __init__(self, target_gpu_memory_fraction: float = 0.8):\n        self.system_specs = self._detect_system_specs()\n        self.target_gpu_memory_fraction = target_gpu_memory_fraction\n        self.device = self._setup_optimal_device()\n        self.config = self._generate_optimal_config()\n\n    def _detect_system_specs(self) -> Dict[str, Any]:\n        \"\"\"Detect and validate system specifications\"\"\"\n        specs = {\n            'cpu_count': os.cpu_count(),\n            'physical_cores': psutil.cpu_count(logical=False),\n            'logical_cores': psutil.cpu_count(logical=True),\n            'total_memory_gb': psutil.virtual_memory().total / (1024**3),\n            'available_memory_gb': psutil.virtual_memory().available / (1024**3),\n            'platform': platform.processor(),\n            'is_intel': 'intel' in platform.processor().lower(),\n            'supports_mkldnn': torch.backends.mkldnn.is_available(),\n        }\n        return specs\n\n    def _setup_optimal_device(self) -> torch.device:\n        \"\"\"Configure optimal PyTorch device with Intel-specific optimizations\"\"\"\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        if device.type == 'cpu':\n            self._optimize_cpu_performance()\n        else:\n            self._optimize_gpu_performance()\n\n        return device\n\n    def _optimize_cpu_performance(self):\n        \"\"\"Aggressive CPU optimizations for 8-core Intel system\"\"\"\n\n        # Intel MKL optimizations - aggressive for your hardware\n        if self.system_specs['is_intel'] and self.system_specs['supports_mkldnn']:\n            torch.backends.mkldnn.enabled = True\n            torch.backends.mkldnn.verbose = 0  # Reduce logging overhead\n\n        # Threading optimization for 8-core system\n        # Use 6 threads (reserve 2 for system + other processes)\n        optimal_threads = min(6, self.system_specs['physical_cores'])\n        torch.set_num_threads(optimal_threads)\n\n        # Single interop thread to reduce overhead on multi-core systems\n        torch.set_num_interop_threads(1)\n\n        # Intel-specific environment variables\n        os.environ['OMP_NUM_THREADS'] = str(optimal_threads)\n        os.environ['MKL_NUM_THREADS'] = str(optimal_threads)\n        os.environ['NUMEXPR_NUM_THREADS'] = str(optimal_threads)\n\n        # Intel MKL specific optimizations\n        os.environ['KMP_AFFINITY'] = 'granularity=fine,compact,1,0'\n        os.environ['KMP_BLOCKTIME'] = '1'  # Reduce thread idle time\n\n        # Memory optimization for 32GB system\n        torch.set_default_dtype(torch.float32)  # Efficient precision\n\n        print(f\"🔧 Intel CPU optimizations enabled:\")\n        print(f\"   - Threads: {optimal_threads}/{self.system_specs['logical_cores']}\")\n        print(f\"   - MKL-DNN: {torch.backends.mkldnn.enabled}\")\n        print(f\"   - Available RAM: {self.system_specs['available_memory_gb']:.1f}GB\")\n\n    def _optimize_gpu_performance(self):\n        \"\"\"GPU optimizations with CPU fallback awareness\"\"\"\n        if torch.cuda.is_available():\n            # Enable optimized attention for GPU\n            torch.backends.cuda.enable_flash_sdp(True)\n\n            # Memory management\n            torch.cuda.empty_cache()\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n\n            print(f\"🚀 GPU acceleration: {torch.cuda.get_device_name(0)}\")\n            print(f\"   - GPU Memory: {gpu_memory:.1f}GB\")\n            print(f\"   - Target usage: {self.target_gpu_memory_fraction*100}%\")\n\n    def _generate_optimal_config(self) -> Dict[str, Any]:\n        \"\"\"Generate hardware-optimized configuration\"\"\"\n\n        if self.device.type == 'cpu':\n            # Aggressive CPU config for your 32GB + 8-core system\n            return {\n                # Batch sizing - larger batches for 32GB RAM\n                'batch_size_train': 64,    # Increased from conservative 32\n                'batch_size_eval': 128,    # Large eval batches for throughput\n\n                # Uncertainty quantification\n                'mc_samples': 8,           # More samples for better uncertainty\n                'ensemble_size': 4,        # Multiple models for robust UQ\n\n                # DataLoader optimization for 8 cores\n                'num_workers': 6,          # Reserve 2 cores for training process\n                'pin_memory': True,        # Faster CPU->GPU transfer when available\n                'persistent_workers': True, # Reduce worker startup overhead\n\n                # Memory and precision\n                'precision': torch.float32,\n                'memory_format': torch.channels_last, # Intel optimization\n\n                # Training optimization\n                'gradient_accumulation_steps': 1,  # Direct updates with large RAM\n                'max_memory_usage_gb': 24,         # Conservative limit for 32GB\n            }\n        else:\n            # GPU configuration with CPU preprocessing support\n            return {\n                'batch_size_train': 256,   # Large GPU batches\n                'batch_size_eval': 512,\n                'mc_samples': 10,\n                'ensemble_size': 5,\n                'num_workers': 8,          # Full core utilization for data loading\n                'pin_memory': True,\n                'persistent_workers': True,\n                'precision': torch.float16, # GPU half precision\n                'memory_format': torch.channels_last,\n                'gradient_accumulation_steps': 1,\n                'max_memory_usage_gb': self.target_gpu_memory_fraction *\n                                     torch.cuda.get_device_properties(0).total_memory / (1024**3)\n            }\n\n    def create_optimized_dataloader(self, dataset, shuffle: bool = True, drop_last: bool = True):\n        \"\"\"Create hardware-optimized DataLoader\"\"\"\n        return torch.utils.data.DataLoader(\n            dataset,\n            batch_size=self.config['batch_size_train'] if shuffle else self.config['batch_size_eval'],\n            shuffle=shuffle,\n            num_workers=self.config['num_workers'],\n            pin_memory=self.config['pin_memory'],\n            persistent_workers=self.config['persistent_workers'],\n            drop_last=drop_last\n        )\n\n    def profile_performance(self, model, test_loader) -> Dict[str, float]:\n        \"\"\"Benchmark model performance on your hardware\"\"\"\n        import time\n\n        model.eval()\n        total_samples = 0\n        total_time = 0\n\n        with torch.no_grad():\n            start_time = time.time()\n            for batch_idx, (data, targets) in enumerate(test_loader):\n                data = data.to(self.device)\n\n                # Your uncertainty-aware prediction\n                if hasattr(model, 'predict_with_uncertainty'):\n                    predictions, uncertainty = model.predict_with_uncertainty(data)\n                else:\n                    predictions = model(data)\n\n                total_samples += data.size(0)\n\n                # Break after reasonable sample for quick profiling\n                if batch_idx > 50:  # ~6400 samples with batch_size=128\n                    break\n\n            total_time = time.time() - start_time\n\n        throughput = total_samples / total_time\n        latency = 1000 / throughput  # ms per sample\n\n        return {\n            'device': str(self.device),\n            'samples_processed': total_samples,\n            'total_time_sec': total_time,\n            'throughput_samples_per_sec': throughput,\n            'avg_latency_ms': latency,\n            'memory_usage_gb': self._get_memory_usage()\n        }\n\n    def _get_memory_usage(self) -> float:\n        \"\"\"Get current memory usage\"\"\"\n        if self.device.type == 'cuda':\n            return torch.cuda.memory_allocated() / (1024**3)\n        else:\n            return psutil.Process().memory_info().rss / (1024**3)\n\n    def print_optimization_summary(self):\n        \"\"\"Print comprehensive optimization summary\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"🎯 INTEL COMPUTE OPTIMIZATION SUMMARY\")\n        print(\"=\"*60)\n        print(f\"Hardware: {self.system_specs['physical_cores']}-core Intel, {self.system_specs['total_memory_gb']:.0f}GB RAM\")\n        print(f\"Device: {self.device}\")\n        print(f\"PyTorch threads: {torch.get_num_threads()}\")\n        print(f\"MKL-DNN enabled: {torch.backends.mkldnn.enabled}\")\n        print(\"\\nOptimal Configuration:\")\n        for key, value in self.config.items():\n            print(f\"  {key}: {value}\")\n        print(\"=\"*60)\n\n# Convenience function for immediate use\ndef setup_optimal_compute(target_gpu_memory_fraction: float = 0.8) -> IntelComputeOptimizer:\n    \"\"\"One-line setup for your Intel 8-core system\"\"\"\n    optimizer = IntelComputeOptimizer(target_gpu_memory_fraction)\n    optimizer.print_optimization_summary()\n    return optimizer\n",
      "size_bytes": 9097,
      "size_kb": 8.88,
      "content_preview": "#  compute_optimizer.py\nimport torch\nimport os\nimport psutil\nimport platform\nfrom typing import Dict, Tuple, Any\nimport warnings\n\nclass IntelComputeOptimizer:\n    \"\"\"\n    Hardware-aware compute optimi...",
      "directory": "adaptive_bayesian_driver\\utils",
      "path": "adaptive_bayesian_driver/utils/compute_optimizations.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "extension": ".py",
      "filename": "demo.py",
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nDemo script for adaptive-bayesian-driver package.\r\nTests basic functionality and package import.\r\n\"\"\"\r\n\r\nimport sys\r\nimport os\r\n\r\n# Add current directory to Python path for development\r\nsys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\r\n\r\ntry:\r\n    import adaptive_bayesian_driver\r\n    print(\"✓ Package import successful\")\r\n    print(f\"✓ Version: {adaptive_bayesian_driver.__version__}\")\r\n    print(f\"✓ Author: {adaptive_bayesian_driver.__author__}\")\r\n    print(f\"✓ CUDA Available: {adaptive_bayesian_driver.CUDA_AVAILABLE}\")\r\n    print(f\"✓ Device: {adaptive_bayesian_driver.DEVICE}\")\r\n\r\n    # Test device utility\r\n    from adaptive_bayesian_driver.utils.device import (\r\n        get_device,\r\n        get_device_info\r\n    )\r\n    device = get_device()\r\n    print(f\"✓ Device utility: {device}\")\r\n    print(f\"✓ Device info: {get_device_info()}\")\r\n\r\n    print(\"\\n🎉 Demo completed successfully!\")\r\n\r\nexcept Exception as e:\r\n    print(f\"❌ Error: {e}\")\r\n    import traceback\r\n    traceback.print_exc()\r\n    sys.exit(1)\r\n",
      "size_bytes": 1074,
      "size_kb": 1.05,
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nDemo script for adaptive-bayesian-driver package.\r\nTests basic functionality and package import.\r\n\"\"\"\r\n\r\nimport sys\r\nimport os\r\n\r\n# Add current directory to Python path fo...",
      "directory": "adaptive_bayesian_driver\\utils",
      "path": "adaptive_bayesian_driver/utils/demo.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 05:39:22",
      "extension": ".py",
      "filename": "device.py",
      "content": "\"\"\"Advanced GPU device management for CUDA-first ML development.\"\"\"\r\n\r\nimport torch\r\nimport platform\r\nimport sys\r\nfrom typing import Dict, Any, Optional\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef get_device(prefer_cuda: bool = True) -> torch.device:\r\n    \"\"\"\r\n    Get optimal compute device with fallback hierarchy.\r\n\r\n    Args:\r\n        prefer_cuda: Whether to prefer CUDA if available\r\n\r\n    Returns:\r\n        PyTorch device optimized for current hardware\r\n    \"\"\"\r\n    if prefer_cuda and torch.cuda.is_available():\r\n        device = torch.device('cuda')\r\n        logger.info(f\"Using CUDA device: {torch.cuda.get_device_name()}\")\r\n        return device\r\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\r\n        device = torch.device('mps')\r\n        logger.info(\"Using Apple Silicon MPS device\")\r\n        return device\r\n    else:\r\n        device = torch.device('cpu')\r\n        logger.info(\"Using CPU device\")\r\n        return device\r\n\r\n\r\ndef setup_cuda_environment() -> Dict[str, Any]:\r\n    \"\"\"\r\n    Comprehensive CUDA environment setup and diagnostics.\r\n\r\n    Returns:\r\n        Complete device information dictionary\r\n    \"\"\"\r\n    device_info = {\r\n        'cuda_available': torch.cuda.is_available(),\r\n        'cuda_version': torch.version.cuda,\r\n        'cudnn_version': (torch.backends.cudnn.version()\r\n                          if torch.backends.cudnn.is_available() else None),\r\n        'device_count': (torch.cuda.device_count()\r\n                         if torch.cuda.is_available() else 0),\r\n        'devices': []\r\n    }\r\n\r\n    if torch.cuda.is_available():\r\n        for i in range(torch.cuda.device_count()):\r\n            device_props = torch.cuda.get_device_properties(i)\r\n            major_minor = f\"{device_props.major}.{device_props.minor}\"\r\n            device_info['devices'].append({\r\n                'index': i,\r\n                'name': device_props.name,\r\n                'total_memory': device_props.total_memory,\r\n                'memory_allocated': torch.cuda.memory_allocated(i),\r\n                'memory_reserved': torch.cuda.memory_reserved(i),\r\n                'compute_capability': major_minor\r\n            })\r\n\r\n        # Set memory allocation strategy for better performance\r\n        torch.cuda.empty_cache()\r\n        if hasattr(torch.cuda, 'set_per_process_memory_fraction'):\r\n            torch.cuda.set_per_process_memory_fraction(0.8)\r\n\r\n    # Add system information\r\n    device_info['system'] = {\r\n        'platform': platform.platform(),\r\n        'python_version': sys.version,\r\n        'torch_version': torch.__version__,\r\n        'cpu_count': torch.get_num_threads()\r\n    }\r\n\r\n    return device_info\r\n\r\n\r\ndef optimize_cuda_performance():\r\n    \"\"\"Apply CUDA performance optimizations.\"\"\"\r\n    if not torch.cuda.is_available():\r\n        logger.warning(\"CUDA not available - skipping optimizations\")\r\n        return\r\n\r\n    # Enable optimizations\r\n    torch.backends.cudnn.benchmark = True\r\n    torch.backends.cudnn.deterministic = False\r\n\r\n    # Memory management\r\n    torch.cuda.empty_cache()\r\n\r\n    logger.info(\"CUDA performance optimizations applied\")\r\n\r\n\r\ndef get_device_info() -> str:\r\n    \"\"\"\r\n    Get information about the current device.\r\n\r\n    Returns:\r\n        Device information string\r\n    \"\"\"\r\n    device = get_device()\r\n    if device.type == 'cuda':\r\n        return f\"CUDA device: {torch.cuda.get_device_name()}\"\r\n    elif device.type == 'mps':\r\n        return \"Apple Silicon MPS device\"\r\n    else:\r\n        return \"CPU device\"\r\n",
      "size_bytes": 3527,
      "size_kb": 3.44,
      "content_preview": "\"\"\"Advanced GPU device management for CUDA-first ML development.\"\"\"\r\n\r\nimport torch\r\nimport platform\r\nimport sys\r\nfrom typing import Dict, Any, Optional\r\nimport logging\r\n\r\nlogger = logging.getLogger(_...",
      "directory": "adaptive_bayesian_driver\\utils",
      "path": "adaptive_bayesian_driver/utils/device.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-08-02 09:48:03",
      "extension": ".ps1",
      "filename": "Export-JsonRepo.ps1",
      "content": "# Repository to JSON Exporter - Environment Agnostic\r\n# Usage: .\\Export-JsonRepo.ps1 -RepositoryPath \".\" -OutputPath \"backup.json\"\r\n# If no OutputPath specified, auto-generates: repo_export_{reponame}_{timestamp}.json\r\n\r\n\r\n# Repository to JSON Exporter - Environment Agnostic\r\nparam(\r\n    [string]$RepositoryPath = \".\",\r\n    [string]$OutputPath = \"\",  # Will auto-generate if not provided\r\n    [switch]$IncludeAllFiles = $false,\r\n    [int]$MaxFileSizeKB = 10000,  # Skip files larger than\r\n    [switch]$Verbose = $true,   # Show excluded files\r\n    [switch]$Debug = $false      # Show detailed filtering info\r\n)\r\n\r\n# Define file extensions to include (when not using -IncludeAllFiles)\r\n$IncludedExtensions = @(\r\n    '.py', '.md', '.yaml', '.yml', '.ipynb', '.cfg', '.env',\r\n    '.xml', '.html', '.css', '.js', '.ts', '.sql', '.sh',\r\n    '.bat', '.ps1', '.ini', '.conf', '.log', '.csv', '.toml', '.lock',\r\n    '.gitignore', '.dockerignore', '.editorconfig', '.json'\r\n    # '.txt',\r\n)\r\n\r\n# Directories to exclude\r\n#\r\n$ExcludedDirs = @('_snapshot', 'Modules', 'site-packages', '_backup', '_dev', '_cop', '_depr', '_dev', '_gem', '_prp', 'txt', '.git', '.vscode', '__pycache__', 'node_modules', '.pytest_cache', '_mypy_cache', '.mypy_cache', 'venv', 'env', '.env', 'dist', 'build', '.ruff_cache', '.black_cache')\r\n\r\nfunction Test-IsBinaryFile {\r\n    param([string]$FilePath)\r\n\r\n    try {\r\n        $bytes = [System.IO.File]::ReadAllBytes($FilePath)\r\n        if ($bytes.Length -eq 0) { return $false }\r\n\r\n        # Check first 1024 bytes for null characters (common in binary files)\r\n        $checkLength = [Math]::Min(1024, $bytes.Length)\r\n        for ($i = 0; $i -lt $checkLength; $i++) {\r\n            if ($bytes[$i] -eq 0) { return $true }\r\n        }\r\n        return $false\r\n    }\r\n    catch {\r\n        return $true  # If we can't read it, assume it's binary\r\n    }\r\n}\r\n\r\nfunction Get-FileContent {\r\n    param([string]$FilePath)\r\n\r\n    try {\r\n        if (Test-IsBinaryFile -FilePath $FilePath) {\r\n            return \"[BINARY FILE - Content not included]\"\r\n        }\r\n\r\n        # Try UTF-8 first, then fall back to default encoding\r\n        try {\r\n            return [System.IO.File]::ReadAllText($FilePath, [System.Text.Encoding]::UTF8)\r\n        }\r\n        catch {\r\n            return [System.IO.File]::ReadAllText($FilePath)\r\n        }\r\n    }\r\n    catch {\r\n        return \"[ERROR: Could not read file - $($_.Exception.Message)]\"\r\n    }\r\n}\r\n\r\n# Resolve repository path to absolute path\r\ntry {\r\n    $resolvedRepoPath = (Resolve-Path $RepositoryPath -ErrorAction Stop).Path\r\n}\r\ncatch {\r\n    Write-Host \"Error: Cannot resolve repository path '$RepositoryPath'\" -ForegroundColor Red\r\n    exit 1\r\n}\r\n\r\n# Auto-generate output path if not provided\r\nif ([string]::IsNullOrWhiteSpace($OutputPath)) {\r\n    $repoName = Split-Path $resolvedRepoPath -Leaf\r\n    $timestamp = Get-Date -Format \"yyyyMMdd_HHmmss\"\r\n    $OutputPath = Join-Path $resolvedRepoPath \"${repoName}_${timestamp}.json\"\r\n}\r\nelse {\r\n    # If OutputPath is relative, make it relative to the repository\r\n    if (-not [System.IO.Path]::IsPathRooted($OutputPath)) {\r\n        $OutputPath = Join-Path $resolvedRepoPath $OutputPath\r\n    }\r\n}\r\n\r\n# Ensure output directory exists\r\n$outputDir = Split-Path $OutputPath -Parent\r\nif (-not (Test-Path $outputDir)) {\r\n    try {\r\n        New-Item -ItemType Directory -Path $outputDir -Force -ErrorAction Stop | Out-Null\r\n        Write-Host \"Created output directory: $outputDir\" -ForegroundColor Cyan\r\n    }\r\n    catch {\r\n        Write-Host \"Error: Cannot create output directory '$outputDir': $($_.Exception.Message)\" -ForegroundColor Red\r\n        exit 1\r\n    }\r\n}\r\n\r\nWrite-Host \"Scanning repository: $resolvedRepoPath\" -ForegroundColor Green\r\nWrite-Host \"Output file: $OutputPath\" -ForegroundColor Green\r\n\r\n# Initialize the export structure\r\n$exportData = @{\r\n    metadata = @{\r\n        export_date        = Get-Date -Format \"yyyy-MM-dd HH:mm:ss\"\r\n        repository_path    = $resolvedRepoPath\r\n        repository_name    = Split-Path $resolvedRepoPath -Leaf\r\n        total_files        = 0\r\n        exported_files     = 0\r\n        skipped_files      = 0\r\n        script_version     = \"1.1\"\r\n        powershell_version = $PSVersionTable.PSVersion.ToString()\r\n    }\r\n    files    = @()\r\n}\r\n\r\n# Get all files recursively with improved exclusion logic\r\n$allFiles = Get-ChildItem -Path $resolvedRepoPath -Recurse -File -ErrorAction SilentlyContinue | Where-Object {\r\n    # Exclude files in excluded directories - improved logic\r\n    $relativePath = $_.FullName.Replace($resolvedRepoPath, \"\").TrimStart('\\', '/').Replace('\\', '/')\r\n    $inExcludedDir = $false\r\n\r\n    foreach ($excludedDir in $ExcludedDirs) {\r\n        # Check multiple patterns to catch all variations\r\n        $patterns = @(\r\n            \"$excludedDir/*\",           # Direct child: mypy_cache/file.txt\r\n            \"*/$excludedDir/*\",         # Nested: something/mypy_cache/file.txt\r\n            \"$excludedDir\\*\",           # Windows separator\r\n            \"*\\$excludedDir\\*\",         # Windows nested\r\n            \"*/$excludedDir\",           # Directory itself\r\n            \"*\\$excludedDir\"            # Directory itself (Windows)\r\n        )\r\n\r\n        foreach ($pattern in $patterns) {\r\n            if ($relativePath -like $pattern) {\r\n                if ($Debug) { Write-Host \"EXCLUDED: $relativePath (matched pattern: $pattern for dir: $excludedDir)\" -ForegroundColor Red }\r\n                elseif ($Verbose) { Write-Host \"Excluding: $relativePath\" -ForegroundColor Yellow }\r\n                $inExcludedDir = $true\r\n                break\r\n            }\r\n        }\r\n        if ($inExcludedDir) { break }\r\n    }\r\n\r\n    return -not $inExcludedDir\r\n}\r\n\r\n$exportData.metadata.total_files = $allFiles.Count\r\nWrite-Host \"Found $($allFiles.Count) files to process\" -ForegroundColor Cyan\r\n\r\nif ($Debug) {\r\n    Write-Host \"`nExcluded directories: $($ExcludedDirs -join ', ')\" -ForegroundColor Yellow\r\n    $testFiles = Get-ChildItem -Path $resolvedRepoPath -Recurse -File -ErrorAction SilentlyContinue | Where-Object { $_.FullName -like \"*mypy_cache*\" } | Select-Object -First 3\r\n    if ($testFiles) {\r\n        Write-Host \"`nFound mypy_cache files:\" -ForegroundColor Magenta\r\n        foreach ($tf in $testFiles) {\r\n            $testRelPath = $tf.FullName.Replace($resolvedRepoPath, \"\").TrimStart('\\', '/').Replace('\\', '/')\r\n            Write-Host \"  $testRelPath\" -ForegroundColor Magenta\r\n        }\r\n    }\r\n\r\n    # Count excluded vs included files\r\n    $allFilesDebug = Get-ChildItem -Path $resolvedRepoPath -Recurse -File -ErrorAction SilentlyContinue\r\n    Write-Host \"`nTotal files before filtering: $($allFilesDebug.Count)\" -ForegroundColor Cyan\r\n    Write-Host \"Files after exclusion filtering: $($allFiles.Count)\" -ForegroundColor Cyan\r\n    Write-Host \"Files filtered out: $($allFilesDebug.Count - $allFiles.Count)\" -ForegroundColor Yellow\r\n}\r\n\r\nforeach ($file in $allFiles) {\r\n    $relativePath = $file.FullName.Replace($resolvedRepoPath, \"\").TrimStart('\\', '/').Replace('\\', '/')\r\n    $extension = $file.Extension.ToLower()\r\n    $fileSizeKB = [Math]::Round($file.Length / 1KB, 2)\r\n\r\n    # Skip large files\r\n    if ($fileSizeKB -gt $MaxFileSizeKB) {\r\n        Write-Host \"Skipping large file: $relativePath ($fileSizeKB KB)\" -ForegroundColor Yellow\r\n        $exportData.metadata.skipped_files++\r\n        continue\r\n    }\r\n\r\n    # Check if file should be included\r\n    $shouldInclude = $IncludeAllFiles -or ($extension -in $IncludedExtensions) -or ($file.Name -in @('.gitignore', '.dockerignore', '.editorconfig', 'Dockerfile', 'Makefile', 'README'))\r\n\r\n    if (-not $shouldInclude) {\r\n        $exportData.metadata.skipped_files++\r\n        continue\r\n    }\r\n\r\n    Write-Host \"Processing: $relativePath\" -ForegroundColor Gray\r\n\r\n    # Get file content\r\n    $content = Get-FileContent -FilePath $file.FullName\r\n\r\n    # Create file object\r\n    $fileObject = @{\r\n        path            = $relativePath\r\n        directory       = Split-Path $relativePath -Parent\r\n        filename        = $file.Name\r\n        extension       = $extension\r\n        size_bytes      = $file.Length\r\n        size_kb         = $fileSizeKB\r\n        last_modified   = $file.LastWriteTime.ToString(\"yyyy-MM-dd HH:mm:ss\")\r\n        content         = $content\r\n        content_preview = if ($content.Length -gt 200) { $content.Substring(0, 200) + \"...\" } else { $content }\r\n        is_binary       = Test-IsBinaryFile -FilePath $file.FullName\r\n    }\r\n\r\n    $exportData.files += $fileObject\r\n    $exportData.metadata.exported_files++\r\n}\r\n\r\n# Sort files by path for better organization\r\n$exportData.files = $exportData.files | Sort-Object path\r\n\r\nWrite-Host \"`nExport Summary:\" -ForegroundColor Green\r\nWrite-Host \"Total files found: $($exportData.metadata.total_files)\" -ForegroundColor White\r\nWrite-Host \"Files exported: $($exportData.metadata.exported_files)\" -ForegroundColor Green\r\nWrite-Host \"Files skipped: $($exportData.metadata.skipped_files)\" -ForegroundColor Yellow\r\n\r\n# Export to JSON with enhanced error handling\r\ntry {\r\n    $jsonOutput = $exportData | ConvertTo-Json -Depth 10 -Compress:$false\r\n    [System.IO.File]::WriteAllText($OutputPath, $jsonOutput, [System.Text.Encoding]::UTF8)\r\n    Write-Host \"`nRepository exported successfully to: $OutputPath\" -ForegroundColor Green\r\n\r\n    # Show file size\r\n    $outputSize = [Math]::Round((Get-Item $OutputPath).Length / 1KB, 2)\r\n    Write-Host \"Output file size: $outputSize KB\" -ForegroundColor Cyan\r\n\r\n    # Return the output path for potential pipeline use\r\n    return $OutputPath\r\n}\r\ncatch {\r\n    Write-Host \"Error exporting to JSON: $($_.Exception.Message)\" -ForegroundColor Red\r\n    Write-Host \"Attempted output path: $OutputPath\" -ForegroundColor Yellow\r\n    exit 1\r\n}\r\n",
      "size_bytes": 9751,
      "size_kb": 9.52,
      "content_preview": "# Repository to JSON Exporter - Environment Agnostic\r\n# Usage: .\\Export-JsonRepo.ps1 -RepositoryPath \".\" -OutputPath \"backup.json\"\r\n# If no OutputPath specified, auto-generates: repo_export_{reponame}...",
      "directory": "adaptive_bayesian_driver\\utils",
      "path": "adaptive_bayesian_driver/utils/Export-JsonRepo.ps1"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "extension": ".py",
      "filename": "visualization.py",
      "content": "\"\"\"\r\nVisualization utilities for adaptive Bayesian learning.\r\n\"\"\"\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport numpy as np\r\nfrom typing import Dict, List, Optional\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n# This file was restored after being corrupted\r\n# Content needs to be reimplemented based on project needs\r\n",
      "size_bytes": 352,
      "size_kb": 0.34,
      "content_preview": "\"\"\"\r\nVisualization utilities for adaptive Bayesian learning.\r\n\"\"\"\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport numpy as np\r\nfrom typing import Dict, List, Optional\r\nimport logging...",
      "directory": "adaptive_bayesian_driver\\utils",
      "path": "adaptive_bayesian_driver/utils/visualization.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "extension": ".md",
      "filename": "changelog.md",
      "content": "# Changelog\r\n\r\n## [v0.10 - 2025-07-12]\r\n\r\n    Changed: Repo to with cleanup selections from in wip project \r\n    Fixed: Various minor adjustments in ported files, wip\r\n\r\n..\r\n\r\n## [0.0.01 2025-07-01] \r\nSkeletal prototype created via Ai Promptineering e.g. \"vibe coding\"\r\n",
      "size_bytes": 270,
      "size_kb": 0.26,
      "content_preview": "# Changelog\r\n\r\n## [v0.10 - 2025-07-12]\r\n\r\n    Changed: Repo to with cleanup selections from in wip project \r\n    Fixed: Various minor adjustments in ported files, wip\r\n\r\n..\r\n\r\n## [0.0.01 2025-07-01] \r...",
      "directory": "",
      "path": "changelog.md"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "extension": ".md",
      "filename": "CONTRIBUTING.md",
      "content": "",
      "size_bytes": 0,
      "size_kb": 0.0,
      "content_preview": "",
      "directory": "",
      "path": "CONTRIBUTING.md"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": "",
      "filename": "Dockerfile",
      "content": "# =============================================================================\r\n# Multi-stage Docker build for adaptive-bayesian-driver\r\n# Optimized for development and demonstration purposes\r\n# =============================================================================\r\n\r\n# Build stage - install dependencies and build tools\r\nFROM python:3.11-slim as builder\r\n\r\nWORKDIR /usr/src/app\r\n\r\n# Install system dependencies for PyTorch compilation\r\nRUN apt-get update && apt-get install -y \\\r\n    build-essential \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n# Copy requirements and install Python dependencies\r\nCOPY requirements.txt .\r\nRUN pip install --no-cache-dir --user -r requirements.txt\r\n\r\n# Production stage - minimal runtime environment\r\nFROM python:3.11-slim\r\n\r\nWORKDIR /usr/src/app\r\n\r\n# Create non-root user for security\r\nRUN groupadd --system app && useradd --system --group app\r\n\r\n# Copy installed packages from builder stage\r\nCOPY --from=builder /root/.local /home/app/.local\r\n\r\n# Copy application code\r\nCOPY ./adaptive_bayesian_driver ./adaptive_bayesian_driver\r\nCOPY ./config ./config\r\nCOPY ./demo.py .\r\n\r\n# Set ownership and switch to non-root user\r\nRUN chown -R app:app /usr/src/app\r\nUSER app\r\n\r\n# Add local Python packages to PATH\r\nENV PATH=/home/app/.local/bin:$PATH\r\n\r\n# Expose port for potential API serving\r\nEXPOSE 8000\r\n\r\n# Default command runs the demo\r\nCMD [\"python\", \"demo.py\"]\r\n",
      "size_bytes": 1399,
      "size_kb": 1.37,
      "content_preview": "# =============================================================================\r\n# Multi-stage Docker build for adaptive-bayesian-driver\r\n# Optimized for development and demonstration purposes\r\n# ====...",
      "directory": "",
      "path": "Dockerfile"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:42:12",
      "extension": ".toml",
      "filename": "pyproject.toml",
      "content": "[build-system]\r\nrequires = [\"setuptools>=61.0\", \"wheel\"]\r\nbuild-backend = \"setuptools.build_meta\"\r\n\r\n[project]\r\nname = \"adaptive-bayesian-driver\"\r\nversion = \"0.2.0\"\r\ndescription = \"LC-NE inspired adaptive Bayesian learning for autonomous driving\"\r\nauthors = [{name = \"Azriel Ghadooshahy\", email = \"azriel.ghadooshahy@example.com\"}]\r\nreadme = \"README.md\"\r\nrequires-python = \">=3.11\"\r\nlicense = {text = \"MIT\"}\r\nkeywords = [\"machine-learning\", \"bayesian\", \"autonomous-driving\", \"neuroscience\", \"cuda\"]\r\nclassifiers = [\r\n    \"Development Status :: 4 - Beta\",\r\n    \"Intended Audience :: Science/Research\",\r\n    \"License :: OSI Approved :: MIT License\",\r\n    \"Programming Language :: Python :: 3.11\",\r\n    \"Programming Language :: Python :: 3.12\",\r\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\r\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\r\n]\r\n\r\ndependencies = [\r\n    \"torch>=2.2.0,<2.4.0\",\r\n    \"torchvision>=0.17.0,<0.19.0\",\r\n    \"numpy>=1.24.0,<1.27.0\",\r\n    \"matplotlib>=3.6.0,<3.9.0\",\r\n    \"pillow>=9.0.0,<11.0.0\",\r\n    \"pyyaml>=6.0.0,<7.0.0\",\r\n    \"tqdm>=4.66.0,<5.0.0\",\r\n]\r\n\r\n[project.optional-dependencies]\r\ndev = [\r\n    \"pytest>=8.0.0,<9.0.0\",\r\n    \"pytest-cov>=5.0.0,<6.0.0\",\r\n    \"ruff>=0.4.4,<0.5.0\",\r\n    \"mypy>=1.10.0,<2.0.0\",\r\n    \"pre-commit>=3.7.0,<4.0.0\",\r\n]\r\n\r\ncuda = [\r\n    \"cupy-cuda12x>=12.0.0,<13.0.0\",\r\n]\r\n\r\nproduction = [\r\n    \"mlflow>=2.13.0,<3.0.0\",\r\n    \"hydra-core>=1.3.0,<2.0.0\",\r\n    \"wandb>=0.17.0,<1.0.0\",\r\n    \"fastapi>=0.111.0,<1.0.0\",\r\n    \"uvicorn[standard]>=0.29.0,<1.0.0\",\r\n    \"onnx>=1.16.0,<2.0.0\",\r\n    \"onnxruntime-gpu>=1.18.0,<2.0.0\",\r\n]\r\n\r\n[project.scripts]\r\nadaptive-demo = \"adaptive_bayesian_driver.main:main\"\r\nadaptive-train = \"adaptive_bayesian_driver.applications.training:main\"\r\n\r\n[tool.ruff]\r\nline-length = 88\r\ntarget-version = \"py311\"\r\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\", \"UP\", \"B\", \"C4\", \"SIM\", \"TCH\"]\r\nignore = [\"E501\", \"N803\", \"N806\", \"B008\", \"UP007\"]\r\nexclude = [\".git\", \"__pycache__\", \"build\", \"dist\", \".eggs\", \"_chatbot\"]\r\n\r\n[tool.ruff.format]\r\nquote-style = \"double\"\r\nindent-style = \"space\"\r\nskip-magic-trailing-comma = false\r\n\r\n[tool.ruff.isort]\r\nknown-first-party = [\"adaptive_bayesian_driver\"]\r\nforce-single-line = false\r\n\r\n[tool.mypy]\r\npython_version = \"3.11\"\r\nwarn_return_any = true\r\nwarn_unused_configs = true\r\ndisallow_untyped_defs = true\r\nignore_missing_imports = true\r\nexclude = [\"_chatbot/\", \"build/\", \"dist/\", \"tests/\"]\r\n\r\n[tool.pytest.ini_options]\r\ntestpaths = [\"tests\"]\r\npython_files = [\"test_*.py\"]\r\npython_classes = [\"Test*\"]\r\npython_functions = [\"test_*\"]\r\naddopts = [\r\n    \"--strict-markers\",\r\n    \"--strict-config\",\r\n    \"--cov=adaptive_bayesian_driver\",\r\n    \"--cov-report=term-missing\",\r\n    \"--cov-report=xml\",\r\n    \"--cov-fail-under=80\"\r\n]\r\nmarkers = [\r\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\r\n    \"gpu: marks tests as requiring GPU\",\r\n    \"integration: marks tests as integration tests\",\r\n    \"unit: marks tests as unit tests\",\r\n]\r\n\r\n[tool.coverage.run]\r\nsource = [\"adaptive_bayesian_driver\"]\r\nomit = [\"*/tests/*\", \"*/_chatbot/*\", \"*/setup.py\", \"*/demo.py\"]\r\n\r\n[tool.coverage.report]\r\nexclude_lines = [\r\n    \"pragma: no cover\",\r\n    \"def __repr__\",\r\n    \"if self.debug:\",\r\n    \"if settings.DEBUG\",\r\n    \"raise AssertionError\",\r\n    \"raise NotImplementedError\",\r\n    \"if 0:\",\r\n    \"if __name__ == .__main__.:\",\r\n    \"class .*\\\\\\\\bProtocol\\\\$\\\\$:\",\r\n    \"@(abc\\\\.)?abstractmethod\",\r\n]\r\n\r\n[project.urls]\r\n\"Bug Reports\" = \"https://github.com/yourusername/adaptive-bayesian-driver/issues\"\r\n",
      "size_bytes": 3533,
      "size_kb": 3.45,
      "content_preview": "[build-system]\r\nrequires = [\"setuptools>=61.0\", \"wheel\"]\r\nbuild-backend = \"setuptools.build_meta\"\r\n\r\n[project]\r\nname = \"adaptive-bayesian-driver\"\r\nversion = \"0.2.0\"\r\ndescription = \"LC-NE inspired adap...",
      "directory": "",
      "path": "pyproject.toml"
    },
    {
      "is_binary": false,
      "last_modified": "2025-08-04 23:52:42",
      "extension": ".md",
      "filename": "README.md",
      "content": "Adaptive Bayesian Driver: Bridging Neuroscience and Computer Vision\nProject Overview\nAdaptive Bayesian Driver is a demonstration project that bridges biological uncertainty modeling principles from neuroscience to computer vision applications in autonomous driving. This work translates dual-timescale uncertainty encoding from the locus coeruleus-norepinephrine (LC-NE) system into geometric priors for neural network perception, directly connecting to Helm.ai's Deep Teaching methodology.\n\nCore Innovation: From Biological Vision to Artificial Perception\nThis project demonstrates how biological uncertainty processing can inform artificial vision systems through:\n\nDual-Timescale Uncertainty Modeling\nBased on my COSYNE 2014 research, this implementation captures uncertainty at two distinct temporal scales:\n\nWithin-trial uncertainty: Real-time confidence in individual predictions\n\nAcross-trial uncertainty: Contextual adaptation based on environmental volatility\n\nGeometric Priors in Latent Space\nMoving beyond traditional classification, we implement:\n\nUncertainty-aware latent representations that encode geometric structure\n\nBayesian neural layers for principled uncertainty quantification\n\nAdaptive learning dynamics modulated by confidence estimates\n# Adaptive Bayesian Driver: From Neuroscience to Autonomous Driving\n\n## 🎯 Current Implementation Status\n\n### ✅ Phase 1: Proof of Concept (In Progress)\n- **MNIST uncertainty classifier** with LC-NE inspired dual-timescale dynamics\n- **Intel compute optimization** for 8-core development system\n- **Geometric priors** in latent space using manifold learning\n- **Professional UQ evaluation** using UNIQUE + NIST frameworks\n\n### 🔮 Phase 2: CARLA Simulation (DESIGNED)\n- Multi-modal sensor fusion architecture\n- Real-time uncertainty propagation\n- Safety-critical decision making framework\n- Integration with Helm.ai Deep Teaching methodology\n\n### 🏭 Phase 3: Production Deployment (PLANNED)\n- Edge optimization for vehicle compute platforms\n- ISO 26262 safety compliance\n- Fleet-scale monitoring and validation\n- Continuous learning from real-world driving data\n\n## 🚀 Quick Start (Current Demo)\n\nTechnical Architecture\nMinimum Viable Demo: MNIST Uncertainty Classification\ntext\nadaptive_bayesian_driver/\n├── models/\n│   ├── uncertainty_cnn.py      # Bayesian CNN with Monte Carlo Dropout\n│   ├── geometric_prior.py      # Latent space constraints\n│   └── dual_timescale.py       # LC-NE inspired uncertainty dynamics\n├── experiments/\n│   └── mnist_demo.ipynb        # Interactive demonstration notebook\n├── utils/\n│   ├── uncertainty_metrics.py  # KL divergence, Mahalanobis distance\n│   └── visualization.py        # Uncertainty evolution plots\n└── config/\n    └── experiment_config.yaml  # Hyperparameters and settings\nKey Components\n1. Uncertainty-Aware CNN Architecture\nMonte Carlo Dropout for epistemic uncertainty estimation\n\nBayesian layers for aleatoric uncertainty quantification\n\nConfidence-based learning rate adaptation\n\n2. Biological Inspiration Integration\nLC-NE dual-timescale dynamics applied to prediction confidence\n\nContextual priors that adapt based on prediction history\n\nExploration/exploitation balance informed by uncertainty estimates\n\n3. Geometric Latent Space\nVAE-style encoding with geometric constraints\n\nManifold learning for structured uncertainty representation\n\nPhysics-informed priors for autonomous driving relevance\n\nConnection to Helm.ai's Technology Stack\nDeep Teaching Methodology Alignment\nThis project directly connects to Helm.ai's core principles:\n\nUnsupervised Learning: Geometric priors enable learning without extensive labeled data\n\nGenerative Priors: Latent space structure encodes environmental understanding\n\nNon-convex Optimization: Bayesian inference provides guarantees in complex landscapes\n\nAutonomous Driving Applications\nThe MNIST demo serves as proof-of-concept for:\n\nMulti-modal sensor fusion (extending to LIDAR, camera, radar)\n\nReal-time uncertainty quantification for safety-critical decisions\n\nAdaptive perception in changing environmental conditions\n\nUnique Value Proposition\nBiological Systems Perspective\nUnlike traditional CS/ML approaches, this work brings:\n\nSystems neuroscience insights into adaptive behavior\n\nCybernetics principles for feedback-driven learning\n\nDual-timescale modeling from computational neuroscience\n\nResearch-to-Application Bridge\nTheoretical foundation: Bayesian inference and uncertainty encoding\n\nPractical implementation: PyTorch-based neural networks\n\nIndustrial relevance: Autonomous driving safety and perception\n\nFuture Roadmap\nPhase 1: MNIST Proof-of-Concept ✅\n Uncertainty-aware classification\n\n Geometric latent space implementation\n\n Biological dynamics integration\n\nPhase 2: Computer Vision Extension 🔄\n CARLA simulation environment integration\n\n Multi-modal sensor fusion architecture\n\n Real-time perception pipeline\n\nPhase 3: Production Deployment 🔮\n Hardware-agnostic implementation\n\n Edge computing optimization\n\n Vehicle platform integration\n\nTechnical Implementation Notes\nMathematical Foundation\npython\n# Dual-timescale uncertainty update (inspired by LC-NE dynamics)\nwithin_trial_uncertainty = monte_carlo_dropout(model, x)\nacross_trial_uncertainty = bayesian_update(prior_context, prediction_history)\ncombined_uncertainty = geometric_prior_fusion(within_trial, across_trial)\nKey Dependencies\nPyTorch: Neural network implementation and training\n\nNumPy: Mathematical operations and array handling\n\nSciPy: Statistical distributions and optimization\n\nMatplotlib/Seaborn: Uncertainty visualization\n\nInstallation & Usage\nbash\n# Clone repository\ngit clone https://github.com/yourusername/adaptive-bayesian-driver.git\ncd adaptive-bayesian-driver\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run MNIST demonstration\njupyter notebook experiments/mnist_demo.ipynb\nResearch Background\nThis project builds on my published work in computational neuroscience, specifically:\n\nContextual uncertainty modeling in primate locus coeruleus\n\nDual-timescale Bayesian inference for perceptual decision-making\n\nBiological vision principles applied to artificial systems\n\nThe connection to Helm.ai's technology philosophy stems from shared interests in uncertainty-aware perception, biological inspiration, and principled approaches to autonomous driving challenges.\n\n\nAdaptive Bayesian Driver: Bridging Neuroscience and Computer Vision\nProject Overview\nAdaptive Bayesian Driver is a demonstration project that bridges biological uncertainty modeling principles from neuroscience to computer vision applications in autonomous driving. This work translates dual-timescale uncertainty encoding from the locus coeruleus-norepinephrine (LC-NE) system into geometric priors for neural network perception, directly connecting to Helm.ai's Deep Teaching methodology.\n\nCore Innovation: From Biological Vision to Artificial Perception\nThis project demonstrates how biological uncertainty processing can inform artificial vision systems through:\n\nDual-Timescale Uncertainty Modeling\nBased on my COSYNE 2014 research, this implementation captures uncertainty at two distinct temporal scales:\n\nWithin-trial uncertainty: Real-time confidence in individual predictions\n\nAcross-trial uncertainty: Contextual adaptation based on environmental volatility\n\nGeometric Priors in Latent Space\nMoving beyond traditional classification, we implement:\n\nUncertainty-aware latent representations that encode geometric structure\n\nBayesian neural layers for principled uncertainty quantification\n\nAdaptive learning dynamics modulated by confidence estimates\n\nTechnical Architecture\nMinimum Viable Demo: MNIST Uncertainty Classification\ntext\nadaptive_bayesian_driver/\n├── models/\n│   ├── uncertainty_cnn.py      # Bayesian CNN with Monte Carlo Dropout\n│   ├── geometric_prior.py      # Latent space constraints\n│   └── dual_timescale.py       # LC-NE inspired uncertainty dynamics\n├── experiments/\n│   └── mnist_demo.ipynb        # Interactive demonstration notebook\n├── utils/\n│   ├── uncertainty_metrics.py  # KL divergence, Mahalanobis distance\n│   └── visualization.py        # Uncertainty evolution plots\n└── config/\n    └── experiment_config.yaml  # Hyperparameters and settings\nKey Components\n1. Uncertainty-Aware CNN Architecture\nMonte Carlo Dropout for epistemic uncertainty estimation\n\nBayesian layers for aleatoric uncertainty quantification\n\nConfidence-based learning rate adaptation\n\n2. Biological Inspiration Integration\nLC-NE dual-timescale dynamics applied to prediction confidence\n\nContextual priors that adapt based on prediction history\n\nExploration/exploitation balance informed by uncertainty estimates\n\n3. Geometric Latent Space\nVAE-style encoding with geometric constraints\n\nManifold learning for structured uncertainty representation\n\nPhysics-informed priors for autonomous driving relevance\n\nConnection to Helm.ai's Technology Stack\nDeep Teaching Methodology Alignment\nThis project directly connects to Helm.ai's core principles:\n\nUnsupervised Learning: Geometric priors enable learning without extensive labeled data\n\nGenerative Priors: Latent space structure encodes environmental understanding\n\nNon-convex Optimization: Bayesian inference provides guarantees in complex landscapes\n\nAutonomous Driving Applications\nThe MNIST demo serves as proof-of-concept for:\n\nMulti-modal sensor fusion (extending to LIDAR, camera, radar)\n\nReal-time uncertainty quantification for safety-critical decisions\n\nAdaptive perception in changing environmental conditions\n\nUnique Value Proposition\nBiological Systems Perspective\nUnlike traditional CS/ML approaches, this work brings:\n\nSystems neuroscience insights into adaptive behavior\n\nCybernetics principles for feedback-driven learning\n\nDual-timescale modeling from computational neuroscience\n\nResearch-to-Application Bridge\nTheoretical foundation: Bayesian inference and uncertainty encoding\n\nPractical implementation: PyTorch-based neural networks\n\nIndustrial relevance: Autonomous driving safety and perception\n\nFuture Roadmap\nPhase 1: MNIST Proof-of-Concept ✅\n Uncertainty-aware classification\n\n Geometric latent space implementation\n\n Biological dynamics integration\n\nPhase 2: Computer Vision Extension 🔄\n CARLA simulation environment integration\n\n Multi-modal sensor fusion architecture\n\n Real-time perception pipeline\n\nPhase 3: Production Deployment 🔮\n Hardware-agnostic implementation\n\n Edge computing optimization\n\n Vehicle platform integration\n\nTechnical Implementation Notes\nMathematical Foundation\npython\n# Dual-timescale uncertainty update (inspired by LC-NE dynamics)\nwithin_trial_uncertainty = monte_carlo_dropout(model, x)\nacross_trial_uncertainty = bayesian_update(prior_context, prediction_history)\ncombined_uncertainty = geometric_prior_fusion(within_trial, across_trial)\nKey Dependencies\nPyTorch: Neural network implementation and training\n\nNumPy: Mathematical operations and array handling\n\nSciPy: Statistical distributions and optimization\n\nMatplotlib/Seaborn: Uncertainty visualization\n\nInstallation & Usage\nbash\n# Clone repository\ngit clone https://github.com/yourusername/adaptive-bayesian-driver.git\ncd adaptive-bayesian-driver\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run MNIST demonstration\njupyter notebook experiments/mnist_demo.ipynb\nResearch Background\nThis project builds on my published work in computational neuroscience, specifically:\n\nContextual uncertainty modeling in primate locus coeruleus\n\nDual-timescale Bayesian inference for perceptual decision-making\n\nBiological vision principles applied to artificial systems\n\nThe connection to Helm.ai's technology philosophy stems from shared interests in uncertainty-aware perception, biological inspiration, and principled approaches to autonomous driving challenges.\n",
      "size_bytes": 11946,
      "size_kb": 11.67,
      "content_preview": "Adaptive Bayesian Driver: Bridging Neuroscience and Computer Vision\nProject Overview\nAdaptive Bayesian Driver is a demonstration project that bridges biological uncertainty modeling principles from ne...",
      "directory": "",
      "path": "README.md"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "extension": ".cfg",
      "filename": "setup.cfg",
      "content": "[metadata]\r\nname = adaptive-bayesian-driver\r\ndescription = LC-NE inspired adaptive Bayesian learning for autonomous driving\r\nlong_description = file: README.md\r\nlong_description_content_type = text/markdown\r\nlicense = MIT\r\nauthor = Your Name\r\nauthor_email = your.email@example.com\r\nclassifiers =\r\n    Development Status :: 3 - Alpha\r\n    Intended Audience :: Science/Research\r\n    License :: OSI Approved :: MIT License\r\n    Programming Language :: Python :: 3.11\r\n    Topic :: Scientific/Engineering :: Artificial Intelligence\r\n\r\n[options]\r\npackages = find:\r\npython_requires = >=3.11\r\ninclude_package_data = True\r\nzip_safe = False\r\n\r\n[options.packages.find]\r\nwhere = .\r\ninclude = adaptive_bayesian_driver*\r\nexclude = tests*\r\n\r\n[options.extras_require]\r\ndev =\r\n    pytest>=8.0.0\r\n    pytest-cov>=5.0.0\r\n    ruff>=0.4.4\r\n    mypy>=1.10.0\r\n    pre-commit>=3.7.0\r\n\r\n[flake8]\r\nmax-line-length = 88\r\nextend-ignore = E203, W503\r\nexclude = .git,__pycache__,build,dist,.eggs\r\n\r\n[coverage:run]\r\nsource = adaptive_bayesian_driver\r\nomit =\r\n    */tests/*\r\n    */test_*\r\n    setup.py\r\n\r\n[coverage:report]\r\nexclude_lines =\r\n    pragma: no cover\r\n    def __repr__\r\n    raise AssertionError\r\n    raise NotImplementedError\r\n",
      "size_bytes": 1207,
      "size_kb": 1.18,
      "content_preview": "[metadata]\r\nname = adaptive-bayesian-driver\r\ndescription = LC-NE inspired adaptive Bayesian learning for autonomous driving\r\nlong_description = file: README.md\r\nlong_description_content_type = text/ma...",
      "directory": "",
      "path": "setup.cfg"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 12:25:10",
      "extension": ".py",
      "filename": "setup.py",
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nSetup script for adaptive-bayesian-driver package.\r\nModern Python packaging using setuptools with pyproject.toml configuration.\r\n\"\"\"\r\n\r\nfrom setuptools import setup\r\n\r\n# Configuration is now in pyproject.toml\r\n# This file maintained for backwards compatibility and editable installs\r\nif __name__ == \"__main__\":\r\n    setup()\r\n",
      "size_bytes": 354,
      "size_kb": 0.35,
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nSetup script for adaptive-bayesian-driver package.\r\nModern Python packaging using setuptools with pyproject.toml configuration.\r\n\"\"\"\r\n\r\nfrom setuptools import setup\r\n\r\n# C...",
      "directory": "",
      "path": "setup.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": ".dockerignore",
      "filename": ".dockerignore",
      "content": "# Version control\r\n.git/\r\n.gitignore\r\n\r\n# Python artifacts\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.so\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\n*.egg-info/\r\n.installed.cfg\r\n*.egg\r\n\r\n# Development environments\r\n.env\r\n.venv/\r\nenv/\r\nvenv/\r\nENV/\r\n\r\n# Testing and coverage\r\n.pytest_cache/\r\n.coverage\r\nhtmlcov/\r\n.tox/\r\n\r\n# Documentation\r\ndocs/_build/\r\nnotebooks/\r\n\r\n# IDE files\r\n.vscode/\r\n.idea/\r\n*.swp\r\n*.swo\r\n*~\r\n\r\n# OS files\r\n.DS_Store\r\nThumbs.db\r\n\r\n# Project-specific\r\n_chatbot/\r\n_reports/\r\n*.log\r\n",
      "size_bytes": 566,
      "size_kb": 0.55,
      "content_preview": "# Version control\r\n.git/\r\n.gitignore\r\n\r\n# Python artifacts\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n*.so\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nv...",
      "directory": "tests",
      "path": "tests/.dockerignore"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": ".gitignore",
      "filename": ".gitignore",
      "content": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\npip-wheel-metadata/\r\nshare/python-wheels/\r\n.installed.cfg\r\n*.egg-info/\r\n*.egg\r\n\r\n# PyInstaller\r\n#  Usually these files are written by a python script from a template\r\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\r\n*.manifest\r\n*.spec\r\n\r\n# Installer logs\r\npip-log.txt\r\npip-delete-this-directory.txt\r\n\r\n# Unit test / coverage reports\r\nhtmlcov/\r\n.tox/\r\n.nox/\r\n.coverage\r\n.coverage.*\r\n.cache\r\nnosetests.xml\r\ncoverage.xml\r\n*.cover\r\n*.py,cover\r\n.hypothesis/\r\n.pytest_cache/\r\n\r\n# Translations\r\n*.mo\r\n*.pot\r\n\r\n# Django stuff:\r\n*.log\r\nlocal_settings.py\r\ndb.sqlite3\r\ndb.sqlite3-journal\r\n\r\n# Flask stuff:\r\ninstance/\r\n.webassets-cache\r\n\r\n# Scrapy stuff:\r\n.scrapy\r\n\r\n# Sphinx documentation\r\ndocs/_build/\r\n\r\n# PyBuilder\r\ntarget/\r\n\r\n# Jupyter Notebook\r\n.ipynb_checkpoints\r\n\r\n# IPython\r\nprofile_default/\r\nipython_config.py\r\n\r\n# pyenv\r\n.python-version\r\n\r\n# pipenv\r\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\r\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\r\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\r\n#   install all needed dependencies.\r\n#Pipfile.lock\r\n\r\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\r\n__pypackages__/\r\n\r\n# Celery stuff\r\ncelerybeat-schedule\r\ncelerybeat.pid\r\n\r\n# SageMath parsed files\r\n*.sage.py\r\n\r\n# Environments\r\n# .env\r\n.venv\r\nenv/\r\nvenv/\r\nENV/\r\nenv.bak/\r\nvenv.bak/\r\n\r\n# Spyder project settings\r\n.spyderproject\r\n.spyproject\r\n\r\n# Rope project settings\r\n.ropeproject\r\n\r\n# mkdocs documentation\r\n/site\r\n\r\n# mypy\r\n.mypy_cache/\r\n.dmypy.json\r\ndmypy.json\r\n\r\n# Pyre type checker\r\n.pyre/\r\n\r\n# Project-specific\r\ndebug_scripts/\r\n*_debug.py\r\n*test_debug.py\r\nrun_specific_tests.py\r\ntest_particle_filter.py\r\ncomprehensive_test.py\r\ntorch_cache/\r\n.ci_env\r\n*.tmp\r\n*.bak\r\n\r\n# Chatbot assistant tools\r\n# _chatbot/\r\n# !_chatbot/README.md\r\n\r\n# VS Code\r\n.vscode/\r\n*.code-workspace\r\n\r\n# PyCharm\r\n.idea/\r\n\r\n# MacOS\r\n.DS_Store\r\n\r\n# Windows\r\nThumbs.db\r\nehthumbs.db\r\nDesktop.ini\r\n",
      "size_bytes": 2303,
      "size_kb": 2.25,
      "content_preview": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n...",
      "directory": "tests",
      "path": "tests/.gitignore"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-16 21:33:00",
      "extension": "",
      "filename": "Dockerfile",
      "content": "# =============================================================================\r\n# Multi-stage Docker build for adaptive-bayesian-driver\r\n# Optimized for development and demonstration purposes\r\n# =============================================================================\r\n\r\n# Build stage - install dependencies and build tools\r\nFROM python:3.11-slim as builder\r\n\r\nWORKDIR /usr/src/app\r\n\r\n# Install system dependencies for PyTorch compilation\r\nRUN apt-get update && apt-get install -y \\\r\n    build-essential \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n# Copy requirements and install Python dependencies\r\nCOPY requirements.txt .\r\nRUN pip install --no-cache-dir --user -r requirements.txt\r\n\r\n# Production stage - minimal runtime environment\r\nFROM python:3.11-slim\r\n\r\nWORKDIR /usr/src/app\r\n\r\n# Create non-root user for security\r\nRUN groupadd --system app && useradd --system --group app\r\n\r\n# Copy installed packages from builder stage\r\nCOPY --from=builder /root/.local /home/app/.local\r\n\r\n# Copy application code\r\nCOPY ./adaptive_bayesian_driver ./adaptive_bayesian_driver\r\nCOPY ./config ./config\r\nCOPY ./demo.py .\r\n\r\n# Set ownership and switch to non-root user\r\nRUN chown -R app:app /usr/src/app\r\nUSER app\r\n\r\n# Add local Python packages to PATH\r\nENV PATH=/home/app/.local/bin:$PATH\r\n\r\n# Expose port for potential API serving\r\nEXPOSE 8000\r\n\r\n# Default command runs the demo\r\nCMD [\"python\", \"demo.py\"]\r\n",
      "size_bytes": 1399,
      "size_kb": 1.37,
      "content_preview": "# =============================================================================\r\n# Multi-stage Docker build for adaptive-bayesian-driver\r\n# Optimized for development and demonstration purposes\r\n# ====...",
      "directory": "tests",
      "path": "tests/Dockerfile"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "extension": ".py",
      "filename": "test_environment.py",
      "content": "\"\"\"\r\nTest suite for environment components.\r\n\"\"\"\r\n\r\nimport unittest\r\nimport numpy as np\r\nimport torch\r\nfrom adaptive_bayesian_driver.environment import (\r\n    VolatilityController, VolatilityRegime,\r\n    SceneRenderer, TaskHMM, TaskState\r\n)\r\nfrom adaptive_bayesian_driver.config import load_config\r\n\r\nclass TestVolatilityController(unittest.TestCase):\r\n    \"\"\"Test VolatilityController functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'volatility_parameters': {\r\n                'hazard_rate': 0.01,\r\n                'tau_drift': 30,\r\n                'bias_extremes': [0.2, 0.8]\r\n            }\r\n        }\r\n        self.controller = VolatilityController(self.config)\r\n\r\n    def test_initialization(self):\r\n        \"\"\"Test proper initialization.\"\"\"\r\n        self.assertEqual(self.controller.current_bias, 0.5)\r\n        self.assertEqual(self.controller.target_bias, 0.5)\r\n        self.assertEqual(self.controller.volatility_regime, VolatilityRegime.STABLE)\r\n\r\n    def test_update(self):\r\n        \"\"\"Test update functionality.\"\"\"\r\n        result = self.controller.update(0)\r\n\r\n        self.assertIn('current_bias', result)\r\n        self.assertIn('target_bias', result)\r\n        self.assertIn('volatility_regime', result)\r\n        self.assertIn('context_switched', result)\r\n\r\n    def test_bias_bounds(self):\r\n        \"\"\"Test bias stays within bounds.\"\"\"\r\n        for _ in range(100):\r\n            self.controller.update(0)\r\n            self.assertGreaterEqual(self.controller.current_bias, 0.0)\r\n            self.assertLessEqual(self.controller.current_bias, 1.0)\r\n\r\n    def test_reset(self):\r\n        \"\"\"Test reset functionality.\"\"\"\r\n        # Change state\r\n        for _ in range(10):\r\n            self.controller.update(0)\r\n\r\n        # Reset\r\n        self.controller.reset()\r\n\r\n        self.assertEqual(self.controller.current_bias, 0.5)\r\n        self.assertEqual(len(self.controller.bias_history), 0)\r\n\r\nclass TestSceneRenderer(unittest.TestCase):\r\n    \"\"\"Test SceneRenderer functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'visual_parameters': {\r\n                'scene_size': [64, 64],\r\n                'safe_background_color': [50, 50, 50],\r\n                'danger_background_color': [255, 215, 0]\r\n            }\r\n        }\r\n        self.renderer = SceneRenderer(self.config)\r\n\r\n    def test_initialization(self):\r\n        \"\"\"Test proper initialization.\"\"\"\r\n        self.assertEqual(self.renderer.scene_size, [64, 64])\r\n        self.assertEqual(self.renderer.safe_color, [50, 50, 50])\r\n\r\n    def test_render_scene(self):\r\n        \"\"\"Test scene rendering.\"\"\"\r\n        params = {\r\n            'has_danger': True,\r\n            'orientation_angle': 45.0,\r\n            'difficulty': 0.2,\r\n            'correct_direction': 'right'\r\n        }\r\n\r\n        scene, metadata = self.renderer.render_scene(params)\r\n\r\n        # Check output format\r\n        self.assertIsInstance(scene, torch.Tensor)\r\n        self.assertEqual(scene.shape, (1, 64, 64, 3))  # RGB channels\r\n        self.assertIsInstance(metadata, dict)\r\n\r\n        # Check metadata\r\n        self.assertEqual(metadata['has_danger'], True)\r\n        self.assertEqual(metadata['correct_direction'], 'right')\r\n\r\n    def test_scene_sequence(self):\r\n        \"\"\"Test scene sequence generation.\"\"\"\r\n        scenes, metadata_list = self.renderer.create_scene_sequence(\r\n            n_scenes=10, bias=0.7, danger_probability=0.8\r\n        )\r\n\r\n        self.assertEqual(scenes.shape[0], 10)  # Batch size\r\n        self.assertEqual(len(metadata_list), 10)\r\n\r\n    def test_difficulty_application(self):\r\n        \"\"\"Test difficulty effects.\"\"\"\r\n        params_easy = {\r\n            'has_danger': False,\r\n            'orientation_angle': 0.0,\r\n            'difficulty': 0.0,\r\n            'correct_direction': 'right'\r\n        }\r\n\r\n        params_hard = {\r\n            'has_danger': False,\r\n            'orientation_angle': 0.0,\r\n            'difficulty': 0.8,\r\n            'correct_direction': 'right'\r\n        }\r\n\r\n        scene_easy, _ = self.renderer.render_scene(params_easy)\r\n        scene_hard, _ = self.renderer.render_scene(params_hard)\r\n\r\n        # Hard scene should be different due to noise/occlusion\r\n        self.assertFalse(torch.equal(scene_easy, scene_hard))\r\n\r\nclass TestTaskHMM(unittest.TestCase):\r\n    \"\"\"Test TaskHMM functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'hmm_learning_rate': 0.01,\r\n            'hmm_adaptation': True\r\n        }\r\n        self.hmm = TaskHMM(self.config)\r\n\r\n    def test_initialization(self):\r\n        \"\"\"Test proper initialization.\"\"\"\r\n        self.assertEqual(self.hmm.n_states, len(TaskState))\r\n        self.assertEqual(self.hmm.current_state, 0)\r\n        self.assertEqual(self.hmm.transition_matrix.shape, (self.hmm.n_states, self.hmm.n_states))\r\n\r\n    def test_state_update(self):\r\n        \"\"\"Test state update.\"\"\"\r\n        observation = {\r\n            'has_danger': True,\r\n            'orientation_angle': 90.0,\r\n            'correct_direction': 'left'\r\n        }\r\n\r\n        result = self.hmm.update_state(observation)\r\n\r\n        self.assertIn('current_state', result)\r\n        self.assertIn('state_name', result)\r\n        self.assertIn('state_probabilities', result)\r\n\r\n    def test_observation_generation(self):\r\n        \"\"\"Test observation generation.\"\"\"\r\n        observation = self.hmm.generate_observation()\r\n\r\n        self.assertIn('has_danger', observation)\r\n        self.assertIn('orientation_angle', observation)\r\n        self.assertIn('correct_direction', observation)\r\n\r\n    def test_likelihood_computation(self):\r\n        \"\"\"Test likelihood computation.\"\"\"\r\n        observations = [\r\n            {'has_danger': True, 'orientation_angle': 90.0, 'correct_direction': 'left'},\r\n            {'has_danger': False, 'orientation_angle': 0.0, 'correct_direction': 'right'}\r\n        ]\r\n\r\n        likelihood = self.hmm.compute_likelihood(observations)\r\n        self.assertIsInstance(likelihood, float)\r\n\r\n    def test_reset(self):\r\n        \"\"\"Test reset functionality.\"\"\"\r\n        # Process some observations\r\n        for i in range(5):\r\n            obs = self.hmm.generate_observation()\r\n            self.hmm.update_state(obs)\r\n\r\n        # Reset\r\n        self.hmm.reset()\r\n\r\n        self.assertEqual(self.hmm.current_state, 0)\r\n        self.assertEqual(len(self.hmm.state_history), 0)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n",
      "size_bytes": 6546,
      "size_kb": 6.39,
      "content_preview": "\"\"\"\r\nTest suite for environment components.\r\n\"\"\"\r\n\r\nimport unittest\r\nimport numpy as np\r\nimport torch\r\nfrom adaptive_bayesian_driver.environment import (\r\n    VolatilityController, VolatilityRegime,\r\n...",
      "directory": "tests",
      "path": "tests/test_environment.py"
    },
    {
      "is_binary": false,
      "last_modified": "2025-07-13 07:23:36",
      "extension": ".py",
      "filename": "test_recursive_bayes.py",
      "content": "\"\"\"\r\nTest suite for recursive Bayesian learning components.\r\n\"\"\"\r\n\r\nimport unittest\r\nimport numpy as np\r\nimport torch\r\nfrom adaptive_bayesian_driver.models import (\r\n    RecursiveBayesianLearner, InferenceMode,\r\n    SurpriseMeter, SurpriseType,\r\n    AdaptiveParticleFilter\r\n)\r\nfrom adaptive_bayesian_driver.config import load_config\r\n\r\nclass TestRecursiveBayesianLearner(unittest.TestCase):\r\n    \"\"\"Test RecursiveBayesianLearner functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'model_parameters': {\r\n                'learning_rate': 0.01,\r\n                'adaptation_rate': 0.1,\r\n                'state_dim': 4,\r\n                'obs_dim': 2,\r\n                'baseline_window': 20\r\n            },\r\n            'experiment': {'seed': 42}\r\n        }\r\n\r\n    def test_gaussian_initialization(self):\r\n        \"\"\"Test Gaussian inference mode initialization.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        self.assertEqual(learner.inference_mode, InferenceMode.GAUSSIAN)\r\n        self.assertEqual(learner.state_mean.shape, (4,))\r\n        self.assertEqual(learner.state_cov.shape, (4, 4))\r\n\r\n    def test_particle_initialization(self):\r\n        \"\"\"Test particle filter initialization.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.PARTICLE)\r\n\r\n        self.assertEqual(learner.inference_mode, InferenceMode.PARTICLE)\r\n        self.assertEqual(learner.particles.shape, (100, 4))  # Default n_particles\r\n        self.assertEqual(len(learner.weights), 100)\r\n\r\n    def test_gaussian_prediction(self):\r\n        \"\"\"Test Gaussian prediction step.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        prediction = learner.predict()\r\n\r\n        self.assertIn('state_mean', prediction)\r\n        self.assertIn('state_cov', prediction)\r\n        self.assertIn('obs_mean', prediction)\r\n\r\n    def test_gaussian_update(self):\r\n        \"\"\"Test Gaussian update step.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        observation = np.array([1.0, 2.0])\r\n        result = learner.update(observation)\r\n\r\n        self.assertIn('primary_surprise', result)\r\n        self.assertIn('context_beliefs', result)\r\n        self.assertIn('adaptive_learning_rate', result)\r\n\r\n    def test_particle_update(self):\r\n        \"\"\"Test particle filter update step.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.PARTICLE)\r\n\r\n        observation = np.array([1.0, 2.0])\r\n        result = learner.update(observation)\r\n\r\n        self.assertIn('primary_surprise', result)\r\n        self.assertIn('effective_sample_size', result)\r\n\r\n    def test_decision_making(self):\r\n        \"\"\"Test decision making.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        observation = np.array([1.0, 2.0])\r\n        decision = learner.make_decision(observation, ['left', 'right'])\r\n\r\n        self.assertIn('decision', decision)\r\n        self.assertIn('confidence', decision)\r\n        self.assertIn(decision['decision'], ['left', 'right'])\r\n\r\n    def test_reset(self):\r\n        \"\"\"Test reset functionality.\"\"\"\r\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\r\n\r\n        # Process some data\r\n        for _ in range(5):\r\n            observation = np.random.randn(2)\r\n            learner.update(observation)\r\n\r\n        # Reset\r\n        learner.reset()\r\n\r\n        self.assertEqual(len(learner.surprise_history), 0)\r\n        self.assertEqual(learner.context_uncertainty, 0.5)\r\n\r\nclass TestSurpriseMeter(unittest.TestCase):\r\n    \"\"\"Test SurpriseMeter functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.meter = SurpriseMeter(mode=SurpriseType.CHI2)\r\n\r\n    def test_chi2_surprise(self):\r\n        \"\"\"Test chi-squared surprise computation.\"\"\"\r\n        innovation = np.array([1.0, 2.0])\r\n        covariance = np.eye(2) * 0.5\r\n\r\n        surprise = self.meter.compute_surprise(innov=innovation, S=covariance)\r\n\r\n        self.assertIsInstance(surprise, float)\r\n        self.assertGreaterEqual(surprise, 0.0)\r\n\r\n    def test_reconstruction_surprise(self):\r\n        \"\"\"Test reconstruction error surprise.\"\"\"\r\n        meter = SurpriseMeter(mode=SurpriseType.RECONSTRUCTION)\r\n\r\n        x = torch.randn(1, 10)\r\n        x_recon = x + torch.randn(1, 10) * 0.1\r\n\r\n        surprise = meter.compute_surprise(x=x, x_recon=x_recon)\r\n\r\n        self.assertIsInstance(surprise, float)\r\n        self.assertGreaterEqual(surprise, 0.0)\r\n\r\n    def test_baseline_update(self):\r\n        \"\"\"Test baseline statistics update.\"\"\"\r\n        meter = SurpriseMeter(mode=SurpriseType.CHI2, baseline_window=5)\r\n\r\n        # Add some errors\r\n        for i in range(10):\r\n            meter.update_baseline(float(i))\r\n\r\n        # Check baseline stats updated\r\n        self.assertGreater(meter.baseline_stats['mean'], 0)\r\n        self.assertEqual(len(meter.recent_errors), 5)  # Window size\r\n\r\n    def test_normalized_surprise(self):\r\n        \"\"\"Test normalized surprise computation.\"\"\"\r\n        meter = SurpriseMeter(mode=SurpriseType.CHI2)\r\n\r\n        # Add baseline data\r\n        for _ in range(20):\r\n            innovation = np.random.randn(2)\r\n            covariance = np.eye(2)\r\n            meter.compute_normalized_surprise(innov=innovation, S=covariance)\r\n\r\n        # Compute normalized surprise\r\n        innovation = np.array([5.0, 5.0])  # Large innovation\r\n        covariance = np.eye(2)\r\n\r\n        surprise = meter.compute_normalized_surprise(innov=innovation, S=covariance)\r\n\r\n        self.assertGreaterEqual(surprise, 0.0)\r\n\r\nclass TestAdaptiveParticleFilter(unittest.TestCase):\r\n    \"\"\"Test AdaptiveParticleFilter functionality.\"\"\"\r\n\r\n    def setUp(self):\r\n        \"\"\"Set up test configuration.\"\"\"\r\n        self.config = {\r\n            'n_particles': 50,\r\n            'state_dim': 3,\r\n            'observation_dim': 2,\r\n            'process_noise_std': 0.1,\r\n            'observation_noise_std': 0.1,\r\n            'adaptive_noise': True,\r\n            'adaptive_resampling': True\r\n        }\r\n        self.pf = AdaptiveParticleFilter(self.config)\r\n\r\n    def test_initialization(self):\r\n        \"\"\"Test proper initialization.\"\"\"\r\n        self.assertEqual(self.pf.particles.shape, (50, 3))\r\n        self.assertEqual(len(self.pf.weights), 50)\r\n        self.assertAlmostEqual(np.sum(self.pf.weights), 1.0, places=6)\r\n\r\n    def test_prediction(self):\r\n        \"\"\"Test prediction step.\"\"\"\r\n        result = self.pf.predict()\r\n\r\n        self.assertIn('mean', result)\r\n        self.assertIn('covariance', result)\r\n        self.assertEqual(result['mean'].shape, (3,))\r\n\r\n    def test_update(self):\r\n        \"\"\"Test update step.\"\"\"\r\n        observation = np.array([1.0, 2.0])\r\n\r\n        result = self.pf.update(observation)\r\n\r\n        self.assertIn('mean', result)\r\n        self.assertIn('effective_sample_size', result)\r\n        self.assertIn('resampled', result)\r\n\r\n    def test_resampling(self):\r\n        \"\"\"Test resampling functionality.\"\"\"\r\n        # Set uneven weights to trigger resampling\r\n        self.pf.weights[0] = 0.9\r\n        self.pf.weights[1:] = 0.1 / (len(self.pf.weights) - 1)\r\n\r\n        observation = np.array([1.0, 2.0])\r\n        result = self.pf.update(observation)\r\n\r\n        # Should have resampled due to low effective sample size\r\n        # (This might not always trigger, but test structure is correct)\r\n        self.assertIn('resampled', result)\r\n\r\n    def test_state_estimate(self):\r\n        \"\"\"Test state estimation.\"\"\"\r\n        estimate = self.pf.get_state_estimate()\r\n\r\n        self.assertEqual(estimate.shape, (3,))\r\n        self.assertIsInstance(estimate, np.ndarray)\r\n\r\n    def test_uncertainty(self):\r\n        \"\"\"Test uncertainty computation.\"\"\"\r\n        uncertainty = self.pf.get_uncertainty()\r\n\r\n        self.assertIsInstance(uncertainty, float)\r\n        self.assertGreaterEqual(uncertainty, 0.0)\r\n\r\n    def test_reset(self):\r\n        \"\"\"Test reset functionality.\"\"\"\r\n        # Run some updates\r\n        for _ in range(5):\r\n            observation = np.random.randn(2)\r\n            self.pf.update(observation)\r\n\r\n        # Reset\r\n        self.pf.reset()\r\n\r\n        self.assertEqual(len(self.pf.state_history), 0)\r\n        self.assertAlmostEqual(np.sum(self.pf.weights), 1.0, places=6)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n",
      "size_bytes": 8440,
      "size_kb": 8.24,
      "content_preview": "\"\"\"\r\nTest suite for recursive Bayesian learning components.\r\n\"\"\"\r\n\r\nimport unittest\r\nimport numpy as np\r\nimport torch\r\nfrom adaptive_bayesian_driver.models import (\r\n    RecursiveBayesianLearner, Infe...",
      "directory": "tests",
      "path": "tests/test_recursive_bayes.py"
    }
  ],
  "metadata": {
    "repository_name": "SurpriseLearner",
    "repository_path": "C:\\Users\\azriy\\PortDenv\\Projects\\SurpriseLearner\\SurpriseLearner",
    "exported_files": 27,
    "powershell_version": "7.5.2",
    "total_files": 44,
    "export_date": "2025-08-05 00:08:57",
    "skipped_files": 17,
    "script_version": "1.1"
  }
}