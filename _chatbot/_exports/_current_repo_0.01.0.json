{
  "metadata": {
    "skipped_files": 13,
    "export_date": "2025-07-12 08:22:03",
    "total_files": 53,
    "repository_path": "C:\\OneDrive\\Onedrive-PortableDenv\\Projects\\surprise-learning-exploration-exploitation",
    "exported_files": 40
  },
  "files": [
    {
      "last_modified": "2025-07-08 16:01:37",
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nQuick CI/CD readiness check.\r\n\"\"\"\r\n\r\nimport subprocess\r\nimport sys\r\n\r\ndef check_cicd_readiness():\r\n    \"\"\"Quick check for CI/CD readiness.\"\"\"\r\n    print(\"🔍 Quick CI/CD Re...",
      "extension": ".py",
      "size_kb": 1.95,
      "path": "_chatbot/_diagnostics/cicd_check.py",
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nQuick CI/CD readiness check.\r\n\"\"\"\r\n\r\nimport subprocess\r\nimport sys\r\n\r\ndef check_cicd_readiness():\r\n    \"\"\"Quick check for CI/CD readiness.\"\"\"\r\n    print(\"🔍 Quick CI/CD Readiness Check\\n\")\r\n\r\n    checks = [\r\n        {\r\n            'name': 'Package Import',\r\n            'cmd': 'python -c \"import adaptive_bayesian_driver; print(\\'OK\\')\"',\r\n            'critical': True\r\n        },\r\n        {\r\n            'name': 'Critical Whitespace Issues',\r\n            'cmd': 'python -m flake8 adaptive_bayesian_driver/ --select=W293 --count',\r\n            'critical': True\r\n        },\r\n        {\r\n            'name': 'Basic Syntax Check',\r\n            'cmd': 'python -m py_compile setup.py',\r\n            'critical': True\r\n        },\r\n        {\r\n            'name': 'Requirements Check',\r\n            'cmd': 'python -c \"import torch, numpy, matplotlib; print(\\'Dependencies OK\\')\"',\r\n            'critical': False\r\n        }\r\n    ]\r\n\r\n    passed = 0\r\n    critical_passed = 0\r\n    critical_total = sum(1 for check in checks if check['critical'])\r\n\r\n    for check in checks:\r\n        try:\r\n            result = subprocess.run(check['cmd'], shell=True, capture_output=True, text=True, check=True)\r\n            status = \"✅ PASS\"\r\n            passed += 1\r\n            if check['critical']:\r\n                critical_passed += 1\r\n        except subprocess.CalledProcessError:\r\n            status = \"❌ FAIL\"\r\n\r\n        critical_marker = \"🔴\" if check['critical'] else \"⚪\"\r\n        print(f\"{critical_marker} {check['name']}: {status}\")\r\n\r\n    print(f\"\\nResults: {passed}/{len(checks)} total, {critical_passed}/{critical_total} critical\")\r\n\r\n    if critical_passed == critical_total:\r\n        print(\"🎉 CI/CD READY - All critical checks passed!\")\r\n        return True\r\n    else:\r\n        print(\"⚠️  CI/CD NOT READY - Critical issues remain\")\r\n        return False\r\n\r\nif __name__ == \"__main__\":\r\n    success = check_cicd_readiness()\r\n    sys.exit(0 if success else 1)\r\n",
      "directory": "_chatbot\\_diagnostics",
      "is_binary": false,
      "size_bytes": 1992,
      "filename": "cicd_check.py"
    },
    {
      "last_modified": "2025-07-08 16:00:47",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"\nComprehensive formatting scanner for the repository.\nDetects line ending issues, encoding problems, and whitespace issues.\n\"\"\"\n\nimport os\nimport re\nimport chardet\nfrom pathl...",
      "extension": ".py",
      "size_kb": 4.64,
      "path": "_chatbot/_diagnostics/format_scanner.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nComprehensive formatting scanner for the repository.\nDetects line ending issues, encoding problems, and whitespace issues.\n\"\"\"\n\nimport os\nimport re\nimport chardet\nfrom pathlib import Path\n\ndef check_file_encoding(file_path):\n    \"\"\"Check file encoding and detect potential issues.\"\"\"\n    try:\n        with open(file_path, 'rb') as f:\n            raw_data = f.read()\n\n        if not raw_data:\n            return \"empty\", None\n\n        detected = chardet.detect(raw_data)\n        encoding = detected.get('encoding', 'unknown')\n        confidence = detected.get('confidence', 0)\n\n        # Check for BOM\n        has_bom = raw_data.startswith(b'\\xef\\xbb\\xbf')\n\n        return encoding, confidence, has_bom\n    except Exception as e:\n        return f\"error: {e}\", None, None\n\ndef check_line_endings(file_path):\n    \"\"\"Check line ending consistency.\"\"\"\n    try:\n        with open(file_path, 'rb') as f:\n            content = f.read()\n\n        if not content:\n            return \"empty\"\n\n        crlf_count = content.count(b'\\r\\n')\n        lf_count = content.count(b'\\n') - crlf_count\n        cr_count = content.count(b'\\r') - crlf_count\n\n        endings = []\n        if crlf_count > 0:\n            endings.append(f\"CRLF({crlf_count})\")\n        if lf_count > 0:\n            endings.append(f\"LF({lf_count})\")\n        if cr_count > 0:\n            endings.append(f\"CR({cr_count})\")\n\n        if len(endings) > 1:\n            return f\"MIXED: {', '.join(endings)}\"\n        elif endings:\n            return endings[0]\n        else:\n            return \"NO_ENDINGS\"\n\n    except Exception as e:\n        return f\"error: {e}\"\n\ndef check_whitespace_issues(file_path):\n    \"\"\"Check for whitespace problems.\"\"\"\n    issues = []\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            lines = f.readlines()\n\n        for i, line in enumerate(lines, 1):\n            # Trailing whitespace\n            if line.rstrip('\\n\\r') != line.rstrip():\n                issues.append(f\"Line {i}: trailing whitespace\")\n\n            # Tabs mixed with spaces\n            if '\\t' in line and '    ' in line:\n                issues.append(f\"Line {i}: mixed tabs and spaces\")\n\n            # Non-ASCII characters that might cause issues\n            try:\n                line.encode('ascii')\n            except UnicodeEncodeError:\n                non_ascii = [c for c in line if ord(c) > 127]\n                if non_ascii:\n                    issues.append(f\"Line {i}: non-ASCII chars: {non_ascii[:3]}\")\n\n        return issues\n    except Exception as e:\n        return [f\"error reading file: {e}\"]\n\ndef scan_repository():\n    \"\"\"Scan the entire repository for formatting issues.\"\"\"\n    project_root = Path(\".\")\n    issues_found = {}\n\n    # File patterns to check\n    patterns = ['*.py', '*.txt', '*.md', '*.yml', '*.yaml', '*.cfg', '*.ini', '*.json']\n\n    print(\"🔍 Scanning repository for formatting issues...\\n\")\n\n    for pattern in patterns:\n        for file_path in project_root.rglob(pattern):\n            # Skip certain directories\n            if any(part in str(file_path) for part in ['.git', '__pycache__', '.pytest_cache', 'build', 'dist']):\n                continue\n\n            print(f\"Checking: {file_path}\")\n            file_issues = []\n\n            # Check encoding\n            encoding_info = check_file_encoding(file_path)\n            if encoding_info[0] not in ['utf-8', 'ascii'] or (len(encoding_info) > 2 and encoding_info[2]):\n                file_issues.append(f\"Encoding: {encoding_info}\")\n\n            # Check line endings\n            line_endings = check_line_endings(file_path)\n            if 'MIXED' in line_endings or 'CR(' in line_endings:\n                file_issues.append(f\"Line endings: {line_endings}\")\n\n            # Check whitespace issues for text files\n            if file_path.suffix in ['.py', '.txt', '.md', '.yml', '.yaml', '.cfg']:\n                whitespace_issues = check_whitespace_issues(file_path)\n                if whitespace_issues:\n                    file_issues.extend(whitespace_issues[:5])  # Limit to first 5 issues\n\n            if file_issues:\n                issues_found[str(file_path)] = file_issues\n\n    return issues_found\n\ndef main():\n    \"\"\"Main function to run the formatting scan.\"\"\"\n    issues = scan_repository()\n\n    if not issues:\n        print(\"✅ No formatting issues found!\")\n        return 0\n\n    print(f\"\\n❌ Found formatting issues in {len(issues)} files:\\n\")\n\n    for file_path, file_issues in issues.items():\n        print(f\"📁 {file_path}:\")\n        for issue in file_issues:\n            print(f\"   • {issue}\")\n        print()\n\n    print(f\"Total files with issues: {len(issues)}\")\n    return 1\n\nif __name__ == \"__main__\":\n    exit(main())\n",
      "directory": "_chatbot\\_diagnostics",
      "is_binary": false,
      "size_bytes": 4747,
      "filename": "format_scanner.py"
    },
    {
      "last_modified": "2025-07-08 16:00:47",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"\nComprehensive formatter to fix all repository formatting issues.\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\n\ndef fix_file_encoding_and_format(file_path):\n    \"\"\"Fix e...",
      "extension": ".py",
      "size_kb": 4.65,
      "path": "_chatbot/_fixing/fix_formatting.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nComprehensive formatter to fix all repository formatting issues.\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\n\ndef fix_file_encoding_and_format(file_path):\n    \"\"\"Fix encoding and formatting issues in a file.\"\"\"\n    print(f\"Fixing: {file_path}\")\n\n    try:\n        # Try to read with various encodings\n        content = None\n        for encoding in ['utf-8', 'iso-8859-1', 'windows-1252', 'macroman']:\n            try:\n                with open(file_path, 'r', encoding=encoding) as f:\n                    content = f.read()\n                break\n            except UnicodeDecodeError:\n                continue\n\n        if content is None:\n            print(f\"  ❌ Could not decode {file_path}\")\n            return False\n\n        # Fix common non-ASCII characters in code\n        fixes = {\n            '->': '->',  # Arrow to dash-greater\n            '**2': '**2',  # Superscript 2\n            '+/-': '+/-',  # Plus-minus\n            'tau': 'tau',  # Greek tau\n        }\n\n        original_content = content\n        for old, new in fixes.items():\n            content = content.replace(old, new)\n\n        # Remove trailing whitespace from each line\n        lines = content.splitlines()\n        lines = [line.rstrip() for line in lines]\n        content = '\\n'.join(lines)\n\n        # Ensure file ends with newline if it's not empty\n        if content and not content.endswith('\\n'):\n            content += '\\n'\n\n        # Write back as UTF-8\n        with open(file_path, 'w', encoding='utf-8', newline='\\n') as f:\n            f.write(content)\n\n        if content != original_content:\n            print(f\"  ✅ Fixed formatting and encoding\")\n        else:\n            print(f\"  ✅ Already correct\")\n\n        return True\n\n    except Exception as e:\n        print(f\"  ❌ Error fixing {file_path}: {e}\")\n        return False\n\ndef restore_empty_files():\n    \"\"\"Restore content to important empty files.\"\"\"\n    empty_files_content = {\n        'src/__init__.py': '\"\"\"Main package initialization.\"\"\"\\n',\n        'src/environment/__init__.py': '\"\"\"Environment module.\"\"\"\\n',\n        'src/models/__init__.py': '\"\"\"Models module.\"\"\"\\n',\n        'src/utils/__init__.py': '\"\"\"Utilities module.\"\"\"\\n',\n        'src/tests/__init__.py': '\"\"\"Tests module.\"\"\"\\n',\n        'adaptive_bayesian_driver/models/utils_particle.py': '''\"\"\"\nParticle filter utilities for adaptive Bayesian learning.\n\"\"\"\n\nimport numpy as np\nimport torch\nfrom typing import Dict, List, Tuple, Optional, Callable\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# This file was restored after being corrupted\n# Content needs to be reimplemented based on project needs\n''',\n        'adaptive_bayesian_driver/utils/visualization.py': '''\"\"\"\nVisualization utilities for adaptive Bayesian learning.\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom typing import Dict, List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# This file was restored after being corrupted\n# Content needs to be reimplemented based on project needs\n''',\n    }\n\n    for file_path, content in empty_files_content.items():\n        if os.path.exists(file_path):\n            try:\n                with open(file_path, 'r') as f:\n                    current = f.read().strip()\n                if not current:  # File is empty\n                    with open(file_path, 'w', encoding='utf-8') as f:\n                        f.write(content)\n                    print(f\"✅ Restored: {file_path}\")\n            except Exception as e:\n                print(f\"❌ Error restoring {file_path}: {e}\")\n\ndef main():\n    \"\"\"Main formatting fix function.\"\"\"\n    print(\"🔧 Fixing repository formatting issues...\\n\")\n\n    # First restore empty files\n    print(\"📝 Restoring empty files...\")\n    restore_empty_files()\n    print()\n\n    # Fix all Python and text files\n    patterns = ['*.py', '*.md', '*.txt', '*.yml', '*.yaml']\n    project_root = Path(\".\")\n\n    fixed_count = 0\n    total_count = 0\n\n    for pattern in patterns:\n        for file_path in project_root.rglob(pattern):\n            # Skip certain directories\n            if any(part in str(file_path) for part in ['.git', '__pycache__', '.pytest_cache', 'build', 'dist', '.egg-info']):\n                continue\n\n            total_count += 1\n            if fix_file_encoding_and_format(file_path):\n                fixed_count += 1\n\n    print(f\"\\n🎉 Fixed {fixed_count}/{total_count} files\")\n    print(\"\\n✅ Repository formatting cleanup complete!\")\n    print(\"\\nRecommendations:\")\n    print(\"1. Run 'git status' to see what changed\")\n    print(\"2. Test your code after these fixes\")\n    print(\"3. Run 'python -m flake8 . --count' to check for remaining issues\")\n\nif __name__ == \"__main__\":\n    main()\n",
      "directory": "_chatbot\\_fixing",
      "is_binary": false,
      "size_bytes": 4757,
      "filename": "fix_formatting.py"
    },
    {
      "last_modified": "2025-07-08 16:08:08",
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nChatbot file organization helper.\r\nUse this to ensure all chatbot-created files are properly organized.\r\n\"\"\"\r\n\r\nimport os\r\nfrom pathlib import Path\r\n\r\ndef organize_chatbot...",
      "extension": ".py",
      "size_kb": 1.79,
      "path": "_chatbot/_fixing/organize.py",
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nChatbot file organization helper.\r\nUse this to ensure all chatbot-created files are properly organized.\r\n\"\"\"\r\n\r\nimport os\r\nfrom pathlib import Path\r\n\r\ndef organize_chatbot_files():\r\n    \"\"\"Organize chatbot-created files into proper directories.\"\"\"\r\n\r\n    # Define the organizational structure\r\n    structure = {\r\n        '_chatbot/_diagnostics': [\r\n            'format_scanner.py',\r\n            'cicd_check.py',\r\n            'diagnostic.py',\r\n            'validate_build.py',\r\n            'ci_test.py',\r\n            'final_validation.py',\r\n            '*_check.py',\r\n            '*_scanner.py',\r\n            '*_diagnostic.py'\r\n        ],\r\n        '_chatbot/_fixing': [\r\n            'fix_formatting.py',\r\n            'clean_whitespace.py',\r\n            '*_fix.py',\r\n            '*_cleanup.py'\r\n        ]\r\n    }\r\n\r\n    project_root = Path('.')\r\n\r\n    # Create directories if they don't exist\r\n    for dir_path in structure.keys():\r\n        os.makedirs(dir_path, exist_ok=True)\r\n        print(f\"✓ Directory ready: {dir_path}\")\r\n\r\n    print(\"\\n📁 Chatbot file organization structure ready!\")\r\n    print(\"\\nFor future AI assistants:\")\r\n    print(\"- Place diagnostic tools in: _chatbot/_diagnostics/\")\r\n    print(\"- Place fixing tools in: _chatbot/_fixing/\")\r\n    print(\"- Never create scripts in project root\")\r\n    print(\"- Always use descriptive names\")\r\n\r\ndef get_chatbot_dir(script_type):\r\n    \"\"\"Get the appropriate directory for a chatbot script type.\"\"\"\r\n    mapping = {\r\n        'diagnostic': '_chatbot/_diagnostics',\r\n        'fix': '_chatbot/_fixing',\r\n        'check': '_chatbot/_diagnostics',\r\n        'scan': '_chatbot/_diagnostics',\r\n        'cleanup': '_chatbot/_fixing'\r\n    }\r\n    return mapping.get(script_type, '_chatbot')\r\n\r\nif __name__ == \"__main__\":\r\n    organize_chatbot_files()\r\n",
      "directory": "_chatbot\\_fixing",
      "is_binary": false,
      "size_bytes": 1831,
      "filename": "organize.py"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"Comprehensive test runner to identify failing tests.\"\"\"\n\nimport sys\nimport os\nimport subprocess\nimport traceback\n\n# Add the project root to the Python path\nsys.path.insert(0,...",
      "extension": ".py",
      "size_kb": 2.18,
      "path": "_chatbot/_testing/comprehensive_test.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Comprehensive test runner to identify failing tests.\"\"\"\n\nimport sys\nimport os\nimport subprocess\nimport traceback\n\n# Add the project root to the Python path\nsys.path.insert(0, os.path.abspath('.'))\n\ndef run_test(test_name):\n    \"\"\"Run a single test and capture output.\"\"\"\n    try:\n        result = subprocess.run(\n            [sys.executable, '-m', 'pytest', test_name, '-v'],\n            capture_output=True,\n            text=True,\n            cwd=os.path.abspath('.')\n        )\n\n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr\n        }\n    except Exception as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }\n\ndef main():\n    \"\"\"Run all the failing tests mentioned.\"\"\"\n    failing_tests = [\n        \"tests/test_environment.py::TestSceneRenderer::test_render_scene\",\n        \"tests/test_recursive_bayes.py::TestSurpriseMeter::test_baseline_update\",\n        \"tests/test_recursive_bayes.py::TestSurpriseMeter::test_chi2_surprise\",\n        \"tests/test_recursive_bayes.py::TestSurpriseMeter::test_normalized_surprise\",\n        \"tests/test_recursive_bayes.py::TestSurpriseMeter::test_reconstruction_surprise\",\n    ]\n\n    results = {}\n\n    for test in failing_tests:\n        print(f\"\\n{'='*60}\")\n        print(f\"Running test: {test}\")\n        print('='*60)\n\n        result = run_test(test)\n        results[test] = result\n\n        if result['returncode'] == 0:\n            print(\"✅ PASSED\")\n        else:\n            print(\"❌ FAILED\")\n            print(f\"Return code: {result['returncode']}\")\n            print(f\"STDOUT:\\n{result['stdout']}\")\n            print(f\"STDERR:\\n{result['stderr']}\")\n\n    # Summary\n    print(f\"\\n{'='*60}\")\n    print(\"SUMMARY\")\n    print('='*60)\n\n    passed = 0\n    failed = 0\n\n    for test, result in results.items():\n        status = \"✅ PASSED\" if result['returncode'] == 0 else \"❌ FAILED\"\n        print(f\"{test}: {status}\")\n        if result['returncode'] == 0:\n            passed += 1\n        else:\n            failed += 1\n\n    print(f\"\\nTotal: {passed} passed, {failed} failed\")\n\nif __name__ == \"__main__\":\n    main()\n",
      "directory": "_chatbot\\_testing",
      "is_binary": false,
      "size_bytes": 2230,
      "filename": "comprehensive_test.py"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"Run the specific failing tests directly.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nimport unittest\nfrom tests.test_environment import TestSceneRender...",
      "extension": ".py",
      "size_kb": 1.79,
      "path": "_chatbot/_testing/run_specific_tests.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Run the specific failing tests directly.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nimport unittest\nfrom tests.test_environment import TestSceneRenderer\nfrom tests.test_recursive_bayes import TestSurpriseMeter\n\ndef main():\n    # Test SceneRenderer\n    print(\"=\"*50)\n    print(\"Testing SceneRenderer\")\n    print(\"=\"*50)\n\n    try:\n        suite = unittest.TestLoader().loadTestsFromTestCase(TestSceneRenderer)\n        runner = unittest.TextTestRunner(verbosity=2)\n        result = runner.run(suite)\n\n        if result.failures:\n            print(\"\\nFAILURES:\")\n            for test, traceback in result.failures:\n                print(f\"Test: {test}\")\n                print(f\"Traceback: {traceback}\")\n\n        if result.errors:\n            print(\"\\nERRORS:\")\n            for test, traceback in result.errors:\n                print(f\"Test: {test}\")\n                print(f\"Traceback: {traceback}\")\n\n    except Exception as e:\n        print(f\"Error running SceneRenderer tests: {e}\")\n\n    # Test SurpriseMeter\n    print(\"\\n\" + \"=\"*50)\n    print(\"Testing SurpriseMeter\")\n    print(\"=\"*50)\n\n    try:\n        suite = unittest.TestLoader().loadTestsFromTestCase(TestSurpriseMeter)\n        runner = unittest.TextTestRunner(verbosity=2)\n        result = runner.run(suite)\n\n        if result.failures:\n            print(\"\\nFAILURES:\")\n            for test, traceback in result.failures:\n                print(f\"Test: {test}\")\n                print(f\"Traceback: {traceback}\")\n\n        if result.errors:\n            print(\"\\nERRORS:\")\n            for test, traceback in result.errors:\n                print(f\"Test: {test}\")\n                print(f\"Traceback: {traceback}\")\n\n    except Exception as e:\n        print(f\"Error running SurpriseMeter tests: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
      "directory": "_chatbot\\_testing",
      "is_binary": false,
      "size_bytes": 1834,
      "filename": "run_specific_tests.py"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"Test AdaptiveParticleFilter reset functionality.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nfrom adaptive_bayesian_driver.models import AdaptivePartic...",
      "extension": ".py",
      "size_kb": 1.97,
      "path": "_chatbot/_testing/test_particle_filter.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Test AdaptiveParticleFilter reset functionality.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nfrom adaptive_bayesian_driver.models import AdaptiveParticleFilter\nimport numpy as np\n\ndef test_particle_filter_reset():\n    \"\"\"Test that AdaptiveParticleFilter reset works without IndexError.\"\"\"\n    print(\"Testing AdaptiveParticleFilter reset...\")\n\n    # Create config\n    config = {\n        'n_particles': 50,\n        'state_dim': 3,\n        'observation_dim': 2,\n        'process_noise_std': 0.1,\n        'observation_noise_std': 0.1,\n        'adaptive_noise': True,\n        'adaptive_resampling': True\n    }\n\n    # Create particle filter\n    pf = AdaptiveParticleFilter(config)\n\n    print(f\"Initial particles shape: {pf.particles.shape}\")\n    print(f\"Initial weights shape: {pf.weights.shape}\")\n\n    # Run some updates to trigger potential resampling\n    for i in range(5):\n        observation = np.random.randn(2)\n        print(f\"Update {i+1}: observation = {observation}\")\n        try:\n            result = pf.update(observation)\n            print(f\"  Update successful, ESS: {result['effective_sample_size']:.2f}\")\n        except Exception as e:\n            print(f\"  Update failed: {e}\")\n            return False\n\n    # Test reset\n    print(\"\\nTesting reset...\")\n    try:\n        pf.reset()\n        print(\"Reset successful!\")\n\n        # Verify reset worked\n        assert len(pf.state_history) == 0, \"State history should be empty after reset\"\n        assert abs(np.sum(pf.weights) - 1.0) < 1e-6, \"Weights should sum to 1.0\"\n        assert pf.particles.shape == (50, 3), \"Particles shape should be preserved\"\n\n        print(\"All reset checks passed!\")\n        return True\n\n    except Exception as e:\n        print(f\"Reset failed: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    success = test_particle_filter_reset()\n    if success:\n        print(\"\\n✅ All tests passed!\")\n    else:\n        print(\"\\n❌ Tests failed!\")\n    sys.exit(0 if success else 1)\n",
      "directory": "_chatbot\\_testing",
      "is_binary": false,
      "size_bytes": 2016,
      "filename": "test_particle_filter.py"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "#!/usr/bin/env python3\n\"\"\"Test SceneRenderer directly to understand the issue.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nfrom adaptive_bayesian_driver.environment import Scene...",
      "extension": ".py",
      "size_kb": 1.04,
      "path": "_chatbot/debugging/test_scene_debug.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Test SceneRenderer directly to understand the issue.\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('.'))\n\nfrom adaptive_bayesian_driver.environment import SceneRenderer\n\n# Test configuration\nconfig = {\n    'visual_parameters': {\n        'scene_size': [64, 64],\n        'safe_background_color': [50, 50, 50],\n        'danger_background_color': [255, 215, 0]\n    }\n}\n\nrenderer = SceneRenderer(config)\n\n# Test parameters\nparams = {\n    'has_danger': True,\n    'orientation_angle': 45.0,\n    'difficulty': 0.2,\n    'correct_direction': 'right'\n}\n\nscene, metadata = renderer.render_scene(params)\n\nprint(\"Test Results:\")\nprint(f\"Scene shape: {scene.shape}\")\nprint(f\"Scene type: {type(scene)}\")\nprint(f\"Expected shape: (1, 64, 64, 3)\")\nprint(f\"Assertion would pass: {scene.shape == (1, 64, 64, 3)}\")\nprint(f\"Metadata: {metadata}\")\n\n# Test if it's actually (1, 64, 64) without the 3\nprint(f\"Scene shape equals (1, 64, 64): {scene.shape == (1, 64, 64)}\")\nprint(f\"Scene shape equals (1, 64, 64, 3): {scene.shape == (1, 64, 64, 3)}\")\n",
      "directory": "_chatbot\\debugging",
      "is_binary": false,
      "size_bytes": 1063,
      "filename": "test_scene_debug.py"
    },
    {
      "last_modified": "2025-07-08 16:22:41",
      "content_preview": "# Quick Reference for AI Assistants\r\n\r\n## 🚀 Essential Commands\r\n\r\n### Project Setup\r\n```bash\r\n# Install dependencies\r\npip install -r requirements.txt\r\n\r\n# Install in development mode\r\npip install -e ...",
      "extension": ".md",
      "size_kb": 5.36,
      "path": "_chatbot/QUICK_REFERENCE.md",
      "content": "# Quick Reference for AI Assistants\r\n\r\n## 🚀 Essential Commands\r\n\r\n### Project Setup\r\n```bash\r\n# Install dependencies\r\npip install -r requirements.txt\r\n\r\n# Install in development mode\r\npip install -e .\r\n\r\n# Verify installation\r\npython -c \"import adaptive_bayesian_driver; print('✓ Package ready')\"\r\n```\r\n\r\n### Testing & Validation\r\n```bash\r\n# Run all tests\r\npython -m pytest tests/ -v\r\n\r\n# Run specific test file\r\npython -m pytest tests/test_environment.py -v\r\n\r\n# Check formatting issues\r\npython _chatbot/_diagnostics/format_scanner.py\r\n\r\n# Quick CI/CD check\r\npython _chatbot/_diagnostics/cicd_check.py\r\n\r\n# Comprehensive validation\r\npython _chatbot/_diagnostics/validate_build.py\r\n```\r\n\r\n### Fixing Common Issues\r\n```bash\r\n# Fix formatting problems\r\npython _chatbot/_fixing/fix_formatting.py\r\n\r\n# Clean whitespace\r\npython _chatbot/_fixing/clean_whitespace.py\r\n\r\n# Check for linting errors\r\npython -m flake8 adaptive_bayesian_driver/ --count\r\n```\r\n\r\n### Git Workflow\r\n```bash\r\n# Check status\r\ngit status\r\n\r\n# Stage changes\r\ngit add .\r\n\r\n# Commit with message\r\ngit commit -m \"Description of changes\"\r\n\r\n# Push to remote\r\ngit push origin branch-name\r\n```\r\n\r\n## 📁 Directory Structure\r\n\r\n```\r\n├── adaptive_bayesian_driver/    # Main package\r\n│   ├── models/                  # Learning algorithms\r\n│   ├── environment/             # Simulation components\r\n│   ├── utils/                   # Utilities\r\n│   └── applications/            # Domain applications\r\n├── tests/                       # Test suites\r\n├── config/                      # Configuration files\r\n├── notebooks/                   # Jupyter notebooks\r\n├── _chatbot/                    # AI assistant tools\r\n│   ├── _diagnostics/           # Diagnostic scripts\r\n│   ├── _fixing/                # Repair utilities\r\n│   └── _testing/               # Test helpers\r\n├── _reports/                    # Generated reports\r\n└── _assistant/                  # Session guidelines\r\n```\r\n\r\n## 🔧 Common File Operations\r\n\r\n### Creating Helper Scripts\r\n```bash\r\n# Diagnostic tools go here:\r\n_chatbot/_diagnostics/my_scanner.py\r\n\r\n# Fixing tools go here:\r\n_chatbot/_fixing/my_fix.py\r\n\r\n# Never create scripts in project root!\r\n```\r\n\r\n### Importing the Package\r\n```python\r\n# Main package\r\nimport adaptive_bayesian_driver\r\n\r\n# Specific components\r\nfrom adaptive_bayesian_driver.models import RecursiveBayesianLearner\r\nfrom adaptive_bayesian_driver.environment import SceneRenderer\r\nfrom adaptive_bayesian_driver.config import load_config\r\n```\r\n\r\n### Running Experiments\r\n```python\r\n# Load configuration\r\nfrom adaptive_bayesian_driver.config import load_config\r\nconfig = load_config('config/experiment.yaml')\r\n\r\n# Initialize components\r\nfrom adaptive_bayesian_driver.models import RecursiveBayesianLearner\r\nlearner = RecursiveBayesianLearner(config)\r\n\r\n# Run experiment\r\nresults = learner.train(data)\r\n```\r\n\r\n## 🐛 Troubleshooting\r\n\r\n### Import Errors\r\n```bash\r\n# Check package structure\r\npython -c \"import adaptive_bayesian_driver; print('OK')\"\r\n\r\n# Reinstall package\r\npip install -e .\r\n```\r\n\r\n### Test Failures\r\n```bash\r\n# Run with verbose output\r\npython -m pytest tests/ -v -s\r\n\r\n# Run specific test\r\npython -m pytest tests/test_specific.py::TestClass::test_method -v\r\n```\r\n\r\n### Formatting Issues\r\n```bash\r\n# Scan for problems\r\npython _chatbot/_diagnostics/format_scanner.py\r\n\r\n# Auto-fix most issues\r\npython _chatbot/_fixing/fix_formatting.py\r\n```\r\n\r\n### CI/CD Failures\r\n```bash\r\n# Quick health check\r\npython _chatbot/_diagnostics/cicd_check.py\r\n\r\n# Check linting\r\npython -m flake8 adaptive_bayesian_driver/ --count\r\n\r\n# Verify package builds\r\npython setup.py check\r\n```\r\n\r\n## 📝 Code Snippets\r\n\r\n### Basic Test Template\r\n```python\r\nimport unittest\r\nimport numpy as np\r\nfrom adaptive_bayesian_driver.models import YourModel\r\n\r\nclass TestYourModel(unittest.TestCase):\r\n    def setUp(self):\r\n        self.model = YourModel()\r\n\r\n    def test_basic_functionality(self):\r\n        result = self.model.process(test_data)\r\n        self.assertIsNotNone(result)\r\n```\r\n\r\n### Configuration Loading\r\n```python\r\nfrom adaptive_bayesian_driver.config import load_config\r\n\r\n# Load default config\r\nconfig = load_config()\r\n\r\n# Load specific config\r\nconfig = load_config('config/custom.yaml')\r\n```\r\n\r\n### Error Handling Template\r\n```python\r\ntry:\r\n    # Your code here\r\n    result = risky_operation()\r\nexcept Exception as e:\r\n    logger.error(f\"Operation failed: {e}\")\r\n    # Handle gracefully\r\n```\r\n\r\n## ⚡ Performance Tips\r\n\r\n### Memory Management\r\n```python\r\n# Use generators for large datasets\r\ndef process_data_generator(data):\r\n    for item in data:\r\n        yield process_item(item)\r\n\r\n# Clear variables when done\r\ndel large_variable\r\n```\r\n\r\n### Debugging\r\n```python\r\nimport logging\r\nlogging.basicConfig(level=logging.DEBUG)\r\n\r\n# Add debug prints\r\nlogger.debug(f\"Variable value: {variable}\")\r\n```\r\n\r\n## 🔍 Useful Commands\r\n\r\n### System Information\r\n```bash\r\n# Python version\r\npython --version\r\n\r\n# Package versions\r\npip list | grep torch\r\npip list | grep numpy\r\n\r\n# Environment info\r\npython -c \"import sys; print(sys.path)\"\r\n```\r\n\r\n### File Operations\r\n```bash\r\n# Find files\r\nfind . -name \"*.py\" -type f\r\n\r\n# Search in files\r\ngrep -r \"pattern\" adaptive_bayesian_driver/\r\n\r\n# Count lines of code\r\nfind . -name \"*.py\" -exec wc -l {} +\r\n```\r\n\r\n---\r\n\r\n*Keep this reference handy for quick access to common operations and troubleshooting steps.*\r\n",
      "directory": "_chatbot",
      "is_binary": false,
      "size_bytes": 5489,
      "filename": "QUICK_REFERENCE.md"
    },
    {
      "last_modified": "2025-07-08 16:08:08",
      "content_preview": "# Chatbot Assistant Tools\r\n\r\nThis directory contains helper scripts and tools created by AI assistants to support development and debugging.\r\n\r\n## Directory Structure\r\n\r\n### `_diagnostics/`\r\nScripts f...",
      "extension": ".md",
      "size_kb": 1.51,
      "path": "_chatbot/README.md",
      "content": "# Chatbot Assistant Tools\r\n\r\nThis directory contains helper scripts and tools created by AI assistants to support development and debugging.\r\n\r\n## Directory Structure\r\n\r\n### `_diagnostics/`\r\nScripts for diagnosing issues and checking system health:\r\n- `format_scanner.py` - Scans repository for formatting issues\r\n- `cicd_check.py` - Quick CI/CD readiness verification\r\n- `validate_build.py` - Comprehensive build validation\r\n- Other diagnostic utilities\r\n\r\n### `_fixing/`\r\nScripts for automatically fixing common issues:\r\n- `fix_formatting.py` - Comprehensive formatting repair\r\n- `clean_whitespace.py` - Whitespace cleanup utility\r\n- Other automated fix tools\r\n\r\n## Usage Guidelines\r\n\r\n- **For Developers**: These tools can be run manually for debugging\r\n- **For AI Assistants**: Use these directories for organizing helper scripts\r\n- **CI/CD**: These scripts are excluded from CI/CD processes\r\n\r\n## Best Practices\r\n\r\n1. **Organization**: Always place chatbot-created utilities in appropriate subdirectories\r\n2. **Naming**: Use descriptive names that indicate the tool's purpose\r\n3. **Documentation**: Include brief comments explaining what each script does\r\n4. **Cleanup**: Periodically review and remove obsolete scripts\r\n\r\n## File Naming Convention\r\n\r\n- `*_scanner.py` - Analysis and detection tools\r\n- `*_check.py` - Quick verification scripts\r\n- `*_fix.py` - Automated repair utilities\r\n- `*_diagnostic.py` - Health check tools\r\n\r\n---\r\n*This directory structure helps keep the main repository clean while providing useful development tools.*\r\n",
      "directory": "_chatbot",
      "is_binary": false,
      "size_bytes": 1550,
      "filename": "README.md"
    },
    {
      "last_modified": "2025-07-12 08:01:19",
      "content_preview": "",
      "extension": ".json",
      "size_kb": 0.0,
      "path": "_chatbot/repo_export.json",
      "content": "",
      "directory": "_chatbot",
      "is_binary": false,
      "size_bytes": 0,
      "filename": "repo_export.json"
    },
    {
      "last_modified": "2025-07-08 16:22:41",
      "content_preview": "# Repository Guidelines for AI Assistants\r\n\r\n## Overview\r\nThis document provides guidelines for AI assistants working on the surprise-learning-exploration-exploitation project. These guidelines ensure...",
      "extension": ".md",
      "size_kb": 5.22,
      "path": "_chatbot/REPO_GUIDELINES.md",
      "content": "# Repository Guidelines for AI Assistants\r\n\r\n## Overview\r\nThis document provides guidelines for AI assistants working on the surprise-learning-exploration-exploitation project. These guidelines ensure consistent, professional, and maintainable development practices.\r\n\r\n## File Organization\r\n\r\n### 🎯 Core Principle: Keep Project Root Clean\r\n- **NEVER** create helper scripts in the project root\r\n- **ALWAYS** use organized subdirectories under `_chatbot/`\r\n\r\n### Directory Structure\r\n```\r\n_chatbot/\r\n├── _diagnostics/     # Scanning, checking, validation tools\r\n├── _fixing/          # Automated repair and cleanup tools\r\n├── _testing/         # Test utilities and debugging tools\r\n├── debugging/        # Debug helpers and analysis tools\r\n├── validation/       # CI/CD and quality assurance tools\r\n├── reporting/        # Report generation and documentation tools\r\n└── README.md         # This file\r\n```\r\n\r\n## Naming Conventions\r\n\r\n### File Naming Patterns\r\n- `*_scanner.py` - Analysis and detection tools\r\n- `*_check.py` - Quick verification scripts\r\n- `*_fix.py` - Automated repair utilities\r\n- `*_diagnostic.py` - Health check tools\r\n- `*_validator.py` - Validation utilities\r\n- `*_reporter.py` - Report generation tools\r\n\r\n### Code Quality Standards\r\n- **Always** include proper docstrings\r\n- **Always** handle errors gracefully\r\n- **Always** use type hints where appropriate\r\n- **Always** follow PEP 8 style guidelines\r\n\r\n## Development Workflow\r\n\r\n### Before Making Changes\r\n1. **Scan** for existing issues: `python _chatbot/_diagnostics/format_scanner.py`\r\n2. **Check** CI/CD readiness: `python _chatbot/_diagnostics/cicd_check.py`\r\n3. **Validate** current state: `python _chatbot/_diagnostics/validate_build.py`\r\n\r\n### After Making Changes\r\n1. **Fix** formatting issues: `python _chatbot/_fixing/fix_formatting.py`\r\n2. **Clean** whitespace: `python _chatbot/_fixing/clean_whitespace.py`\r\n3. **Test** the changes: `python -m pytest tests/`\r\n4. **Verify** CI/CD: `python _chatbot/_diagnostics/cicd_check.py`\r\n\r\n### Git Workflow\r\n1. **Stage** changes: `git add .`\r\n2. **Commit** with descriptive message: `git commit -m \"Brief description of changes\"`\r\n3. **Update** changelog if significant changes were made\r\n\r\n## Package Structure Guidelines\r\n\r\n### Main Package: `adaptive_bayesian_driver/`\r\n- Core learning algorithms and models\r\n- Well-tested, production-ready code\r\n- Comprehensive docstrings and type hints\r\n\r\n### Testing: `tests/`\r\n- Unit tests for all major functionality\r\n- Integration tests for key workflows\r\n- Clear test names describing what is tested\r\n\r\n### Configuration: `config/`\r\n- YAML configuration files\r\n- Environment-specific settings\r\n- Experiment parameters\r\n\r\n## Documentation Standards\r\n\r\n### README Updates\r\n- Keep installation instructions current\r\n- Update usage examples when APIs change\r\n- Maintain clear project description\r\n\r\n### Changelog Maintenance\r\n- Document all significant changes\r\n- Use semantic versioning\r\n- Include migration notes for breaking changes\r\n\r\n### Code Documentation\r\n- Include docstrings for all public functions/classes\r\n- Add inline comments for complex logic\r\n- Maintain API documentation\r\n\r\n## CI/CD Considerations\r\n\r\n### Critical Requirements\r\n- All tests must pass: `python -m pytest`\r\n- No linting errors: `python -m flake8 adaptive_bayesian_driver/`\r\n- Package imports correctly: `python -c \"import adaptive_bayesian_driver\"`\r\n- No formatting issues: Check with format scanner\r\n\r\n### Common Issues to Avoid\r\n- ❌ Non-ASCII characters in source code\r\n- ❌ Trailing whitespace\r\n- ❌ Mixed line endings\r\n- ❌ Import errors\r\n- ❌ Broken package structure\r\n\r\n## Emergency Procedures\r\n\r\n### If CI/CD Fails\r\n1. Run: `python _chatbot/_diagnostics/format_scanner.py`\r\n2. Fix issues: `python _chatbot/_fixing/fix_formatting.py`\r\n3. Verify: `python _chatbot/_diagnostics/cicd_check.py`\r\n\r\n### If Package Import Fails\r\n1. Check `setup.py` configuration\r\n2. Verify directory structure matches package layout\r\n3. Ensure all `__init__.py` files exist\r\n\r\n### If Tests Fail\r\n1. Run specific test: `python -m pytest tests/test_specific.py -v`\r\n2. Check for missing dependencies\r\n3. Verify test data and fixtures\r\n\r\n## Best Practices\r\n\r\n### Code Quality\r\n- Write self-documenting code\r\n- Use meaningful variable names\r\n- Keep functions focused and small\r\n- Handle edge cases gracefully\r\n\r\n### Performance\r\n- Profile before optimizing\r\n- Use appropriate data structures\r\n- Cache expensive computations when beneficial\r\n- Consider memory usage for large datasets\r\n\r\n### Security\r\n- Never commit sensitive information\r\n- Use environment variables for configuration\r\n- Validate all inputs\r\n- Follow secure coding practices\r\n\r\n## Tools and Utilities\r\n\r\n### Available Diagnostic Tools\r\n- `format_scanner.py` - Repository health check\r\n- `cicd_check.py` - CI/CD readiness verification\r\n- `validate_build.py` - Comprehensive build validation\r\n\r\n### Available Fixing Tools\r\n- `fix_formatting.py` - Automated formatting repair\r\n- `clean_whitespace.py` - Whitespace cleanup\r\n\r\n### Integration with IDE\r\n- Use VS Code with Python extension\r\n- Configure linting and formatting\r\n- Set up debugging configurations\r\n\r\n---\r\n\r\n*These guidelines help maintain code quality and ensure smooth development workflows. Update this document as the project evolves.*\r\n",
      "directory": "_chatbot",
      "is_binary": false,
      "size_bytes": 5345,
      "filename": "REPO_GUIDELINES.md"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "",
      "extension": ".md",
      "size_kb": 0.0,
      "path": "_chatbot/reporting/CONTEXT_SHARING.md",
      "content": "",
      "directory": "_chatbot\\reporting",
      "is_binary": false,
      "size_bytes": 0,
      "filename": "CONTEXT_SHARING.md"
    },
    {
      "last_modified": "2025-07-08 15:07:19",
      "content_preview": "",
      "extension": ".ps1",
      "size_kb": 0.0,
      "path": "_chatbot/reporting/generate-report.ps1",
      "content": "",
      "directory": "_chatbot\\reporting",
      "is_binary": false,
      "size_bytes": 0,
      "filename": "generate-report.ps1"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "",
      "extension": ".md",
      "size_kb": 0.0,
      "path": "_chatbot/reporting/merge_reports/REFACTORING_REPORT_iteration_20250707_20250708_120000.md",
      "content": "",
      "directory": "_chatbot\\reporting\\merge_reports",
      "is_binary": false,
      "size_bytes": 0,
      "filename": "REFACTORING_REPORT_iteration_20250707_20250708_120000.md"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "",
      "extension": ".md",
      "size_kb": 0.0,
      "path": "_chatbot/reporting/QUICK_COMMANDS.md",
      "content": "",
      "directory": "_chatbot\\reporting",
      "is_binary": false,
      "size_bytes": 0,
      "filename": "QUICK_COMMANDS.md"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "",
      "extension": ".md",
      "size_kb": 0.0,
      "path": "_chatbot/reporting/README.md",
      "content": "",
      "directory": "_chatbot\\reporting",
      "is_binary": false,
      "size_bytes": 0,
      "filename": "README.md"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "",
      "extension": ".md",
      "size_kb": 0.0,
      "path": "_chatbot/reporting/templates/REFACTORING_REPORT_TEMPLATE.md",
      "content": "",
      "directory": "_chatbot\\reporting\\templates",
      "is_binary": false,
      "size_bytes": 0,
      "filename": "REFACTORING_REPORT_TEMPLATE.md"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "",
      "extension": ".md",
      "size_kb": 0.0,
      "path": "_chatbot/SESSION_GUIDELINES.md",
      "content": "",
      "directory": "_chatbot",
      "is_binary": false,
      "size_bytes": 0,
      "filename": "SESSION_GUIDELINES.md"
    },
    {
      "last_modified": "2025-07-08 15:07:19",
      "content_preview": "#!/usr/bin/env python3\r\n\"\"\"\r\nQuick validation script to test if the package builds and imports correctly.\r\nThis helps identify CI/CD issues locally.\r\n\"\"\"\r\n\r\nimport sys\r\nimport traceback\r\n\r\ndef test_im...",
      "extension": ".py",
      "size_kb": 2.39,
      "path": "_chatbot/validation/validate_build.py",
      "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nQuick validation script to test if the package builds and imports correctly.\r\nThis helps identify CI/CD issues locally.\r\n\"\"\"\r\n\r\nimport sys\r\nimport traceback\r\n\r\ndef test_imports():\r\n    \"\"\"Test that all main modules can be imported.\"\"\"\r\n    print(\"Testing imports...\")\r\n\r\n    try:\r\n        import adaptive_bayesian_driver\r\n        print(\"✓ Main package import successful\")\r\n    except Exception as e:\r\n        print(f\"✗ Main package import failed: {e}\")\r\n        return False\r\n\r\n    try:\r\n        from adaptive_bayesian_driver.models import RecursiveBayesianLearner, SurpriseMeter\r\n        print(\"✓ Core models import successful\")\r\n    except Exception as e:\r\n        print(f\"✗ Core models import failed: {e}\")\r\n        return False\r\n\r\n    try:\r\n        from adaptive_bayesian_driver.environment import SceneRenderer, VolatilityController\r\n        print(\"✓ Environment modules import successful\")\r\n    except Exception as e:\r\n        print(f\"✗ Environment modules import failed: {e}\")\r\n        return False\r\n\r\n    try:\r\n        from adaptive_bayesian_driver.config import load_config\r\n        print(\"✓ Config module import successful\")\r\n    except Exception as e:\r\n        print(f\"✗ Config module import failed: {e}\")\r\n        return False\r\n\r\n    return True\r\n\r\ndef test_basic_functionality():\r\n    \"\"\"Test basic functionality without full computation.\"\"\"\r\n    print(\"\\nTesting basic functionality...\")\r\n\r\n    try:\r\n        from adaptive_bayesian_driver.models import SurpriseMeter\r\n        from adaptive_bayesian_driver.models.surprise import SurpriseType\r\n\r\n        # Test SurpriseMeter instantiation\r\n        meter = SurpriseMeter(mode=SurpriseType.EPISTEMIC)\r\n        print(\"✓ SurpriseMeter instantiation successful\")\r\n\r\n        return True\r\n    except Exception as e:\r\n        print(f\"✗ Basic functionality test failed: {e}\")\r\n        traceback.print_exc()\r\n        return False\r\n\r\ndef main():\r\n    \"\"\"Main validation function.\"\"\"\r\n    print(\"=== Build Validation ===\")\r\n\r\n    success = True\r\n\r\n    # Test imports\r\n    if not test_imports():\r\n        success = False\r\n\r\n    # Test basic functionality\r\n    if not test_basic_functionality():\r\n        success = False\r\n\r\n    if success:\r\n        print(\"\\n✅ All validation tests passed!\")\r\n        return 0\r\n    else:\r\n        print(\"\\n❌ Some validation tests failed!\")\r\n        return 1\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n",
      "directory": "_chatbot\\validation",
      "is_binary": false,
      "size_bytes": 2445,
      "filename": "validate_build.py"
    },
    {
      "last_modified": "2025-07-11 12:31:17",
      "content_preview": "# Helm.ai Portable Development Environment Variables\r\nPYTHONPATH=${workspaceFolder}\r\nPORTABLE_DEV_ROOT=${env:OneDrive}/Onedrive-PortableDenv\r\nPROJECTS_ROOT=${env:OneDrive}/Onedrive-PortableDenv/Projec...",
      "extension": ".env",
      "size_kb": 1.02,
      "path": ".env",
      "content": "# Helm.ai Portable Development Environment Variables\r\nPYTHONPATH=${workspaceFolder}\r\nPORTABLE_DEV_ROOT=${env:OneDrive}/Onedrive-PortableDenv\r\nPROJECTS_ROOT=${env:OneDrive}/Onedrive-PortableDenv/Projects\r\nUSER_APPS=${env:OneDrive}/Onedrive-PortableDenv/UserApps\r\nUSER_APP_DATA=${env:OneDrive}/Onedrive-PortableDenv/UserAppData\r\nPROJECT_ROOT=${workspaceFolder}\r\n\r\n# Development Tools\r\nPYTHON_PORTABLE=${env:PYTHON_PORTABLE}\r\nCONDA_ROOT=${env:CONDA_ROOT}\r\nGIT_PORTABLE=${env:GIT_PORTABLE}\r\n\r\n# Git Configuration\r\nGIT_EDITOR=code --wait\r\nGIT_PAGER=cat\r\n\r\n# Development Settings\r\nDEBUG=1\r\nENVIRONMENT=development\r\nLOG_LEVEL=INFO\r\n\r\n# Helm.ai Project Settings\r\nHELM_PROJECT_NAME=surprise-learning-exploration-exploitation-wip\r\nHELM_EXPERIMENT_TRACKING=wandb\r\n\r\n# Data Paths\r\nDATA_DIR=${workspaceFolder}/data\r\nMODELS_DIR=${workspaceFolder}/models\r\nEXPERIMENTS_DIR=${workspaceFolder}/experiments\r\nLOGS_DIR=${workspaceFolder}/logs\r\n\r\n# ML/AI Framework Settings\r\nCUDA_VISIBLE_DEVICES=0\r\nTF_CPP_MIN_LOG_LEVEL=2\r\nPYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\r\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 1048,
      "filename": ".env"
    },
    {
      "last_modified": "2025-07-08 16:19:52",
      "content_preview": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n...",
      "extension": ".gitignore",
      "size_kb": 2.23,
      "path": ".gitignore",
      "content": "# Byte-compiled / optimized / DLL files\r\n__pycache__/\r\n*.py[cod]\r\n*$py.class\r\n\r\n# C extensions\r\n*.so\r\n\r\n# Distribution / packaging\r\nMANIFEST\r\n.Python\r\nbuild/\r\ndevelop-eggs/\r\ndist/\r\ndownloads/\r\neggs/\r\n.eggs/\r\nlib/\r\nlib64/\r\nparts/\r\nsdist/\r\nvar/\r\nwheels/\r\npip-wheel-metadata/\r\nshare/python-wheels/\r\n.installed.cfg\r\n*.egg-info/\r\n*.egg\r\n\r\n# PyInstaller\r\n#  Usually these files are written by a python script from a template\r\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\r\n*.manifest\r\n*.spec\r\n\r\n# Installer logs\r\npip-log.txt\r\npip-delete-this-directory.txt\r\n\r\n# Unit test / coverage reports\r\nhtmlcov/\r\n.tox/\r\n.nox/\r\n.coverage\r\n.coverage.*\r\n.cache\r\nnosetests.xml\r\ncoverage.xml\r\n*.cover\r\n*.py,cover\r\n.hypothesis/\r\n.pytest_cache/\r\n\r\n# Translations\r\n*.mo\r\n*.pot\r\n\r\n# Django stuff:\r\n*.log\r\nlocal_settings.py\r\ndb.sqlite3\r\ndb.sqlite3-journal\r\n\r\n# Flask stuff:\r\ninstance/\r\n.webassets-cache\r\n\r\n# Scrapy stuff:\r\n.scrapy\r\n\r\n# Sphinx documentation\r\ndocs/_build/\r\n\r\n# PyBuilder\r\ntarget/\r\n\r\n# Jupyter Notebook\r\n.ipynb_checkpoints\r\n\r\n# IPython\r\nprofile_default/\r\nipython_config.py\r\n\r\n# pyenv\r\n.python-version\r\n\r\n# pipenv\r\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\r\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\r\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\r\n#   install all needed dependencies.\r\n#Pipfile.lock\r\n\r\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\r\n__pypackages__/\r\n\r\n# Celery stuff\r\ncelerybeat-schedule\r\ncelerybeat.pid\r\n\r\n# SageMath parsed files\r\n*.sage.py\r\n\r\n# Environments\r\n.env\r\n.venv\r\nenv/\r\nvenv/\r\nENV/\r\nenv.bak/\r\nvenv.bak/\r\n\r\n# Spyder project settings\r\n.spyderproject\r\n.spyproject\r\n\r\n# Rope project settings\r\n.ropeproject\r\n\r\n# mkdocs documentation\r\n/site\r\n\r\n# mypy\r\n.mypy_cache/\r\n.dmypy.json\r\ndmypy.json\r\n\r\n# Pyre type checker\r\n.pyre/\r\n\r\n# Project-specific\r\ndebug_scripts/\r\n*_debug.py\r\n*test_debug.py\r\nrun_specific_tests.py\r\ntest_particle_filter.py\r\ncomprehensive_test.py\r\ntorch_cache/\r\n.ci_env\r\n*.tmp\r\n*.bak\r\n\r\n# Chatbot assistant tools\r\n_chatbot/\r\n!_chatbot/README.md\r\n\r\n# VS Code\r\n.vscode/\r\n\r\n# PyCharm\r\n.idea/\r\n\r\n# MacOS\r\n.DS_Store\r\n\r\n# Windows\r\nThumbs.db\r\nehthumbs.db\r\nDesktop.ini\r\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 2279,
      "filename": ".gitignore"
    },
    {
      "last_modified": "2025-07-08 16:44:01",
      "content_preview": "",
      "extension": ".yaml",
      "size_kb": 0.0,
      "path": ".pre-commit-config.yaml",
      "content": "",
      "directory": "",
      "is_binary": false,
      "size_bytes": 0,
      "filename": ".pre-commit-config.yaml"
    },
    {
      "last_modified": "2025-07-08 15:07:19",
      "content_preview": "{\r\n    \"python.defaultInterpreterPath\": \"${env:PYTHON_PORTABLE}\",\r\n    \"python.terminal.activateEnvironment\": true,\r\n    \"python.envFile\": \"${workspaceFolder}/.env\",\r\n    \"git.path\": \"${env:GIT_PORTAB...",
      "extension": ".json",
      "size_kb": 1.49,
      "path": ".vscode/settings.json",
      "content": "{\r\n    \"python.defaultInterpreterPath\": \"${env:PYTHON_PORTABLE}\",\r\n    \"python.terminal.activateEnvironment\": true,\r\n    \"python.envFile\": \"${workspaceFolder}/.env\",\r\n    \"git.path\": \"${env:GIT_PORTABLE}\",\r\n    \"terminal.integrated.env.windows\": {\r\n        \"PYTHONPATH\": \"${workspaceFolder}\",\r\n        \"PORTABLE_DEV_ROOT\": \"${env:OneDrive}\\\\Onedrive-PortableDenv\",\r\n        \"PROJECTS_ROOT\": \"${env:OneDrive}\\\\Onedrive-PortableDenv\\\\Projects\",\r\n        \"USER_APPS\": \"${env:OneDrive}\\\\Onedrive-PortableDenv\\\\UserApps\",\r\n        \"PROJECT_ROOT\": \"${workspaceFolder}\",\r\n        \"GIT_PORTABLE\": \"${env:GIT_PORTABLE}\",\r\n        \"PYTHON_PORTABLE\": \"${env:PYTHON_PORTABLE}\"\r\n    },\r\n    \"python.analysis.extraPaths\": [\r\n        \"${workspaceFolder}\",\r\n        \"${workspaceFolder}/src\",\r\n        \"${env:OneDrive}\\\\Onedrive-PortableDenv\\\\Projects\"\r\n    ],\r\n    \"files.exclude\": {\r\n        \"**/__pycache__\": true,\r\n        \"**/.pytest_cache\": true,\r\n        \"**/*.pyc\": true,\r\n        \"**/node_modules\": true,\r\n        \"**/.git\": false\r\n    },\r\n    \"git.enabled\": true,\r\n    \"git.autorefresh\": true,\r\n    \"git.autofetch\": true,\r\n    \"git.decorations.enabled\": true,\r\n    \"python.linting.enabled\": true,\r\n    \"python.linting.pylintEnabled\": true,\r\n    \"python.formatting.provider\": \"black\",\r\n    \"python.testing.pytestEnabled\": true,\r\n    \"editor.rulers\": [\r\n        88,\r\n        120\r\n    ],\r\n    \"files.associations\": {\r\n        \"*.py\": \"python\",\r\n        \"*.yml\": \"yaml\",\r\n        \"*.yaml\": \"yaml\",\r\n        \"*.ps1\": \"powershell\"\r\n    }\r\n}\r\n",
      "directory": ".vscode",
      "is_binary": false,
      "size_bytes": 1529,
      "filename": "settings.json"
    },
    {
      "last_modified": "2025-07-12 07:42:40",
      "content_preview": "# Changelog\n\n## [v0.10 - 2025-07-12]\n\n    Changed: Repo to with cleanup selections from in wip project \n    Fixed: Various minor adjustments in ported files, wip\n\n..\n\n## [0.0.01 2025-07-01] \nSkeletal ...",
      "extension": ".md",
      "size_kb": 0.25,
      "path": "changelog.md",
      "content": "# Changelog\n\n## [v0.10 - 2025-07-12]\n\n    Changed: Repo to with cleanup selections from in wip project \n    Fixed: Various minor adjustments in ported files, wip\n\n..\n\n## [0.0.01 2025-07-01] \nSkeletal prototype created via Ai Promptineering e.g. \"vibe coding\"\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 259,
      "filename": "changelog.md"
    },
    {
      "last_modified": "2025-07-08 15:58:42",
      "content_preview": "# Adaptive Visual Decision Making Configuration\nexperiment:\n  name: \"fork_intersection_avoidance\"\n  description: \"Recursive Bayesian framework with generative priors\"\n  scene_duration: 1000 # Total sc...",
      "extension": ".yaml",
      "size_kb": 0.6,
      "path": "config/experiment.yaml",
      "content": "# Adaptive Visual Decision Making Configuration\nexperiment:\n  name: \"fork_intersection_avoidance\"\n  description: \"Recursive Bayesian framework with generative priors\"\n  scene_duration: 1000 # Total scene length in steps\n\ntask_parameters:\n  danger_probability: 0.8\n  difficulty_levels: 3\n\nvolatility_parameters:\n  hazard_rate: 0.005\n  tau_drift: 30\n  bias_extremes: [0.2, 0.8]\n\nvisual_parameters:\n  scene_size: [64, 64]\n  safe_background_color: [50, 50, 50]\n  danger_background_color: [255, 215, 0]\n\nmodel_parameters:\n  latent_dim: 10\n  generator_hidden: 32\n  learning_rate: 0.001\n  reconstruction_baseline_window: 50\n",
      "directory": "config",
      "is_binary": false,
      "size_bytes": 617,
      "filename": "experiment.yaml"
    },
    {
      "last_modified": "2025-07-08 16:16:50",
      "content_preview": "",
      "extension": ".md",
      "size_kb": 0.0,
      "path": "CONTRIBUTING.md",
      "content": "",
      "directory": "",
      "is_binary": false,
      "size_bytes": 0,
      "filename": "CONTRIBUTING.md"
    },
    {
      "last_modified": "2025-07-08 16:59:39",
      "content_preview": "autonomous_perception_learner/\r\n├── src/\r\n│   ├── __init__.py\r\n│   ├── environment/\r\n│   │   ├── __init__.py\r\n│   │   ├── scene_generator.py      # Continuous scene generation\r\n│   │   ├── volatility_...",
      "extension": ".txt",
      "size_kb": 1.07,
      "path": "project.txt",
      "content": "autonomous_perception_learner/\r\n├── src/\r\n│   ├── __init__.py\r\n│   ├── environment/\r\n│   │   ├── __init__.py\r\n│   │   ├── scene_generator.py      # Continuous scene generation\r\n│   │   ├── volatility_controller.py # Time-varying bias dynamics\r\n│   │   └── task_parameters.py      # Hyperparameters & probabilities\r\n│   ├── models/\r\n│   │   ├── __init__.py\r\n│   │   ├── generator.py            # 2-layer ReLU generative prior\r\n│   │   ├── bayesian_learner.py     # Recursive Bayesian framework\r\n│   │   └── surprise_detector.py    # LC-NE surprise signals\r\n│   ├── utils/\r\n│   │   ├── __init__.py\r\n│   │   └── visualization.py        # Scene rendering & plotting\r\n│   └── main.py                     # Main execution\r\n├── configs/\r\n│   └── experiment_config.yaml      # All hyperparameters\r\n├── requirements.txt\r\n├── README.md\r\n└── notebooks/\r\n    └── demo.ipynb                  # Interactive demonstration\r\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 1092,
      "filename": "project.txt"
    },
    {
      "last_modified": "2025-07-08 15:07:19",
      "content_preview": "[build-system]\r\nrequires = [\"setuptools>=61.0\"]\r\nbuild-backend = \"setuptools.build_meta\"\r\n\r\n[project]\r\nname = \"adaptive_bayesian_driver\"\r\nversion = \"1.0.0\"\r\ndescription = \"A recursive Bayesian learnin...",
      "extension": ".toml",
      "size_kb": 2.87,
      "path": "pyproject copy.toml",
      "content": "[build-system]\r\nrequires = [\"setuptools>=61.0\"]\r\nbuild-backend = \"setuptools.build_meta\"\r\n\r\n[project]\r\nname = \"adaptive_bayesian_driver\"\r\nversion = \"1.0.0\"\r\ndescription = \"A recursive Bayesian learning system with surprise detection for autonomous driving, based on the LC-NE neuroscience framework.\"\r\nauthors = [{name = \"Your Name\", email = \"your.email@example.com\"}]\r\nreadme = \"README.md\"\r\nlicense = {file = \"LICENSE\"}\r\nrequires-python = \">=3.8\"\r\nclassifiers = [\r\n    \"Development Status :: 4 - Beta\",\r\n    \"Intended Audience :: Science/Research\",\r\n    \"License :: OSI Approved :: MIT License\",\r\n    \"Operating System :: OS Independent\",\r\n    \"Programming Language :: Python :: 3\",\r\n    \"Programming Language :: Python :: 3.8\",\r\n    \"Programming Language :: Python :: 3.9\",\r\n    \"Programming Language :: Python :: 3.10\",\r\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\r\n]\r\ndependencies = [\r\n    \"numpy>=1.20.0\",\r\n    \"torch>=1.9.0\",\r\n    \"torchvision>=0.10.0\",\r\n    \"Pillow>=8.0.0\",\r\n    \"PyYAML>=5.4.0\",\r\n    \"matplotlib>=3.3.0\",\r\n    \"scipy>=1.7.0\",\r\n    \"scikit-learn>=0.24.0\"\r\n]\r\n\r\n[project.optional-dependencies]\r\ndev = [\r\n    \"pytest>=6.0\",\r\n    \"pytest-cov>=2.0\",\r\n    \"flake8>=3.8\",\r\n    \"mypy>=0.800\",\r\n    \"black>=21.0\",\r\n    \"jupyter>=1.0.0\",\r\n    \"seaborn>=0.11.0\"\r\n]\r\ndocs = [\r\n    \"sphinx>=4.0\",\r\n    \"sphinx-rtd-theme>=0.5\",\r\n    \"myst-parser>=0.15\"\r\n]\r\n\r\n[project.scripts]\r\nadaptive-bayesian-driver = \"adaptive_bayesian_driver.main:main\"\r\n\r\n[project.urls]\r\nHomepage = \"https://github.com/yourusername/adaptive-bayesian-driver\"\r\nDocumentation = \"https://adaptive-bayesian-driver.readthedocs.io/\"\r\nRepository = \"https://github.com/yourusername/adaptive-bayesian-driver.git\"\r\n\"Bug Tracker\" = \"https://github.com/yourusername/adaptive-bayesian-driver/issues\"\r\n\r\n[tool.setuptools.packages.find]\r\ninclude = [\"adaptive_bayesian_driver*\"]\r\n\r\n[tool.pytest.ini_options]\r\ntestpaths = [\"tests\"]\r\npython_files = [\"test_*.py\"]\r\npython_classes = [\"Test*\"]\r\npython_functions = [\"test_*\"]\r\naddopts = \"-v --tb=short --strict-markers\"\r\nmarkers = [\r\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\r\n    \"integration: marks tests as integration tests\",\r\n]\r\n\r\n[tool.black]\r\nline-length = 100\r\ntarget-version = ['py38']\r\ninclude = '\\.pyi?$'\r\nextend-exclude = '''\r\n/(\r\n  # directories\r\n  \\.eggs\r\n  | \\.git\r\n  | \\.hg\r\n  | \\.mypy_cache\r\n  | \\.tox\r\n  | \\.venv\r\n  | build\r\n  | dist\r\n)/\r\n'''\r\n\r\n[tool.mypy]\r\npython_version = \"3.8\"\r\nwarn_return_any = true\r\nwarn_unused_configs = true\r\ndisallow_untyped_defs = true\r\nno_implicit_optional = true\r\nwarn_redundant_casts = true\r\nwarn_unused_ignores = true\r\nshow_error_codes = true\r\n\r\n[[tool.mypy.overrides]]\r\nmodule = [\r\n    \"matplotlib.*\",\r\n    \"scipy.*\",\r\n    \"sklearn.*\",\r\n    \"PIL.*\",\r\n    \"torchvision.*\"\r\n]\r\nignore_missing_imports = true\r\n\r\n[tool.flake8]\r\nmax-line-length = 100\r\nextend-ignore = [\"E203\", \"W503\"]\r\nper-file-ignores = [\r\n    \"__init__.py:F401\",\r\n]\r\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 2939,
      "filename": "pyproject copy.toml"
    },
    {
      "last_modified": "2025-07-08 15:07:19",
      "content_preview": "[build-system]\r\nrequires = [\"setuptools>=61.0\"]\r\nbuild-backend = \"setuptools.build_meta\"\r\n\r\n[project]\r\nname = \"adaptive_bayesian_driver\"\r\nversion = \"1.0.0\"\r\ndescription = \"A recursive Bayesian learnin...",
      "extension": ".toml",
      "size_kb": 2.87,
      "path": "pyproject.toml",
      "content": "[build-system]\r\nrequires = [\"setuptools>=61.0\"]\r\nbuild-backend = \"setuptools.build_meta\"\r\n\r\n[project]\r\nname = \"adaptive_bayesian_driver\"\r\nversion = \"1.0.0\"\r\ndescription = \"A recursive Bayesian learning system with surprise detection for autonomous driving, based on the LC-NE neuroscience framework.\"\r\nauthors = [{name = \"Your Name\", email = \"your.email@example.com\"}]\r\nreadme = \"README.md\"\r\nlicense = {file = \"LICENSE\"}\r\nrequires-python = \">=3.8\"\r\nclassifiers = [\r\n    \"Development Status :: 4 - Beta\",\r\n    \"Intended Audience :: Science/Research\",\r\n    \"License :: OSI Approved :: MIT License\",\r\n    \"Operating System :: OS Independent\",\r\n    \"Programming Language :: Python :: 3\",\r\n    \"Programming Language :: Python :: 3.8\",\r\n    \"Programming Language :: Python :: 3.9\",\r\n    \"Programming Language :: Python :: 3.10\",\r\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\r\n]\r\ndependencies = [\r\n    \"numpy>=1.20.0\",\r\n    \"torch>=1.9.0\",\r\n    \"torchvision>=0.10.0\",\r\n    \"Pillow>=8.0.0\",\r\n    \"PyYAML>=5.4.0\",\r\n    \"matplotlib>=3.3.0\",\r\n    \"scipy>=1.7.0\",\r\n    \"scikit-learn>=0.24.0\"\r\n]\r\n\r\n[project.optional-dependencies]\r\ndev = [\r\n    \"pytest>=6.0\",\r\n    \"pytest-cov>=2.0\",\r\n    \"flake8>=3.8\",\r\n    \"mypy>=0.800\",\r\n    \"black>=21.0\",\r\n    \"jupyter>=1.0.0\",\r\n    \"seaborn>=0.11.0\"\r\n]\r\ndocs = [\r\n    \"sphinx>=4.0\",\r\n    \"sphinx-rtd-theme>=0.5\",\r\n    \"myst-parser>=0.15\"\r\n]\r\n\r\n[project.scripts]\r\nadaptive-bayesian-driver = \"adaptive_bayesian_driver.main:main\"\r\n\r\n[project.urls]\r\nHomepage = \"https://github.com/yourusername/adaptive-bayesian-driver\"\r\nDocumentation = \"https://adaptive-bayesian-driver.readthedocs.io/\"\r\nRepository = \"https://github.com/yourusername/adaptive-bayesian-driver.git\"\r\n\"Bug Tracker\" = \"https://github.com/yourusername/adaptive-bayesian-driver/issues\"\r\n\r\n[tool.setuptools.packages.find]\r\ninclude = [\"adaptive_bayesian_driver*\"]\r\n\r\n[tool.pytest.ini_options]\r\ntestpaths = [\"tests\"]\r\npython_files = [\"test_*.py\"]\r\npython_classes = [\"Test*\"]\r\npython_functions = [\"test_*\"]\r\naddopts = \"-v --tb=short --strict-markers\"\r\nmarkers = [\r\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\r\n    \"integration: marks tests as integration tests\",\r\n]\r\n\r\n[tool.black]\r\nline-length = 100\r\ntarget-version = ['py38']\r\ninclude = '\\.pyi?$'\r\nextend-exclude = '''\r\n/(\r\n  # directories\r\n  \\.eggs\r\n  | \\.git\r\n  | \\.hg\r\n  | \\.mypy_cache\r\n  | \\.tox\r\n  | \\.venv\r\n  | build\r\n  | dist\r\n)/\r\n'''\r\n\r\n[tool.mypy]\r\npython_version = \"3.8\"\r\nwarn_return_any = true\r\nwarn_unused_configs = true\r\ndisallow_untyped_defs = true\r\nno_implicit_optional = true\r\nwarn_redundant_casts = true\r\nwarn_unused_ignores = true\r\nshow_error_codes = true\r\n\r\n[[tool.mypy.overrides]]\r\nmodule = [\r\n    \"matplotlib.*\",\r\n    \"scipy.*\",\r\n    \"sklearn.*\",\r\n    \"PIL.*\",\r\n    \"torchvision.*\"\r\n]\r\nignore_missing_imports = true\r\n\r\n[tool.flake8]\r\nmax-line-length = 100\r\nextend-ignore = [\"E203\", \"W503\"]\r\nper-file-ignores = [\r\n    \"__init__.py:F401\",\r\n]\r\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 2939,
      "filename": "pyproject.toml"
    },
    {
      "last_modified": "2025-07-11 15:08:20",
      "content_preview": "# Adaptive Bayesian Driver\n\nA professional, biologically-inspired framework for adaptive learning and decision-making in non-stationary environments. This project implements computational models inspi...",
      "extension": ".md",
      "size_kb": 10.11,
      "path": "README.md",
      "content": "# Adaptive Bayesian Driver\n\nA professional, biologically-inspired framework for adaptive learning and decision-making in non-stationary environments. This project implements computational models inspired by the locus coeruleus-norepinephrine (LC-NE) system for adaptive visual decision making with applications in autonomous driving and medical imaging.\n\n## 🎯 Overview\n\nThis project implements a computational framework that demonstrates adaptive decision-making in non-stationary visual environments using recursive Bayesian updating, surprise detection, and generative priors. The system learns to navigate complex scenarios while adapting to:\n\n- **Time-varying environmental statistics** (changing traffic patterns, evolving medical conditions)\n- **Context-dependent rules** (danger sign avoidance, anomaly detection)\n- **Perceptual uncertainty** (varying visual difficulty, noise levels)\n- **Multi-modal sensory inputs** (visual, temporal, spatial features)\n\n### Key Features\n\n- **Recursive Bayesian Learning**: Dual-mode inference (particle filter + variational) for robust uncertainty estimation\n- **Surprise Detection**: Multi-modal surprise metrics with adaptive thresholding\n- **Volatility Control**: HMM-based volatility modeling for non-stationary environments\n- **Scene Rendering**: Configurable environments for driving and medical imaging scenarios\n- **Medical Applications**: Longitudinal MRI processing for anomaly detection\n- **Professional Engineering**: Comprehensive testing, CI/CD, and configuration management\n\n## 📁 Project Structure\n\n```\nsurprise-learning-exploration-exploitation-wip/\n├── adaptive_bayesian_driver/     # Main package\n│   ├── __init__.py              # Package initialization\n│   ├── config.py                # Configuration management\n│   ├── main.py                  # CLI entry point\n│   ├── models/                  # Core learning models\n│   │   ├── base.py              # Base classes and generative models\n│   │   ├── recursive_bayesian.py # Recursive Bayesian learner\n│   │   ├── surprise.py          # Surprise detection and metrics\n│   │   └── utils_particle.py    # Particle filter utilities\n│   ├── environment/             # Environment simulation\n│   │   ├── volatility.py        # Volatility controllers\n│   │   ├── scene_renderer.py    # Scene rendering (driving/MNIST)\n│   │   └── task_hmm.py          # Hidden Markov Models\n│   ├── utils/                   # Utility functions\n│   │   └── visualization.py     # Plotting and analysis\n│   └── applications/            # Domain-specific applications\n│       └── medical_imaging.py   # MRI anomaly detection\n├── config/                      # Configuration files\n│   └── experiment.yaml          # Experiment configurations\n├── tests/                       # Test suites\n│   ├── test_environment.py      # Environment tests\n│   └── test_recursive_bayes.py  # Model tests\n├── .github/workflows/           # CI/CD pipeline\n│   └── ci.yml                   # GitHub Actions workflow\n├── _assistant/                  # AI assistant guidelines\n│   ├── README.md               # Assistant guidelines overview\n│   ├── SESSION_GUIDELINES.md   # Session management procedures\n│   ├── CONTEXT_SHARING.md      # Context sharing standards\n│   ├── QUICK_COMMANDS.md       # Command reference for assistants\n│   └── templates/              # Session and handoff templates\n├── _reports/                   # Development reports\n│   ├── README.md               # Report generation standards\n│   ├── generate-report.ps1     # Automated report generation\n│   └── templates/              # Report templates\n├── notebooks/                   # Jupyter notebooks\n│   └── demo.ipynb              # Demonstration notebook\n├── deprecated/                  # Legacy code (archived)\n├── requirements.txt             # Python dependencies\n├── pyproject.toml              # Project configuration\n├── REPO_GUIDELINES.md          # Development standards\n├── QUICK_REFERENCE.md          # Quick command reference\n└── README.md                   # This file\n```\n\n## 🚀 Quick Start\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/surprise-learning-exploration-exploitation-wip.git\ncd surprise-learning-exploration-exploitation-wip\n\n# Install dependencies\npip install -r requirements.txt\n\n# Install in development mode\npip install -e .\n```\n\n### Basic Usage\n\n```python\n# Run a complete experiment\npython -m adaptive_bayesian_driver.main --experiment driving_demo\n\n# Run medical imaging demo\npython -m adaptive_bayesian_driver.main --experiment medical_imaging\n\n# Interactive demo\npython -m adaptive_bayesian_driver.main --demo\n```\n\n### Configuration\n\nCustomize experiments using `config/experiment.yaml`:\n\n```yaml\nexperiment:\n  name: \"driving_demo\"\n  num_trials: 1000\n  save_results: true\n\nenvironment:\n  volatility_params:\n    hazard_rate: 0.005\n    tau_drift: 30\n  task_params:\n    danger_probability: 0.8\n    difficulty_levels: 3\n\nmodel:\n  inference_mode: \"dual\"  # \"particle\", \"variational\", or \"dual\"\n  particle_count: 100\n  learning_rate: 0.01\n```\n\n## 🧠 Core Concepts\n\n### Surprise Detection\nThe system implements multi-modal surprise detection inspired by the LC-NE system:\n\n```python\nfrom adaptive_bayesian_driver.models.surprise import SurpriseMeter\n\nsurprise_meter = SurpriseMeter(\n    metrics=['reconstruction', 'kl_divergence', 'predictive']\n)\nsurprise_score = surprise_meter.compute_surprise(observation, prediction)\n```\n\n### Recursive Bayesian Learning\nDual-mode inference combines particle filtering with variational methods:\n\n```python\nfrom adaptive_bayesian_driver.models.recursive_bayesian import RecursiveBayesianLearner\n\nlearner = RecursiveBayesianLearner(\n    inference_mode='dual',\n    particle_count=100,\n    obs_dim=784,\n    latent_dim=10\n)\n```\n\n### Environment Simulation\nConfigurable environments for different domains:\n\n```python\nfrom adaptive_bayesian_driver.environment.scene_renderer import SceneRenderer\n\nrenderer = SceneRenderer(\n    scene_type='driving',\n    difficulty_level=2,\n    noise_level=0.1\n)\nscene = renderer.render(trial_data)\n```\n\n## 🧪 Testing\n\nRun the test suite:\n\n```bash\n# Run all tests\npytest tests/\n\n# Run specific test categories\npytest tests/test_environment.py\npytest tests/test_recursive_bayes.py\n\n# Run with coverage\npytest --cov=adaptive_bayesian_driver tests/\n```\n\n## 📊 Applications\n\n### Autonomous Driving\n- Fork intersection navigation with adaptive decision-making\n- Danger sign detection and avoidance\n- Uncertainty-aware path planning\n\n### Medical Imaging\n- Longitudinal MRI analysis for anomaly detection\n- Adaptive thresholding for lesion identification\n- Uncertainty quantification in diagnoses\n\n### Research Applications\n- Computational neuroscience modeling\n- Adaptive learning algorithms\n- Non-stationary environment navigation\n\n## 🎯 Key Components\n\n### Models\n- **Base Models**: Generative models, VAE, particle filters\n- **Recursive Bayesian**: Dual-mode inference with uncertainty estimation\n- **Surprise Detection**: Multi-modal surprise metrics\n- **Particle Utilities**: Efficient particle filter operations\n\n### Environment\n- **Volatility Control**: HMM-based environmental changes\n- **Scene Rendering**: Configurable visual environments\n- **Task HMM**: Hidden state modeling for complex tasks\n\n### Applications\n- **Medical Imaging**: MRI processing and anomaly detection\n- **Visualization**: Comprehensive plotting and analysis tools\n\n## 🔧 Development\n\n### Development Guidelines\nThis project follows established development practices. See these files for detailed guidance:\n\n- **[REPO_GUIDELINES.md](REPO_GUIDELINES.md)**: Comprehensive development standards\n- **[QUICK_REFERENCE.md](QUICK_REFERENCE.md)**: Quick command reference\n- **[_reports/README.md](_reports/README.md)**: Report generation standards\n- **[_assistant/README.md](_assistant/README.md)**: AI assistant guidelines and session management\n\n### Project Structure\nThe project follows modern Python packaging standards:\n- Type hints throughout\n- Comprehensive testing\n- CI/CD with GitHub Actions\n- Configuration management\n- Professional documentation\n\n### Contributing\n1. Review [REPO_GUIDELINES.md](REPO_GUIDELINES.md) for development standards\n2. Fork the repository\n3. Create a feature branch following naming conventions\n4. Add tests for new functionality\n5. Update documentation and changelog\n6. Generate appropriate reports\n7. Ensure all tests pass\n8. Submit a pull request with attached reports\n\n### Session Continuity\nFor AI-assisted development sessions, reference these resources:\n- **[_assistant/README.md](_assistant/README.md)**: AI assistant guidelines overview\n- **[_assistant/SESSION_GUIDELINES.md](_assistant/SESSION_GUIDELINES.md)**: Complete session procedures\n- **[_assistant/QUICK_COMMANDS.md](_assistant/QUICK_COMMANDS.md)**: Command reference for assistants\n- **[QUICK_REFERENCE.md](QUICK_REFERENCE.md)**: General quick reference for project state and commands\n\n## 📈 Performance\n\nThe system demonstrates:\n- **Adaptive Learning**: Rapid adaptation to environmental changes\n- **Uncertainty Quantification**: Reliable confidence estimates\n- **Robust Performance**: Stable operation across different scenarios\n- **Scalability**: Efficient processing of large datasets\n\n## 🔗 References\n\n- Locus Coeruleus-Norepinephrine system in adaptive learning\n- Recursive Bayesian inference for non-stationary environments\n- Surprise-based learning and attention mechanisms\n- Variational inference and particle filtering methods\n\n## 📄 License\n\nMIT License - see LICENSE file for details.\n\n## � Acknowledgments\n\nThis project builds upon research in computational neuroscience, adaptive learning, and uncertainty quantification. Special thanks to the biological inspiration from the LC-NE system and the broader machine learning community.\n\n- **Research Background**: [LC-NE system modeling papers]\n- **Technical Documentation**: [API reference]\n- **Demo Video**: [YouTube demonstration]\n- **Related Work**: [Autonomous driving perception]\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 10356,
      "filename": "README.md"
    },
    {
      "last_modified": "2025-07-12 07:59:11",
      "content_preview": "{\r\n  \"metadata\": {\r\n    \"skipped_files\": 0,\r\n    \"export_date\": \"2025-07-12 07:59:11\",\r\n    \"total_files\": 0,\r\n    \"repository_path\": null,\r\n    \"exported_files\": 0\r\n  },\r\n  \"files\": null\r\n}",
      "extension": ".json",
      "size_kb": 0.19,
      "path": "repository_export.json",
      "content": "{\r\n  \"metadata\": {\r\n    \"skipped_files\": 0,\r\n    \"export_date\": \"2025-07-12 07:59:11\",\r\n    \"total_files\": 0,\r\n    \"repository_path\": null,\r\n    \"exported_files\": 0\r\n  },\r\n  \"files\": null\r\n}",
      "directory": "",
      "is_binary": false,
      "size_bytes": 193,
      "filename": "repository_export.json"
    },
    {
      "last_modified": "2025-07-11 12:42:38",
      "content_preview": "# Core dependencies\ntorch>=1.9.0\ntorchvision>=0.10.0\nnumpy>=1.20.0,<2.0.0\nmatplotlib>=3.3.0,<4.0.0\npyyaml>=5.4.0,<7.0.0\nPillow>=8.0.0,<11.0.0\nscipy>=1.7.0,<2.0.0\nscikit-learn>=0.24.0,<2.0.0\n\n# Data an...",
      "extension": ".txt",
      "size_kb": 0.46,
      "path": "requirements.txt",
      "content": "# Core dependencies\ntorch>=1.9.0\ntorchvision>=0.10.0\nnumpy>=1.20.0,<2.0.0\nmatplotlib>=3.3.0,<4.0.0\npyyaml>=5.4.0,<7.0.0\nPillow>=8.0.0,<11.0.0\nscipy>=1.7.0,<2.0.0\nscikit-learn>=0.24.0,<2.0.0\n\n# Data analysis and visualization\npandas>=1.3.0,<3.0.0\nseaborn>=0.11.0,<1.0.0\njupyter>=1.0.0\n\n# Optional dependencies\nopencv-python>=4.5.0 ##\ntqdm>=4.60.0\n\n# Development dependencies\npytest>=6.0,<8.0.0\npytest-cov>=2.0,<5.0.0\nflake8>=3.8,<7.0.0\nmypy>=0.800,<2.0.0\nblack>=21.0,<25.0.0\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 474,
      "filename": "requirements.txt"
    },
    {
      "last_modified": "2025-07-08 15:07:19",
      "content_preview": "[flake8]\r\nmax-line-length = 100\r\nextend-ignore = E203, W503, E501\r\nper-file-ignores =\r\n    __init__.py:F401\r\n    */test_*.py:F401,F811\r\nexclude =\r\n    .git,\r\n    __pycache__,\r\n    build,\r\n    dist,\r\n ...",
      "extension": ".cfg",
      "size_kb": 0.26,
      "path": "setup.cfg",
      "content": "[flake8]\r\nmax-line-length = 100\r\nextend-ignore = E203, W503, E501\r\nper-file-ignores =\r\n    __init__.py:F401\r\n    */test_*.py:F401,F811\r\nexclude =\r\n    .git,\r\n    __pycache__,\r\n    build,\r\n    dist,\r\n    *.egg-info,\r\n    .venv,\r\n    venv,\r\n    env\r\nmax-complexity = 15\r\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 269,
      "filename": "setup.cfg"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "from setuptools import setup, find_packages\n\nsetup(\n    name=\"adaptive_bayesian_driver\",\n    version=\"1.0.0\",\n    description=\"An adaptive learning agent based on a recursive Bayesian framework.\",\n   ...",
      "extension": ".py",
      "size_kb": 0.48,
      "path": "setup.py",
      "content": "from setuptools import setup, find_packages\n\nsetup(\n    name=\"adaptive_bayesian_driver\",\n    version=\"1.0.0\",\n    description=\"An adaptive learning agent based on a recursive Bayesian framework.\",\n    packages=find_packages(),\n    install_requires=[\n        \"numpy\",\n        \"torch\",\n        \"Pillow\",\n        \"PyYAML\",\n        \"matplotlib\"\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"adaptive-bayesian-driver=adaptive_bayesian_driver.main:main\"\n        ]\n    }\n)\n",
      "directory": "",
      "is_binary": false,
      "size_bytes": 488,
      "filename": "setup.py"
    },
    {
      "last_modified": "2025-07-06 23:51:44",
      "content_preview": "# src/models/bayesian_learner.py\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom typing import Dict, List, Tuple, Optional\r\nfrom .generator import Simple2LayerReLU\r\nfrom .surprise_detec...",
      "extension": ".py",
      "size_kb": 25.0,
      "path": "src/models/RecursiveBayesianLearner.py",
      "content": "# src/models/bayesian_learner.py\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom typing import Dict, List, Tuple, Optional\r\nfrom .generator import Simple2LayerReLU\r\nfrom .surprise_detector import SurpriseDetector\r\n\r\nclass RecursiveBayesianLearner:\r\n    \"\"\"\r\n    Complete implementation of adaptive recursive Bayesian learning framework\r\n    \r\n    Key Features:\r\n    - LC-NE inspired surprise detection with adaptive thresholds\r\n    - Ego vehicle embodied learning for rule acquisition\r\n    - Two-timescale dynamics (fast latent, slow generator updates)\r\n    - Emergent bias tracking through reconstruction error patterns\r\n    - Computer vision processing of fork intersection scenes\r\n    - Danger avoidance task with yellow/white background detection\r\n    \"\"\"\r\n    \r\n    def __init__(self, config: Dict):\r\n        self.config = config\r\n        \r\n        # Initialize generative prior (2-layer ReLU)\r\n        scene_size = config['visual_parameters']['scene_size']\r\n        self.generator = Simple2LayerReLU(\r\n            latent_dim=config['model_parameters']['latent_dim'],\r\n            output_dim=np.prod(scene_size),\r\n            hidden_dim=config['model_parameters']['generator_hidden']\r\n        )\r\n        \r\n        # Initialize surprise detection system\r\n        self.surprise_detector = SurpriseDetector(\r\n            baseline_window=config['model_parameters']['reconstruction_baseline_window']\r\n        )\r\n        \r\n        # Learning state tracking\r\n        self.reconstruction_history = []\r\n        self.context_uncertainty_history = []\r\n        self.performance_history = []\r\n        self.surprise_history = []\r\n        self.action_history = []\r\n        \r\n        # Ego vehicle state for embodied learning\r\n        self.ego_vehicle_state = {\r\n            'recent_danger_encounters': [],\r\n            'rule_confidence': 0.5,\r\n            'conservative_bias': 0.0,\r\n            'volatility_estimate': 0.5\r\n        }\r\n        \r\n        # Rule learning system\r\n        self.danger_avoidance_strength = 0.5\r\n        self.rule_violation_memory = []\r\n        \r\n        # Adaptive learning parameters\r\n        self.base_learning_rate = config['model_parameters']['learning_rate']\r\n        self.adaptation_rate = config['biological_parameters']['adaptation_rate']\r\n        \r\n    def process_scene_sequence(self, scenes: torch.Tensor, \r\n                             metadata: List[Dict]) -> Dict:\r\n        \"\"\"Process continuous sequence of fork intersection scenes\"\"\"\r\n        results = {\r\n            'surprise_signals': [],\r\n            'adaptive_thresholds': [],\r\n            'reconstruction_errors': [],\r\n            'actions': [],\r\n            'correct_actions': [],\r\n            'context_uncertainty': [],\r\n            'performance': [],\r\n            'ego_states': [],\r\n            'rule_violations': [],\r\n            'learning_rates': []\r\n        }\r\n        \r\n        for step, (scene, meta) in enumerate(zip(scenes, metadata)):\r\n            # Process single scene through complete framework\r\n            step_results = self._process_single_scene(scene, meta, step)\r\n            \r\n            # Store all results for analysis\r\n            for key, value in step_results.items():\r\n                results[key].append(value)\r\n                \r\n        return results\r\n    \r\n    def _process_single_scene(self, scene: torch.Tensor, metadata: Dict, \r\n                            step: int) -> Dict:\r\n        \"\"\"\r\n        Process single scene through recursive Bayesian framework with ego vehicle perspective\r\n        \"\"\"\r\n        \r\n        # 1. Establish ego vehicle context (enables rule understanding)\r\n        ego_state = self._update_ego_vehicle_state(scene, metadata, step)\r\n        \r\n        # 2. Compute reconstruction error via generative prior\r\n        reconstruction_error = self.generator.compute_reconstruction_error(scene)\r\n        self.reconstruction_history.append(reconstruction_error)\r\n        \r\n        # 3. Compute emergent context uncertainty\r\n        context_uncertainty = self._compute_context_uncertainty()\r\n        prediction_confidence = self._compute_prediction_confidence()\r\n        self.context_uncertainty_history.append(context_uncertainty)\r\n        \r\n        # 4. Adaptive surprise threshold (continuous function of uncertainty)\r\n        adaptive_threshold = self.surprise_detector.adaptive_surprise_threshold(\r\n            context_uncertainty, prediction_confidence, ego_state['volatility_regime']\r\n        )\r\n        \r\n        # 5. Compute surprise signal with adaptive threshold\r\n        surprise = self.surprise_detector.compute_surprise(\r\n            reconstruction_error, \r\n            self.reconstruction_history,\r\n            context_uncertainty=context_uncertainty,\r\n            prediction_confidence=prediction_confidence\r\n        )\r\n        self.surprise_history.append(surprise)\r\n        \r\n        # 6. Make ego vehicle action decision with rule learning\r\n        action = self._make_ego_vehicle_decision(scene, metadata, surprise, ego_state)\r\n        self.action_history.append(action)\r\n        \r\n        # 7. Get feedback and experience consequences\r\n        feedback = self._get_feedback(action, metadata['correct_action'])\r\n        self.performance_history.append(feedback)\r\n        \r\n        # 8. Detect rule violations for learning\r\n        rule_violation = self._detect_rule_violation(action, metadata, ego_state)\r\n        \r\n        # 9. Update beliefs based on embodied experience\r\n        learning_rate = self._update_beliefs_with_ego_experience(\r\n            surprise, feedback, reconstruction_error, ego_state, adaptive_threshold, rule_violation\r\n        )\r\n        \r\n        return {\r\n            'surprise_signal': surprise,\r\n            'adaptive_threshold': adaptive_threshold,\r\n            'reconstruction_error': reconstruction_error,\r\n            'action': action,\r\n            'correct_action': metadata['correct_action'],\r\n            'context_uncertainty': context_uncertainty,\r\n            'performance': feedback,\r\n            'ego_state': ego_state.copy(),\r\n            'rule_violation': rule_violation,\r\n            'learning_rate': learning_rate\r\n        }\r\n    \r\n    def _update_ego_vehicle_state(self, scene: torch.Tensor, metadata: Dict, step: int) -> Dict:\r\n        \"\"\"Update ego vehicle state for embodied rule learning\"\"\"\r\n        \r\n        # Detect current scene characteristics\r\n        danger_detected = self._detect_danger_background(scene)\r\n        scene_difficulty = self._assess_scene_difficulty(scene)\r\n        \r\n        # Update danger encounter history\r\n        if danger_detected:\r\n            self.ego_vehicle_state['recent_danger_encounters'].append(step)\r\n            # Keep only recent encounters (last 100 steps)\r\n            self.ego_vehicle_state['recent_danger_encounters'] = \\\r\n                self.ego_vehicle_state['recent_danger_encounters'][-100:]\r\n        \r\n        # Estimate current volatility regime\r\n        volatility_regime = self._estimate_volatility_regime()\r\n        \r\n        # Update rule confidence based on recent performance\r\n        self._update_rule_confidence()\r\n        \r\n        # Update conservative bias based on recent rule violations\r\n        self._update_conservative_bias()\r\n        \r\n        return {\r\n            'position': 'intersection_approach',\r\n            'speed': 'moderate',\r\n            'visibility': scene_difficulty,\r\n            'volatility_regime': volatility_regime,\r\n            'danger_frequency': len(self.ego_vehicle_state['recent_danger_encounters']) / 100,\r\n            'rule_confidence': self.ego_vehicle_state['rule_confidence'],\r\n            'conservative_bias': self.ego_vehicle_state['conservative_bias'],\r\n            'recent_performance': np.mean(self.performance_history[-10:]) if len(self.performance_history) >= 10 else 0.5\r\n        }\r\n    \r\n    def _compute_context_uncertainty(self) -> float:\r\n        \"\"\"Compute emergent context uncertainty from reconstruction error patterns\"\"\"\r\n        if len(self.reconstruction_history) < 10:\r\n            return 0.5  # Initial uncertainty\r\n        \r\n        # Use variance in recent reconstruction errors as uncertainty proxy\r\n        recent_errors = self.reconstruction_history[-20:]\r\n        recent_surprises = self.surprise_history[-20:] if len(self.surprise_history) >= 20 else [0.0] * 20\r\n        \r\n        # Combine reconstruction error variance with surprise volatility\r\n        error_uncertainty = np.var(recent_errors) / (np.mean(recent_errors) + 1e-6)\r\n        surprise_uncertainty = np.var(recent_surprises) if len(recent_surprises) > 1 else 0.0\r\n        \r\n        # Weight combination\r\n        uncertainty = 0.7 * error_uncertainty + 0.3 * surprise_uncertainty\r\n        \r\n        return np.clip(uncertainty, 0.0, 1.0)\r\n    \r\n    def _compute_prediction_confidence(self) -> float:\r\n        \"\"\"Compute prediction confidence from recent performance\"\"\"\r\n        if len(self.performance_history) < 5:\r\n            return 0.5\r\n        \r\n        recent_performance = self.performance_history[-10:]\r\n        confidence = np.mean(recent_performance)\r\n        \r\n        # Adjust for consistency\r\n        consistency = 1.0 - np.var(recent_performance)\r\n        confidence = 0.8 * confidence + 0.2 * consistency\r\n        \r\n        return np.clip(confidence, 0.0, 1.0)\r\n    \r\n    def _estimate_volatility_regime(self) -> str:\r\n        \"\"\"Estimate current environmental volatility regime\"\"\"\r\n        if len(self.surprise_history) < 20:\r\n            return 'moderate'\r\n        \r\n        recent_surprise_rate = np.mean([abs(s) > 0.5 for s in self.surprise_history[-20:]])\r\n        \r\n        if recent_surprise_rate > 0.4:\r\n            return 'volatile'\r\n        elif recent_surprise_rate < 0.1:\r\n            return 'stable'\r\n        else:\r\n            return 'moderate'\r\n    \r\n    def _make_ego_vehicle_decision(self, scene: torch.Tensor, metadata: Dict, \r\n                                 surprise: float, ego_state: Dict) -> str:\r\n        \"\"\"\r\n        Make ego vehicle decision with learned rule understanding\r\n        \"\"\"\r\n        \r\n        # Detect visual features\r\n        danger_detected = self._detect_danger_background(scene)\r\n        left_cue_detected = self._detect_left_orientation(scene)\r\n        \r\n        # Apply rule hierarchy with uncertainty modulation\r\n        if danger_detected:\r\n            # Danger avoidance rule with confidence modulation\r\n            base_action = 'right' if left_cue_detected else 'left'\r\n            \r\n            # Modulate based on rule confidence and surprise\r\n            rule_strength = self.ego_vehicle_state['rule_confidence']\r\n            if surprise > ego_state.get('surprise_threshold', 0.5):\r\n                # High surprise → more conservative (stronger rule application)\r\n                rule_strength = min(1.0, rule_strength * 1.2)\r\n            \r\n            # Apply conservative bias when uncertain\r\n            if ego_state['rule_confidence'] < 0.7:\r\n                conservative_action = self._apply_conservative_strategy(base_action, ego_state)\r\n                action = conservative_action\r\n            else:\r\n                action = base_action\r\n                \r\n        else:\r\n            # Safe condition: follow bar direction with bias consideration\r\n            base_action = 'left' if left_cue_detected else 'right'\r\n            \r\n            # Apply learned biases from reconstruction error patterns\r\n            action = self._apply_emergent_bias(base_action, ego_state)\r\n        \r\n        return action\r\n    \r\n    def _detect_danger_background(self, scene: torch.Tensor) -> bool:\r\n        \"\"\"Detect danger sign from yellow background vs white/gray\"\"\"\r\n        mean_intensity = torch.mean(scene).item()\r\n        # Yellow background has higher intensity than gray/white\r\n        return mean_intensity > 0.6\r\n    \r\n    def _detect_left_orientation(self, scene: torch.Tensor) -> bool:\r\n        \"\"\"Detect left-pointing vs right-pointing oriented bars\"\"\"\r\n        h, w = scene.shape\r\n        center_region = scene[h//2-8:h//2+8, w//2-8:w//2+8]\r\n        \r\n        if center_region.numel() == 0:\r\n            return False\r\n        \r\n        # Gradient-based orientation detection\r\n        grad_x = torch.diff(center_region, dim=1)\r\n        grad_y = torch.diff(center_region, dim=0)\r\n        \r\n        if grad_x.numel() == 0 or grad_y.numel() == 0:\r\n            return False\r\n        \r\n        # Left-pointing bars have specific gradient correlation pattern\r\n        try:\r\n            # Align dimensions for correlation\r\n            min_size = min(grad_x.shape[0], grad_y.shape[0])\r\n            grad_x_flat = grad_x[:min_size, :].flatten()\r\n            grad_y_flat = grad_y[:min_size, :].flatten()\r\n            \r\n            if len(grad_x_flat) > 1 and len(grad_y_flat) > 1:\r\n                correlation = torch.corrcoef(torch.stack([grad_x_flat, grad_y_flat]))[0,1]\r\n                return correlation.item() > 0\r\n            else:\r\n                return False\r\n        except:\r\n            return False\r\n    \r\n    def _assess_scene_difficulty(self, scene: torch.Tensor) -> float:\r\n        \"\"\"Assess visual difficulty of current scene\"\"\"\r\n        # Use variance and edge density as difficulty proxy\r\n        intensity_var = torch.var(scene).item()\r\n        edges = torch.abs(torch.diff(scene, dim=0)).sum() + torch.abs(torch.diff(scene, dim=1)).sum()\r\n        edge_density = edges.item() / scene.numel()\r\n        \r\n        difficulty = 1.0 - (intensity_var + edge_density) / 2.0\r\n        return np.clip(difficulty, 0.0, 1.0)\r\n    \r\n    def _apply_conservative_strategy(self, base_action: str, ego_state: Dict) -> str:\r\n        \"\"\"Apply conservative bias when rule confidence is low\"\"\"\r\n        conservative_bias = ego_state['conservative_bias']\r\n        \r\n        # When uncertain, prefer right turns (arbitrary but consistent choice)\r\n        if conservative_bias > 0.3:\r\n            return 'right'\r\n        else:\r\n            return base_action\r\n    \r\n    def _apply_emergent_bias(self, base_action: str, ego_state: Dict) -> str:\r\n        \"\"\"Apply emergent directional bias learned from reconstruction patterns\"\"\"\r\n        \r\n        # Extract bias from recent reconstruction error patterns\r\n        if len(self.reconstruction_history) < 20:\r\n            return base_action\r\n        \r\n        # Analyze correlation between actions and reconstruction errors\r\n        recent_actions = self.action_history[-20:]\r\n        recent_errors = self.reconstruction_history[-20:]\r\n        \r\n        if len(recent_actions) < 20:\r\n            return base_action\r\n        \r\n        # Simple bias estimation\r\n        left_errors = [e for a, e in zip(recent_actions, recent_errors) if a == 'left']\r\n        right_errors = [e for a, e in zip(recent_actions, recent_errors) if a == 'right']\r\n        \r\n        if len(left_errors) > 0 and len(right_errors) > 0:\r\n            left_avg_error = np.mean(left_errors)\r\n            right_avg_error = np.mean(right_errors)\r\n            \r\n            # Prefer direction with lower average reconstruction error\r\n            if left_avg_error < right_avg_error * 0.9:  # 10% preference threshold\r\n                bias_action = 'left'\r\n            elif right_avg_error < left_avg_error * 0.9:\r\n                bias_action = 'right'\r\n            else:\r\n                bias_action = base_action\r\n        else:\r\n            bias_action = base_action\r\n        \r\n        return bias_action\r\n    \r\n    def _get_feedback(self, action: str, correct_action: str) -> int:\r\n        \"\"\"Get binary feedback signal (1 = correct, 0 = incorrect)\"\"\"\r\n        return 1 if action == correct_action else 0\r\n    \r\n    def _detect_rule_violation(self, action: str, metadata: Dict, ego_state: Dict) -> bool:\r\n        \"\"\"Detect if action violates the danger avoidance rule\"\"\"\r\n        danger_present = metadata['danger_present']\r\n        left_cue = metadata['left_cue']\r\n        \r\n        if danger_present:\r\n            # Rule: danger + left cue → turn right, danger + right cue → turn left\r\n            correct_avoidance = 'right' if left_cue else 'left'\r\n            return action != correct_avoidance\r\n        \r\n        return False\r\n    \r\n    def _update_beliefs_with_ego_experience(self, surprise: float, feedback: int, \r\n                                          reconstruction_error: float, ego_state: Dict,\r\n                                          adaptive_threshold: float, rule_violation: bool) -> float:\r\n        \"\"\"\r\n        Update beliefs based on embodied experience with adaptive learning rate\r\n        \"\"\"\r\n        \r\n        # Calculate learning signal strength\r\n        performance_error = 1 - feedback  # 1 if wrong, 0 if correct\r\n        surprise_magnitude = max(0, surprise - adaptive_threshold)\r\n        \r\n        # Combine multiple error sources\r\n        total_learning_signal = (\r\n            0.4 * performance_error +           # Task performance\r\n            0.3 * surprise_magnitude +          # Prediction surprise\r\n            0.2 * float(rule_violation) +       # Rule violation\r\n            0.1 * reconstruction_error          # Perceptual error\r\n        )\r\n        \r\n        # Adaptive learning rate based on uncertainty and ego state\r\n        base_rate = self.adaptation_rate\r\n        uncertainty_modulation = 1.0 + ego_state['context_uncertainty']\r\n        confidence_modulation = 2.0 - ego_state['rule_confidence']\r\n        \r\n        adaptive_learning_rate = base_rate * uncertainty_modulation * confidence_modulation\r\n        \r\n        # Apply learning if signal is significant\r\n        if total_learning_signal > 0.1:\r\n            # Update generator parameters (slow timescale)\r\n            self._update_generator_weights(total_learning_signal, adaptive_learning_rate)\r\n            \r\n            # Update rule knowledge (ego-specific learning)\r\n            self._update_rule_knowledge(ego_state, feedback, rule_violation, total_learning_signal)\r\n            \r\n            # Update context beliefs (emergent from patterns)\r\n            self._update_context_beliefs(surprise, feedback, reconstruction_error)\r\n        \r\n        return adaptive_learning_rate\r\n    \r\n    def _update_generator_weights(self, learning_signal: float, learning_rate: float):\r\n        \"\"\"Update generative prior weights based on surprise-driven adaptation\"\"\"\r\n        \r\n        # Implement simplified weight update (in practice, would use proper gradients)\r\n        adaptation_strength = learning_rate * learning_signal\r\n        \r\n        # Simple perturbation-based update for demonstration\r\n        with torch.no_grad():\r\n            for param in self.generator.parameters():\r\n                if param.grad is not None:\r\n                    # Apply adaptation with momentum\r\n                    param.data -= adaptation_strength * param.grad\r\n                else:\r\n                    # Small random perturbation when no gradient available\r\n                    noise = torch.randn_like(param) * adaptation_strength * 0.01\r\n                    param.data += noise\r\n    \r\n    def _update_rule_knowledge(self, ego_state: Dict, feedback: int, \r\n                             rule_violation: bool, learning_signal: float):\r\n        \"\"\"Update internal rule representations based on ego vehicle experience\"\"\"\r\n        \r\n        # Update rule confidence based on performance\r\n        if rule_violation and feedback == 0:\r\n            # Rule violation led to bad outcome → strengthen rule\r\n            self.ego_vehicle_state['rule_confidence'] = min(1.0, \r\n                self.ego_vehicle_state['rule_confidence'] + 0.1 * learning_signal)\r\n        elif not rule_violation and feedback == 1:\r\n            # Following rule led to good outcome → reinforce rule\r\n            self.ego_vehicle_state['rule_confidence'] = min(1.0,\r\n                self.ego_vehicle_state['rule_confidence'] + 0.05 * learning_signal)\r\n        elif rule_violation and feedback == 1:\r\n            # Rule violation led to good outcome → weaken rule slightly\r\n            self.ego_vehicle_state['rule_confidence'] = max(0.0,\r\n                self.ego_vehicle_state['rule_confidence'] - 0.02 * learning_signal)\r\n        \r\n        # Track rule violations for pattern learning\r\n        self.rule_violation_memory.append({\r\n            'violation': rule_violation,\r\n            'feedback': feedback,\r\n            'learning_signal': learning_signal\r\n        })\r\n        \r\n        # Keep only recent memory\r\n        self.rule_violation_memory = self.rule_violation_memory[-50:]\r\n    \r\n    def _update_rule_confidence(self):\r\n        \"\"\"Update rule confidence based on recent performance patterns\"\"\"\r\n        if len(self.performance_history) < 10:\r\n            return\r\n        \r\n        recent_performance = self.performance_history[-10:]\r\n        recent_violations = self.rule_violation_memory[-10:] if len(self.rule_violation_memory) >= 10 else []\r\n        \r\n        # Higher performance → higher rule confidence\r\n        performance_factor = np.mean(recent_performance)\r\n        \r\n        # Fewer violations → higher rule confidence\r\n        violation_factor = 1.0 - np.mean([v['violation'] for v in recent_violations]) if recent_violations else 1.0\r\n        \r\n        # Update with exponential moving average\r\n        alpha = 0.1\r\n        new_confidence = 0.6 * performance_factor + 0.4 * violation_factor\r\n        self.ego_vehicle_state['rule_confidence'] = (\r\n            (1 - alpha) * self.ego_vehicle_state['rule_confidence'] + \r\n            alpha * new_confidence\r\n        )\r\n    \r\n    def _update_conservative_bias(self):\r\n        \"\"\"Update conservative bias based on recent rule violations and uncertainty\"\"\"\r\n        if len(self.context_uncertainty_history) < 5:\r\n            return\r\n        \r\n        recent_uncertainty = np.mean(self.context_uncertainty_history[-5:])\r\n        recent_violations = len([v for v in self.rule_violation_memory[-10:] if v['violation']]) if len(self.rule_violation_memory) >= 10 else 0\r\n        \r\n        # Higher uncertainty → more conservative\r\n        # More violations → more conservative\r\n        target_bias = 0.5 * recent_uncertainty + 0.3 * (recent_violations / 10.0)\r\n        \r\n        # Smooth update\r\n        alpha = 0.05\r\n        self.ego_vehicle_state['conservative_bias'] = (\r\n            (1 - alpha) * self.ego_vehicle_state['conservative_bias'] + \r\n            alpha * target_bias\r\n        )\r\n    \r\n    def _update_context_beliefs(self, surprise: float, feedback: int, reconstruction_error: float):\r\n        \"\"\"Update context beliefs based on surprise and performance patterns\"\"\"\r\n        \r\n        # This implements emergent bias tracking through error patterns\r\n        # The bias emerges from the patterns of reconstruction errors and surprise signals\r\n        # rather than being explicitly programmed\r\n        \r\n        # Track statistical patterns in reconstruction errors\r\n        if len(self.reconstruction_history) >= 20:\r\n            # Analyze recent error patterns for context switching\r\n            recent_errors = self.reconstruction_history[-20:]\r\n            error_trend = np.diff(recent_errors)\r\n            \r\n            # Detect potential context switches from error dynamics\r\n            if np.std(error_trend) > np.mean(recent_errors) * 0.1:\r\n                # High variability suggests context instability\r\n                self.ego_vehicle_state['volatility_estimate'] = min(1.0,\r\n                    self.ego_vehicle_state['volatility_estimate'] + 0.05)\r\n            else:\r\n                # Low variability suggests stable context\r\n                self.ego_vehicle_state['volatility_estimate'] = max(0.0,\r\n                    self.ego_vehicle_state['volatility_estimate'] - 0.02)\r\n    \r\n    def get_learning_metrics(self) -> Dict:\r\n        \"\"\"Get comprehensive learning performance metrics\"\"\"\r\n        if len(self.performance_history) < 10:\r\n            return {\r\n                'performance': 0.5, \r\n                'surprise_rate': 0.5,\r\n                'context_uncertainty': 0.5,\r\n                'rule_confidence': 0.5\r\n            }\r\n        \r\n        recent_performance = np.mean(self.performance_history[-50:])\r\n        recent_surprise = np.mean([abs(s) for s in self.surprise_history[-50:]])\r\n        recent_uncertainty = np.mean(self.context_uncertainty_history[-20:])\r\n        \r\n        return {\r\n            'performance': recent_performance,\r\n            'surprise_rate': recent_surprise,\r\n            'context_uncertainty': recent_uncertainty,\r\n            'rule_confidence': self.ego_vehicle_state['rule_confidence'],\r\n            'conservative_bias': self.ego_vehicle_state['conservative_bias'],\r\n            'volatility_estimate': self.ego_vehicle_state['volatility_estimate'],\r\n            'total_rule_violations': len([v for v in self.rule_violation_memory if v['violation']]),\r\n            'adaptation_rate': np.mean([abs(s) > 0.5 for s in self.surprise_history[-20:]]) if len(self.surprise_history) >= 20 else 0.0\r\n        }\r\n    \r\n    def get_system_state(self) -> Dict:\r\n        \"\"\"Get current complete system state for analysis\"\"\"\r\n        return {\r\n            'ego_vehicle_state': self.ego_vehicle_state.copy(),\r\n            'recent_performance': self.performance_history[-10:] if len(self.performance_history) >= 10 else [],\r\n            'recent_surprises': self.surprise_history[-10:] if len(self.surprise_history) >= 10 else [],\r\n            'recent_uncertainties': self.context_uncertainty_history[-10:] if len(self.context_uncertainty_history) >= 10 else [],\r\n            'generator_state': 'pretrained',  # Could add actual generator analysis\r\n            'learning_metrics': self.get_learning_metrics()\r\n        }\r\n",
      "directory": "src\\models",
      "is_binary": false,
      "size_bytes": 25597,
      "filename": "RecursiveBayesianLearner.py"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "\"\"\"\nParticle filter utilities for adaptive Bayesian learning.\n\"\"\"\n\nimport numpy as np\nimport torch\nfrom typing import Dict, List, Tuple, Optional, Callable\nimport logging\n\nlogger = logging.getLogger(_...",
      "extension": ".py",
      "size_kb": 0.31,
      "path": "src/models/utils_particle.py",
      "content": "\"\"\"\nParticle filter utilities for adaptive Bayesian learning.\n\"\"\"\n\nimport numpy as np\nimport torch\nfrom typing import Dict, List, Tuple, Optional, Callable\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# This file was restored after being corrupted\n# Content needs to be reimplemented based on project needs\n",
      "directory": "src\\models",
      "is_binary": false,
      "size_bytes": 316,
      "filename": "utils_particle.py"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "\"\"\"\nVisualization utilities for adaptive Bayesian learning.\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom typing import Dict, List, Optional\nimport logging\n\nlogger...",
      "extension": ".py",
      "size_kb": 0.33,
      "path": "src/utils/visualization.py",
      "content": "\"\"\"\nVisualization utilities for adaptive Bayesian learning.\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom typing import Dict, List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# This file was restored after being corrupted\n# Content needs to be reimplemented based on project needs\n",
      "directory": "src\\utils",
      "is_binary": false,
      "size_bytes": 338,
      "filename": "visualization.py"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "\"\"\"\nTest suite for environment components.\n\"\"\"\n\nimport unittest\nimport numpy as np\nimport torch\nfrom adaptive_bayesian_driver.environment import (\n    VolatilityController, VolatilityRegime,\n    Scene...",
      "extension": ".py",
      "size_kb": 6.2,
      "path": "tests/test_environment.py",
      "content": "\"\"\"\nTest suite for environment components.\n\"\"\"\n\nimport unittest\nimport numpy as np\nimport torch\nfrom adaptive_bayesian_driver.environment import (\n    VolatilityController, VolatilityRegime,\n    SceneRenderer, TaskHMM, TaskState\n)\nfrom adaptive_bayesian_driver.config import load_config\n\nclass TestVolatilityController(unittest.TestCase):\n    \"\"\"Test VolatilityController functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test configuration.\"\"\"\n        self.config = {\n            'volatility_parameters': {\n                'hazard_rate': 0.01,\n                'tau_drift': 30,\n                'bias_extremes': [0.2, 0.8]\n            }\n        }\n        self.controller = VolatilityController(self.config)\n\n    def test_initialization(self):\n        \"\"\"Test proper initialization.\"\"\"\n        self.assertEqual(self.controller.current_bias, 0.5)\n        self.assertEqual(self.controller.target_bias, 0.5)\n        self.assertEqual(self.controller.volatility_regime, VolatilityRegime.STABLE)\n\n    def test_update(self):\n        \"\"\"Test update functionality.\"\"\"\n        result = self.controller.update(0)\n\n        self.assertIn('current_bias', result)\n        self.assertIn('target_bias', result)\n        self.assertIn('volatility_regime', result)\n        self.assertIn('context_switched', result)\n\n    def test_bias_bounds(self):\n        \"\"\"Test bias stays within bounds.\"\"\"\n        for _ in range(100):\n            self.controller.update(0)\n            self.assertGreaterEqual(self.controller.current_bias, 0.0)\n            self.assertLessEqual(self.controller.current_bias, 1.0)\n\n    def test_reset(self):\n        \"\"\"Test reset functionality.\"\"\"\n        # Change state\n        for _ in range(10):\n            self.controller.update(0)\n\n        # Reset\n        self.controller.reset()\n\n        self.assertEqual(self.controller.current_bias, 0.5)\n        self.assertEqual(len(self.controller.bias_history), 0)\n\nclass TestSceneRenderer(unittest.TestCase):\n    \"\"\"Test SceneRenderer functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test configuration.\"\"\"\n        self.config = {\n            'visual_parameters': {\n                'scene_size': [64, 64],\n                'safe_background_color': [50, 50, 50],\n                'danger_background_color': [255, 215, 0]\n            }\n        }\n        self.renderer = SceneRenderer(self.config)\n\n    def test_initialization(self):\n        \"\"\"Test proper initialization.\"\"\"\n        self.assertEqual(self.renderer.scene_size, [64, 64])\n        self.assertEqual(self.renderer.safe_color, [50, 50, 50])\n\n    def test_render_scene(self):\n        \"\"\"Test scene rendering.\"\"\"\n        params = {\n            'has_danger': True,\n            'orientation_angle': 45.0,\n            'difficulty': 0.2,\n            'correct_direction': 'right'\n        }\n\n        scene, metadata = self.renderer.render_scene(params)\n\n        # Check output format\n        self.assertIsInstance(scene, torch.Tensor)\n        self.assertEqual(scene.shape, (1, 64, 64, 3))  # RGB channels\n        self.assertIsInstance(metadata, dict)\n\n        # Check metadata\n        self.assertEqual(metadata['has_danger'], True)\n        self.assertEqual(metadata['correct_direction'], 'right')\n\n    def test_scene_sequence(self):\n        \"\"\"Test scene sequence generation.\"\"\"\n        scenes, metadata_list = self.renderer.create_scene_sequence(\n            n_scenes=10, bias=0.7, danger_probability=0.8\n        )\n\n        self.assertEqual(scenes.shape[0], 10)  # Batch size\n        self.assertEqual(len(metadata_list), 10)\n\n    def test_difficulty_application(self):\n        \"\"\"Test difficulty effects.\"\"\"\n        params_easy = {\n            'has_danger': False,\n            'orientation_angle': 0.0,\n            'difficulty': 0.0,\n            'correct_direction': 'right'\n        }\n\n        params_hard = {\n            'has_danger': False,\n            'orientation_angle': 0.0,\n            'difficulty': 0.8,\n            'correct_direction': 'right'\n        }\n\n        scene_easy, _ = self.renderer.render_scene(params_easy)\n        scene_hard, _ = self.renderer.render_scene(params_hard)\n\n        # Hard scene should be different due to noise/occlusion\n        self.assertFalse(torch.equal(scene_easy, scene_hard))\n\nclass TestTaskHMM(unittest.TestCase):\n    \"\"\"Test TaskHMM functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test configuration.\"\"\"\n        self.config = {\n            'hmm_learning_rate': 0.01,\n            'hmm_adaptation': True\n        }\n        self.hmm = TaskHMM(self.config)\n\n    def test_initialization(self):\n        \"\"\"Test proper initialization.\"\"\"\n        self.assertEqual(self.hmm.n_states, len(TaskState))\n        self.assertEqual(self.hmm.current_state, 0)\n        self.assertEqual(self.hmm.transition_matrix.shape, (self.hmm.n_states, self.hmm.n_states))\n\n    def test_state_update(self):\n        \"\"\"Test state update.\"\"\"\n        observation = {\n            'has_danger': True,\n            'orientation_angle': 90.0,\n            'correct_direction': 'left'\n        }\n\n        result = self.hmm.update_state(observation)\n\n        self.assertIn('current_state', result)\n        self.assertIn('state_name', result)\n        self.assertIn('state_probabilities', result)\n\n    def test_observation_generation(self):\n        \"\"\"Test observation generation.\"\"\"\n        observation = self.hmm.generate_observation()\n\n        self.assertIn('has_danger', observation)\n        self.assertIn('orientation_angle', observation)\n        self.assertIn('correct_direction', observation)\n\n    def test_likelihood_computation(self):\n        \"\"\"Test likelihood computation.\"\"\"\n        observations = [\n            {'has_danger': True, 'orientation_angle': 90.0, 'correct_direction': 'left'},\n            {'has_danger': False, 'orientation_angle': 0.0, 'correct_direction': 'right'}\n        ]\n\n        likelihood = self.hmm.compute_likelihood(observations)\n        self.assertIsInstance(likelihood, float)\n\n    def test_reset(self):\n        \"\"\"Test reset functionality.\"\"\"\n        # Process some observations\n        for i in range(5):\n            obs = self.hmm.generate_observation()\n            self.hmm.update_state(obs)\n\n        # Reset\n        self.hmm.reset()\n\n        self.assertEqual(self.hmm.current_state, 0)\n        self.assertEqual(len(self.hmm.state_history), 0)\n\nif __name__ == '__main__':\n    unittest.main()\n",
      "directory": "tests",
      "is_binary": false,
      "size_bytes": 6351,
      "filename": "test_environment.py"
    },
    {
      "last_modified": "2025-07-08 15:58:41",
      "content_preview": "\"\"\"\nTest suite for recursive Bayesian learning components.\n\"\"\"\n\nimport unittest\nimport numpy as np\nimport torch\nfrom adaptive_bayesian_driver.models import (\n    RecursiveBayesianLearner, InferenceMod...",
      "extension": ".py",
      "size_kb": 8.0,
      "path": "tests/test_recursive_bayes.py",
      "content": "\"\"\"\nTest suite for recursive Bayesian learning components.\n\"\"\"\n\nimport unittest\nimport numpy as np\nimport torch\nfrom adaptive_bayesian_driver.models import (\n    RecursiveBayesianLearner, InferenceMode,\n    SurpriseMeter, SurpriseType,\n    AdaptiveParticleFilter\n)\nfrom adaptive_bayesian_driver.config import load_config\n\nclass TestRecursiveBayesianLearner(unittest.TestCase):\n    \"\"\"Test RecursiveBayesianLearner functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test configuration.\"\"\"\n        self.config = {\n            'model_parameters': {\n                'learning_rate': 0.01,\n                'adaptation_rate': 0.1,\n                'state_dim': 4,\n                'obs_dim': 2,\n                'baseline_window': 20\n            },\n            'experiment': {'seed': 42}\n        }\n\n    def test_gaussian_initialization(self):\n        \"\"\"Test Gaussian inference mode initialization.\"\"\"\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\n\n        self.assertEqual(learner.inference_mode, InferenceMode.GAUSSIAN)\n        self.assertEqual(learner.state_mean.shape, (4,))\n        self.assertEqual(learner.state_cov.shape, (4, 4))\n\n    def test_particle_initialization(self):\n        \"\"\"Test particle filter initialization.\"\"\"\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.PARTICLE)\n\n        self.assertEqual(learner.inference_mode, InferenceMode.PARTICLE)\n        self.assertEqual(learner.particles.shape, (100, 4))  # Default n_particles\n        self.assertEqual(len(learner.weights), 100)\n\n    def test_gaussian_prediction(self):\n        \"\"\"Test Gaussian prediction step.\"\"\"\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\n\n        prediction = learner.predict()\n\n        self.assertIn('state_mean', prediction)\n        self.assertIn('state_cov', prediction)\n        self.assertIn('obs_mean', prediction)\n\n    def test_gaussian_update(self):\n        \"\"\"Test Gaussian update step.\"\"\"\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\n\n        observation = np.array([1.0, 2.0])\n        result = learner.update(observation)\n\n        self.assertIn('primary_surprise', result)\n        self.assertIn('context_beliefs', result)\n        self.assertIn('adaptive_learning_rate', result)\n\n    def test_particle_update(self):\n        \"\"\"Test particle filter update step.\"\"\"\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.PARTICLE)\n\n        observation = np.array([1.0, 2.0])\n        result = learner.update(observation)\n\n        self.assertIn('primary_surprise', result)\n        self.assertIn('effective_sample_size', result)\n\n    def test_decision_making(self):\n        \"\"\"Test decision making.\"\"\"\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\n\n        observation = np.array([1.0, 2.0])\n        decision = learner.make_decision(observation, ['left', 'right'])\n\n        self.assertIn('decision', decision)\n        self.assertIn('confidence', decision)\n        self.assertIn(decision['decision'], ['left', 'right'])\n\n    def test_reset(self):\n        \"\"\"Test reset functionality.\"\"\"\n        learner = RecursiveBayesianLearner(self.config, InferenceMode.GAUSSIAN)\n\n        # Process some data\n        for _ in range(5):\n            observation = np.random.randn(2)\n            learner.update(observation)\n\n        # Reset\n        learner.reset()\n\n        self.assertEqual(len(learner.surprise_history), 0)\n        self.assertEqual(learner.context_uncertainty, 0.5)\n\nclass TestSurpriseMeter(unittest.TestCase):\n    \"\"\"Test SurpriseMeter functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test configuration.\"\"\"\n        self.meter = SurpriseMeter(mode=SurpriseType.CHI2)\n\n    def test_chi2_surprise(self):\n        \"\"\"Test chi-squared surprise computation.\"\"\"\n        innovation = np.array([1.0, 2.0])\n        covariance = np.eye(2) * 0.5\n\n        surprise = self.meter.compute_surprise(innov=innovation, S=covariance)\n\n        self.assertIsInstance(surprise, float)\n        self.assertGreaterEqual(surprise, 0.0)\n\n    def test_reconstruction_surprise(self):\n        \"\"\"Test reconstruction error surprise.\"\"\"\n        meter = SurpriseMeter(mode=SurpriseType.RECONSTRUCTION)\n\n        x = torch.randn(1, 10)\n        x_recon = x + torch.randn(1, 10) * 0.1\n\n        surprise = meter.compute_surprise(x=x, x_recon=x_recon)\n\n        self.assertIsInstance(surprise, float)\n        self.assertGreaterEqual(surprise, 0.0)\n\n    def test_baseline_update(self):\n        \"\"\"Test baseline statistics update.\"\"\"\n        meter = SurpriseMeter(mode=SurpriseType.CHI2, baseline_window=5)\n\n        # Add some errors\n        for i in range(10):\n            meter.update_baseline(float(i))\n\n        # Check baseline stats updated\n        self.assertGreater(meter.baseline_stats['mean'], 0)\n        self.assertEqual(len(meter.recent_errors), 5)  # Window size\n\n    def test_normalized_surprise(self):\n        \"\"\"Test normalized surprise computation.\"\"\"\n        meter = SurpriseMeter(mode=SurpriseType.CHI2)\n\n        # Add baseline data\n        for _ in range(20):\n            innovation = np.random.randn(2)\n            covariance = np.eye(2)\n            meter.compute_normalized_surprise(innov=innovation, S=covariance)\n\n        # Compute normalized surprise\n        innovation = np.array([5.0, 5.0])  # Large innovation\n        covariance = np.eye(2)\n\n        surprise = meter.compute_normalized_surprise(innov=innovation, S=covariance)\n\n        self.assertGreaterEqual(surprise, 0.0)\n\nclass TestAdaptiveParticleFilter(unittest.TestCase):\n    \"\"\"Test AdaptiveParticleFilter functionality.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up test configuration.\"\"\"\n        self.config = {\n            'n_particles': 50,\n            'state_dim': 3,\n            'observation_dim': 2,\n            'process_noise_std': 0.1,\n            'observation_noise_std': 0.1,\n            'adaptive_noise': True,\n            'adaptive_resampling': True\n        }\n        self.pf = AdaptiveParticleFilter(self.config)\n\n    def test_initialization(self):\n        \"\"\"Test proper initialization.\"\"\"\n        self.assertEqual(self.pf.particles.shape, (50, 3))\n        self.assertEqual(len(self.pf.weights), 50)\n        self.assertAlmostEqual(np.sum(self.pf.weights), 1.0, places=6)\n\n    def test_prediction(self):\n        \"\"\"Test prediction step.\"\"\"\n        result = self.pf.predict()\n\n        self.assertIn('mean', result)\n        self.assertIn('covariance', result)\n        self.assertEqual(result['mean'].shape, (3,))\n\n    def test_update(self):\n        \"\"\"Test update step.\"\"\"\n        observation = np.array([1.0, 2.0])\n\n        result = self.pf.update(observation)\n\n        self.assertIn('mean', result)\n        self.assertIn('effective_sample_size', result)\n        self.assertIn('resampled', result)\n\n    def test_resampling(self):\n        \"\"\"Test resampling functionality.\"\"\"\n        # Set uneven weights to trigger resampling\n        self.pf.weights[0] = 0.9\n        self.pf.weights[1:] = 0.1 / (len(self.pf.weights) - 1)\n\n        observation = np.array([1.0, 2.0])\n        result = self.pf.update(observation)\n\n        # Should have resampled due to low effective sample size\n        # (This might not always trigger, but test structure is correct)\n        self.assertIn('resampled', result)\n\n    def test_state_estimate(self):\n        \"\"\"Test state estimation.\"\"\"\n        estimate = self.pf.get_state_estimate()\n\n        self.assertEqual(estimate.shape, (3,))\n        self.assertIsInstance(estimate, np.ndarray)\n\n    def test_uncertainty(self):\n        \"\"\"Test uncertainty computation.\"\"\"\n        uncertainty = self.pf.get_uncertainty()\n\n        self.assertIsInstance(uncertainty, float)\n        self.assertGreaterEqual(uncertainty, 0.0)\n\n    def test_reset(self):\n        \"\"\"Test reset functionality.\"\"\"\n        # Run some updates\n        for _ in range(5):\n            observation = np.random.randn(2)\n            self.pf.update(observation)\n\n        # Reset\n        self.pf.reset()\n\n        self.assertEqual(len(self.pf.state_history), 0)\n        self.assertAlmostEqual(np.sum(self.pf.weights), 1.0, places=6)\n\nif __name__ == '__main__':\n    unittest.main()\n",
      "directory": "tests",
      "is_binary": false,
      "size_bytes": 8196,
      "filename": "test_recursive_bayes.py"
    }
  ]
}